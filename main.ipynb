{"cells":[{"cell_type":"markdown","id":"zSHSxxYAYxpy","metadata":{"id":"zSHSxxYAYxpy"},"source":["**2023 Samsung AI Challenge : Camera-Invariant Domain Adaptation**\n","\n","- 사용 환경 : Colab Pro Plus\n","\n","- OS : Ubuntu 22.04.2 LTS\n","\n","- GPU : A100 40GB\n","\n","- Cuda 버전 : 11.8\n","\n","- Python 버전 : 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n","\n","- 라이브러리 버전\n","    - numpy: 1.23.5\n","    - matplotlib: 3.7.1\n","    - pandas: 1.5.3\n","    - tqdm: 4.66.1\n","    - torch: 2.0.1\n","    - torchvision: 0.15.2\n","    - albumentations: 1.3.1\n","    - cv2: 4.8.0\n","    - timm: 0.9.7\n","    - huggingface-hub: 0.17.3\n","    - safetensors: 0.3.3"]},{"cell_type":"code","execution_count":1,"id":"bc8tM0qp9KZ1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20988,"status":"ok","timestamp":1696198392006,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"bc8tM0qp9KZ1","outputId":"2bd725f0-157e-4f63-a78a-60212b56e7e3"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32md:\\jyn\\samsung\\open\\[_Private 9th _ 0.63704 ] UDA -review.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/jyn/samsung/open/%5B_Private%209th%20_%200.63704%20%5D%20UDA%20-review.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/jyn/samsung/open/%5B_Private%209th%20_%200.63704%20%5D%20UDA%20-review.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"OymU-K_jboYv","metadata":{"id":"OymU-K_jboYv"},"source":["#라이브러리\n"]},{"cell_type":"code","execution_count":null,"id":"iSbFNVImbqP9","metadata":{"id":"iSbFNVImbqP9"},"outputs":[],"source":["!pip install timm"]},{"cell_type":"code","execution_count":4,"id":"ad9b681e-370a-4cfa-a452-dd2d7f0cd77f","metadata":{"id":"ad9b681e-370a-4cfa-a452-dd2d7f0cd77f"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import cv2, glob,  time, json ,os,tqdm,random,math, shutil\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","\n","from functools import partial\n","\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from timm.models.vision_transformer import _cfg\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","import os\n","from content.Deeplabv3 import network"]},{"cell_type":"code","execution_count":null,"id":"cS-UYRKc95ux","metadata":{"id":"cS-UYRKc95ux"},"outputs":[],"source":["!git clone https://github.com/VainF/DeepLabV3Plus-Pytorch.git /content/Deeplabv3\n","from Deeplabv3 import network"]},{"cell_type":"markdown","id":"E-wMbc1qSPRf","metadata":{"id":"E-wMbc1qSPRf"},"source":["# 데이터 폴더 복사"]},{"cell_type":"code","execution_count":null,"id":"cr-K2-zkZxKZ","metadata":{"id":"cr-K2-zkZxKZ"},"outputs":[],"source":["# Colab에서 장기간 학습 시 Google Drive 연결이 잠깐 끊겨 학습이 중단되는 현상이 발생해 Colab 로컬 드라이브에 복사해두어 학습 진행\n","# Colab 환경이 아닌 경우 root_path 와 data_path를 동일한 값으로 설정하고 이 코드만 실행해도 됨\n","\n","#기본 경로설정\n","root_path = '/content/drive/MyDrive/samsung_seg(512,1024)'\n","data_path = '/content/samsung_seg(512,1024)'"]},{"cell_type":"code","execution_count":null,"id":"cxnCuLdyJenZ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87681,"status":"ok","timestamp":1696198494388,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"cxnCuLdyJenZ","outputId":"a8e998c9-6f7e-4dcb-8dcc-9a8716b6b227"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [01:27<00:00, 87.76s/it]\n"]}],"source":["\n","if os.path.exists(data_path):\n","    shutil.rmtree(data_path)\n","\n","listdir = ['train_source_image',\n","           'train_source_gt',\n","           'train_target_image',\n","           'val_source_gt',\n","           'val_source_image',\n","           'test_image']\n","\n","#listdir = ['test_image'] #추론만 할 경우\n","\n","for dir in tqdm.tqdm(listdir):\n","    shutil.copytree(f'{root_path}/{dir}',f'{data_path}/{dir}')"]},{"cell_type":"markdown","id":"h5PCUOxl2DBg","metadata":{"id":"h5PCUOxl2DBg"},"source":["# 데이터 시각화 설정"]},{"cell_type":"code","execution_count":7,"id":"0noiVoCOLnrE","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1696198494388,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"0noiVoCOLnrE","outputId":"fea77e4e-ca0b-4a0e-89c1-9bbab4f573b3"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# 이미지를 정규화된 상태에서 역정규화하는 함수를 정의합니다.\n","def denormalization(img, source=True):\n","    if source:\n","        # source 매개변수가 True이면, source_mean과 source_std 값을 사용합니다.\n","        mean = source_mean\n","        std = source_std\n","    else:\n","        # source 매개변수가 False이면, target_mean과 target_std 값을 사용합니다.\n","        mean = target_mean\n","        std = target_std\n","\n","    # 이미지를 역정규화합니다.\n","    out_img = img * std + mean\n","    return out_img\n","\n","# 색상 팔레트 및 클래스 이름을 정의합니다.\n","palette = [[0, 0, 0], [0, 255, 0], [127, 127, 127], [255, 0, 255], [153, 76, 0], [0, 153, 153], [255, 0, 0], [204, 255, 204], [0, 255, 255], [255, 255, 204], [255, 0, 127], [255, 127, 0], [255, 255, 255]]\n","cls_name = ['Road', 'Sidewalk', 'Construction', 'Fence', 'Pole', 'Traffic Light', 'Traffic Sign', 'Nature', 'Sky', 'Person', 'Rider', 'Car', 'None']\n","\n","# 팔레트의 각 색상 및 클래스 이름을 순회하면서 시각적으로 표시합니다.\n","indices = list(range(len(palette)))\n","\n","# matplotlib을 사용하여 RGB 값을 가지고 색상을 생성하고 영역을 채웁니다.\n","for i, (rgb, label) in enumerate(zip(palette, cls_name)):\n","    r, g, b = [x / 255 for x in rgb]  # RGB 값을 0에서 1 범위로 정규화합니다.\n","    color = (r, g, b)  # 정규화된 RGB 값을 color 변수에 저장합니다.\n","    plt.fill_between([i, i + 1], 0, 1, color=color)  # 현재 색상으로 영역을 채웁니다.\n","\n","    # 첫 번째 항목 (인덱스 0)인 경우 텍스트 색상을 흰색으로, 그 외에는 검은색으로 설정합니다.\n","    if i == 0:\n","        c = \"white\"\n","    else:\n","        c = \"black\"\n","\n","    # 클래스 이름을 중앙 정렬하여 현재 색상 아래에 표시합니다.\n","    plt.text(i + 0.5, 0.5, label, ha='center', rotation=90, color=c)\n","\n","# x와 y 축의 범위, 레이블 및 제목을 설정하여 그래프를 보여줍니다.\n","plt.xlim(0, len(palette))\n","plt.ylim(0, 1)\n","plt.xlabel('Index')  # x 축 레이블 설정\n","plt.title('RGB Colors and Descriptions')  # 그래프 제목 설정\n","plt.show()  # 그래프를 화면에 표시합니다.\n"]},{"cell_type":"markdown","id":"be76a29e-e9c2-411a-a569-04166f074184","metadata":{"id":"be76a29e-e9c2-411a-a569-04166f074184"},"source":["# 데이터로더 정의"]},{"cell_type":"code","execution_count":null,"id":"94129e2a","metadata":{"id":"94129e2a"},"outputs":[],"source":["#사전에 미리 Resize해둔 이미지 크기\n","IMAGE_WIDTH = 1024\n","IMAGE_HEIGHT = 512\n","\n","#Crop한 이미지의 크기\n","CROP_WIDTH = IMAGE_WIDTH//2\n","CROP_HEIGHT = IMAGE_HEIGHT//2"]},{"cell_type":"code","execution_count":null,"id":"yoRPuBksIxDv","metadata":{"id":"yoRPuBksIxDv"},"outputs":[],"source":["#Train_Source + Valid_Source 의 Mean, Std\n","source_mean = [0.5897106 , 0.5952661 , 0.57897425]\n","source_std = [0.16688786, 0.15721062, 0.1589595]\n","\n","#Train_Target 의 Mean, Std\n","target_mean = [0.4714665 , 0.47141412, 0.49733913]\n","target_std = [0.23908237, 0.24033973, 0.25281718]"]},{"cell_type":"code","execution_count":null,"id":"a8496767-2f64-4285-bec4-c6f53a1fd9d2","metadata":{"id":"a8496767-2f64-4285-bec4-c6f53a1fd9d2"},"outputs":[],"source":["# class CustomDataset(Dataset):\n","#     def __init__(self, csv_file, target_csv_file=None, mix_bg_prob = 0, infer=False):\n","\n","#         #여러 csv 파일 합치기\n","#         if type(csv_file) == list:\n","#             self.data = pd.read_csv(csv_file[0])\n","#             for i in range(1,len(csv_file)):\n","#                 self.data = pd.concat([self.data, pd.read_csv(csv_file[i])], ignore_index=True)\n","#         else :\n","#             self.data = pd.read_csv(csv_file)\n","\n","#         self.infer = infer\n","\n","#         self.mix_bg_prob = mix_bg_prob\n","\n","#         #공통으로 적용되는 Transform\n","\n","#         self.augmentation = A.Compose([\n","#                 A.ColorJitter(p=0.25),\n","#                 A.HorizontalFlip(p=0.5),\n","#                 A.RandomCrop(width = CROP_WIDTH, height = CROP_HEIGHT)\n","#             ])\n","\n","#         self.source_norm = A.Normalize(mean = source_mean , std = source_std)\n","#         self.target_norm = A.Normalize(mean = target_mean , std = target_std)\n","\n","#         self.len_source = len(self.data)\n","\n","#         if target_csv_file :\n","#             self.target_data = pd.read_csv(target_csv_file)\n","#             self.len_target = len(self.target_data)\n","\n","#         else :\n","#             self.target_data = None\n","#             self.len_target = 1\n","\n","#     def mix_bg(self,source_image,source_mask,target_image):\n","\n","#         h,w,c = source_image.shape\n","\n","#         # 타원의 중심 좌표와 크기 설정 (예: 중심 (x, y), 장축 반지름 a, 단축 반지름 b)\n","#         center = (IMAGE_WIDTH//2, int(IMAGE_HEIGHT*0.375))  # x,y\n","#         axis_length = (IMAGE_WIDTH//2, int(IMAGE_HEIGHT*0.64))  # 장축 반지름과 단축 반지름\n","\n","#         # 타원 그리기 (타원을 1로 채우고 나머지 부분은 0으로 채움)\n","#         mask = np.zeros((h,w,1),dtype=np.uint8)\n","#         cv2.ellipse(mask, center, axis_length, 0, 0, 360, 1, -1)\n","\n","#         # 타원 모양으로 crop된 이미지 생성\n","#         mixed_image = source_image * mask + target_image * (1-mask)\n","#         mixed_mask = source_mask * mask[:,:,0] + np.ones_like(source_mask)*12*(1-mask[:,:,0])\n","\n","#         return mixed_image, mixed_mask\n","\n","#     def __len__(self):\n","#         return self.len_source\n","\n","#     def __getitem__(self, idx):\n","#         source_idx = idx\n","\n","#         img_path = self.data.iloc[source_idx, 1].replace('./',data_path+'/')\n","#         image = cv2.imread(img_path)\n","#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","\n","\n","#         if self.infer == True:\n","#             image = A.Compose([self.target_norm,\n","#                                ToTensorV2()])(image=image)['image']\n","#             return image\n","\n","\n","#         mask_path = self.data.iloc[source_idx, 2].replace('./',data_path+'/')\n","#         mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","#         mask[mask == 255] = 12\n","\n","#         # 보닛 부분에 있는 하늘 클래스들을 배경클래스로 변환\n","#         mask[IMAGE_HEIGHT//3*2:][mask[IMAGE_HEIGHT//3*2:]==8] = 12\n","\n","#         target_image = np.zeros_like(image) \n","#         distortion_image = image.copy()\n","#         distortion_mask = mask.copy()\n","\n","#         if self.target_data is not None:\n","#             target_img_path = self.target_data.iloc[idx % self.len_target, 1].replace('./',data_path+'/')\n","#             target_image = cv2.imread(target_img_path)\n","#             target_image = cv2.cvtColor(target_image, cv2.COLOR_BGR2RGB)\n","\n","#             if self.mix_bg_prob > np.random.uniform(0,1):\n","#                 distortion_image,distortion_mask = self.mix_bg(distortion_image.astype(np.uint8),distortion_mask.astype(np.uint8),target_image)\n","\n","#         source_tensor = A.Compose([self.augmentation,\n","#                                    self.source_norm,\n","#                                    ToTensorV2()])(image=image,mask=mask)\n","\n","#         target_image = A.Compose([self.augmentation,\n","#                                   self.target_norm,\n","#                                   ToTensorV2()])(image=target_image)['image']\n","\n","#         distortion_tensor = A.Compose([self.augmentation,\n","#                                     A.ElasticTransform(alpha=100, sigma=10, alpha_affine=25,border_mode = 1,p=1),\n","#                                     self.source_norm,\n","#                                     ToTensorV2()])(image=distortion_image,mask= distortion_mask)\n","\n","#         image = source_tensor['image']\n","#         mask = source_tensor['mask']\n","#         distortion_image = distortion_tensor['image']\n","#         distortion_mask = distortion_tensor['mask']\n","\n","\n","#         return image, mask, target_image, distortion_image, distortion_mask"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# PyTorch의 Dataset 클래스를 상속하여 사용자 정의 데이터셋을 생성합니다.\n","class CustomDataset(Dataset):\n","    def __init__(self, csv_file, target_csv_file=None, mix_bg_prob=0, infer=False):\n","\n","        # 여러 CSV 파일을 병합합니다.\n","        if type(csv_file) == list:\n","            self.data = pd.read_csv(csv_file[0])\n","            for i in range(1, len(csv_file)):\n","                self.data = pd.concat([self.data, pd.read_csv(csv_file[i])], ignore_index=True)\n","        else:\n","            self.data = pd.read_csv(csv_file)\n","\n","        self.infer = infer\n","\n","        self.mix_bg_prob = mix_bg_prob\n","\n","        # 공통으로 적용되는 데이터 변환을 설정합니다.\n","        self.augmentation = A.Compose([\n","            A.ColorJitter(p=0.25),\n","            A.HorizontalFlip(p=0.5),\n","            A.RandomCrop(width=CROP_WIDTH, height=CROP_HEIGHT)\n","        ])\n","\n","        self.source_norm = A.Normalize(mean=source_mean, std=source_std)\n","        self.target_norm = A.Normalize(mean=target_mean, std=target_std)\n","\n","        self.len_source = len(self.data)\n","\n","        if target_csv_file:\n","            self.target_data = pd.read_csv(target_csv_file)\n","            self.len_target = len(self.target_data)\n","        else:\n","            self.target_data = None\n","            self.len_target = 1\n","\n","    # 배경 이미지와 원본 이미지를 혼합하는 메소드\n","    def mix_bg(self, source_image, source_mask, target_image):\n","        # 소스 이미지의 높이, 너비, 채널 수를 가져옵니다.\n","        h, w, c = source_image.shape\n","\n","        # 타원 모양의 마스크를 생성합니다.\n","        # 타원은 중심 좌표와 장축, 단축 반지름 정보를 사용하여 정의됩니다.\n","        center = (IMAGE_WIDTH // 2, int(IMAGE_HEIGHT * 0.375))  # 타원 중심 좌표 (x, y)\n","        axis_length = (IMAGE_WIDTH // 2, int(IMAGE_HEIGHT * 0.64))  # 장축 반지름과 단축 반지름\n","\n","        # 마스크를 초기화하고 타원을 그려 마스크를 생성합니다.\n","        mask = np.zeros((h, w, 1), dtype=np.uint8)\n","        cv2.ellipse(mask, center, axis_length, 0, 0, 360, 1, -1)\n","\n","        # 타원 모양으로 이미지를 혼합합니다.\n","        # 타원 내부는 source_image로, 타원 외부는 target_image로 혼합합니다.\n","        mixed_image = source_image * mask + target_image * (1 - mask)\n","\n","        # 마스크도 혼합하여 새로운 마스크를 생성합니다.\n","        # 타원 내부는 source_mask로, 타원 외부는 클래스 12로 (또는 다른 값으로) 설정합니다.\n","        mixed_mask = source_mask * mask[:, :, 0] + np.ones_like(source_mask) * 12 * (1 - mask[:, :, 0])\n","\n","        # 혼합된 이미지와 마스크를 반환합니다.\n","        return mixed_image, mixed_mask\n","\n","    def __len__(self):\n","        return self.len_source\n","\n","    def __getitem__(self, idx):\n","        source_idx = idx\n","\n","        # 이미지와 마스크 파일 경로 설정\n","        img_path = self.data.iloc[source_idx, 1].replace('./', data_path + '/')\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.infer == True:\n","            # 추론 모드에서 이미지를 정규화하고 PyTorch Tensor로 변환\n","            image = A.Compose([self.target_norm, ToTensorV2()])(image=image)['image']\n","            return image\n","\n","        mask_path = self.data.iloc[source_idx, 2].replace('./', data_path + '/')\n","        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","        mask[mask == 255] = 12\n","\n","        # 이미지의 하늘 부분을 배경 클래스로 변환\n","        mask[IMAGE_HEIGHT // 3 * 2:][mask[IMAGE_HEIGHT // 3 * 2:] == 8] = 12\n","\n","        #image와 동일한 크기와 형태를 가지지만 값은 모두 0으로 초기화\n","        target_image = np.zeros_like(image)\n","        distortion_image = image.copy()\n","        distortion_mask = mask.copy()\n","\n","        # self.target_data가 존재하는 경우에만 실행\n","        if self.target_data is not None:\n","            # 대상 이미지 파일 경로 설정\n","            # idx % self.len_target를 사용하여 대상 데이터에서 이미지 선택\n","            target_img_path = self.target_data.iloc[idx % self.len_target, 1].replace('./', data_path + '/')\n","            \n","            # 대상 이미지를 읽어옵니다.\n","            target_image = cv2.imread(target_img_path)\n","            \n","            # 대상 이미지의 색상 체계를 BGR에서 RGB로 변환합니다.\n","            target_image = cv2.cvtColor(target_image, cv2.COLOR_BGR2RGB)\n","\n","            # mix_bg_prob 확률로 이미지 혼합 작업 수행\n","            if self.mix_bg_prob > np.random.uniform(0, 1):\n","                # 이미지 혼합을 수행할 때 distortion_image와 distortion_mask는 이미 초기화되어 있어야 합니다.\n","                distortion_image, distortion_mask = self.mix_bg(distortion_image.astype(np.uint8),\n","                                                                distortion_mask.astype(np.uint8), target_image)\n","\n","\n","        # 데이터 변환 수행\n","        source_tensor = A.Compose([self.augmentation, self.source_norm, ToTensorV2()])(image=image, mask=mask)\n","        target_image = A.Compose([self.augmentation, self.target_norm, ToTensorV2()])(image=target_image)['image']\n","        distortion_tensor = A.Compose([self.augmentation,\n","                                       A.ElasticTransform(alpha=100, sigma=10, alpha_affine=25, border_mode=1, p=1),\n","                                       self.source_norm, ToTensorV2()])(image=distortion_image, mask=distortion_mask)\n","\n","        image = source_tensor['image']\n","        mask = source_tensor['mask']\n","        distortion_image = distortion_tensor['image']\n","        distortion_mask = distortion_tensor['mask']\n","\n","        return image, mask, target_image, distortion_image, distortion_mask\n"]},{"cell_type":"markdown","id":"dc955893-22fd-4320-88be-7aa0d790cbd9","metadata":{"id":"dc955893-22fd-4320-88be-7aa0d790cbd9"},"source":["# 데이터 증강, 로더 테스트"]},{"cell_type":"code","execution_count":null,"id":"1b708503-2ff9-4584-9d73-40990b3572f8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2438,"status":"ok","timestamp":1696139953909,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"1b708503-2ff9-4584-9d73-40990b3572f8","outputId":"bb61a534-6bb1-464d-b3ed-4a51b89fea1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["3126\n","3126\n","2923\n"]}],"source":["\n","train_set = CustomDataset(csv_file=[f'{root_path}/train_source.csv', f'{root_path}/val_source.csv', f'{root_path}/val_source.csv'],\n","                          target_csv_file = f'{root_path}/train_target.csv',\n","                          mix_bg_prob = 0.25)\n","valid_set = CustomDataset(csv_file=f'{root_path}/val_source.csv' ) #실제로 사용하지는 않음, 1/10의 데이터를 사용하여도 100에폭까지 오버피팅이 발생하지 않았음\n","\n","print(len(train_set))\n","print(train_set.len_source)\n","print(train_set.len_target)"]},{"cell_type":"code","execution_count":9,"id":"MIT-2G1bcvRt","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29571,"status":"ok","timestamp":1696139983477,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"MIT-2G1bcvRt","outputId":"2878fde6-a9b9-4072-a224-219922e0a71d"},"outputs":[{"ename":"NameError","evalue":"name 'train_set' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32md:\\jyn\\samsung\\open\\[_Private 9th _ 0.63704 ] UDA -review.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/jyn/samsung/open/%5B_Private%209th%20_%200.63704%20%5D%20UDA%20-review.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2660\u001b[39m,\u001b[39m2670\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/jyn/samsung/open/%5B_Private%209th%20_%200.63704%20%5D%20UDA%20-review.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/jyn/samsung/open/%5B_Private%209th%20_%200.63704%20%5D%20UDA%20-review.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     img,mask,target_img,distortion_img,distortion_mask \u001b[39m=\u001b[39m train_set\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(i)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/jyn/samsung/open/%5B_Private%209th%20_%200.63704%20%5D%20UDA%20-review.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_time)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/jyn/samsung/open/%5B_Private%209th%20_%200.63704%20%5D%20UDA%20-review.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     img \u001b[39m=\u001b[39m (denormalization(np\u001b[39m.\u001b[39mtranspose(img,(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mnumpy())\u001b[39m*\u001b[39m\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n","\u001b[1;31mNameError\u001b[0m: name 'train_set' is not defined"]}],"source":["# 이미지 인덱스 2660에서 2669까지 반복\n","for i in range(2660, 2670):\n","    # 현재 시간 측정\n","    start_time = time.time()\n","\n","    # train_set.__getitem__(i)을 통해 이미지, 마스크 및 기타 데이터를 가져옵니다.\n","    img, mask, target_img, distortion_img, distortion_mask = train_set.__getitem__(i)\n","    \n","    # 처리 시간 출력\n","    print(time.time() - start_time)\n","\n","    # 이미지 및 대상 이미지를 밀어내기 및 밀어내기 반전 작업 수행\n","    img = (denormalization(np.transpose(img, (1, 2, 0)).numpy()) * 255).astype(np.uint8)\n","    target_img = (denormalization(np.transpose(target_img, (1, 2, 0)).numpy(), False) * 255).astype(np.uint8)\n","    distortion_img = (denormalization(np.transpose(distortion_img, (1, 2, 0)).numpy()) * 255).astype(np.uint8)\n","    print(target_img.min(), target_img.max(), target_img.shape)\n","\n","    # color_mask 및 color_distortion_mask 초기화\n","    color_mask = np.zeros_like(img, dtype=np.uint8)\n","    color_distortion_mask = np.zeros_like(img, dtype=np.uint8)\n","\n","    # 각 클래스에 대해 color_mask 및 color_distortion_mask 업데이트\n","    for j, color in enumerate(palette):\n","        color_mask[mask == j] = color\n","        color_distortion_mask[distortion_mask == j] = color\n","\n","    # 이미지 및 결과를 시각화하기 위한 플롯 생성\n","    plt.figure(figsize=(30, 15))\n","    \n","    # 원본 이미지\n","    plt.subplot(1, 6, 1)\n","    plt.imshow(img)\n","\n","    # 클래스에 따라 색칠된 마스크 이미지\n","    plt.subplot(1, 6, 2)\n","    plt.imshow(color_mask)\n","\n","    # 원본 이미지와 색칠된 마스크 이미지를 중간 비중으로 혼합\n","    plt.subplot(1, 6, 3)\n","    plt.imshow(cv2.addWeighted(img, 0.5, color_mask, 0.5, 0))\n","\n","    # 대상 이미지\n","    plt.subplot(1, 6, 4)\n","    plt.imshow(target_img)\n","\n","    # 왜곡된 이미지\n","    plt.subplot(1, 6, 5)\n","    plt.imshow(distortion_img)\n","\n","    # 왜곡된 이미지와 색칠된 왜곡 마스크 이미지를 중간 비중으로 혼합\n","    plt.subplot(1, 6, 6)\n","    plt.imshow(cv2.addWeighted(distortion_img, 0.5, color_distortion_mask, 0.5, 0))\n","\n","    # 플롯 표시\n","    plt.show()\n"]},{"cell_type":"markdown","id":"V6tw1MEk55ye","metadata":{"id":"V6tw1MEk55ye"},"source":["#모델 선언\n"]},{"cell_type":"markdown","id":"Qc21XLZTalhH","metadata":{"id":"Qc21XLZTalhH"},"source":["##DAFormer\n"]},{"cell_type":"markdown","id":"6fE34U9ee1If","metadata":{"id":"6fE34U9ee1If"},"source":["### MiT"]},{"cell_type":"code","execution_count":null,"id":"LqlZ7HC8wMWl","metadata":{"id":"LqlZ7HC8wMWl"},"outputs":[],"source":["\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.dwconv = DWConv(hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = self.fc1(x)\n","        x = self.dwconv(x, H, W)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n","        super().__init__()\n","        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n","\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        self.sr_ratio = sr_ratio\n","        if sr_ratio > 1:\n","            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n","            self.norm = nn.LayerNorm(dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","\n","        if self.sr_ratio > 1:\n","            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n","            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n","            x_ = self.norm(x_)\n","            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        else:\n","            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        k, v = kv[0], kv[1]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim,\n","            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, H, W):\n","        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n","        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n","\n","        return x\n","\n","\n","class OverlapPatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n","        self.num_patches = self.H * self.W\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n","                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = self.proj(x)\n","        _, _, H, W = x.shape\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.norm(x)\n","\n","        return x, H, W\n","\n","\n","class MixVisionTransformer(nn.Module):\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n","                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n","                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n","                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.depths = depths\n","\n","        # patch_embed\n","        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n","                                              embed_dim=embed_dims[0])\n","        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n","                                              embed_dim=embed_dims[1])\n","        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n","                                              embed_dim=embed_dims[2])\n","        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n","                                              embed_dim=embed_dims[3])\n","\n","        # transformer encoder\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","        cur = 0\n","        self.block1 = nn.ModuleList([Block(\n","            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[0])\n","            for i in range(depths[0])])\n","        self.norm1 = norm_layer(embed_dims[0])\n","\n","        cur += depths[0]\n","        self.block2 = nn.ModuleList([Block(\n","            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[1])\n","            for i in range(depths[1])])\n","        self.norm2 = norm_layer(embed_dims[1])\n","\n","        cur += depths[1]\n","        self.block3 = nn.ModuleList([Block(\n","            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[2])\n","            for i in range(depths[2])])\n","        self.norm3 = norm_layer(embed_dims[2])\n","\n","        cur += depths[2]\n","        self.block4 = nn.ModuleList([Block(\n","            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n","            sr_ratio=sr_ratios[3])\n","            for i in range(depths[3])])\n","        self.norm4 = norm_layer(embed_dims[3])\n","\n","        # classification head\n","        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","        elif isinstance(m, nn.Conv2d):\n","            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            fan_out //= m.groups\n","            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def reset_drop_path(self, drop_path_rate):\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n","        cur = 0\n","        for i in range(self.depths[0]):\n","            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[0]\n","        for i in range(self.depths[1]):\n","            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[1]\n","        for i in range(self.depths[2]):\n","            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n","\n","        cur += self.depths[2]\n","        for i in range(self.depths[3]):\n","            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n","\n","    def freeze_patch_emb(self):\n","        self.patch_embed1.requires_grad = False\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        outs = []\n","\n","        # stage 1\n","        x, H, W = self.patch_embed1(x)\n","        for i, blk in enumerate(self.block1):\n","            x = blk(x, H, W)\n","        x = self.norm1(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 2\n","        x, H, W = self.patch_embed2(x)\n","        for i, blk in enumerate(self.block2):\n","            x = blk(x, H, W)\n","        x = self.norm2(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 3\n","        x, H, W = self.patch_embed3(x)\n","        for i, blk in enumerate(self.block3):\n","            x = blk(x, H, W)\n","        x = self.norm3(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        # stage 4\n","        x, H, W = self.patch_embed4(x)\n","        for i, blk in enumerate(self.block4):\n","            x = blk(x, H, W)\n","        x = self.norm4(x)\n","        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n","        outs.append(x)\n","\n","        return outs\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        # x = self.head(x)\n","\n","        return x\n","\n","\n","class DWConv(nn.Module):\n","    def __init__(self, dim=768):\n","        super(DWConv, self).__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n","\n","    def forward(self, x, H, W):\n","        B, N, C = x.shape\n","        x = x.transpose(1, 2).view(B, C, H, W)\n","        x = self.dwconv(x)\n","        x = x.flatten(2).transpose(1, 2)\n","\n","        return x\n","\n","\n","\n","class mit_b0(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b0, self).__init__(\n","            patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b1(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b1, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b2(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b2, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b3(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b3, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b4(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b4, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)\n","\n","\n","class mit_b5(MixVisionTransformer):\n","    def __init__(self, **kwargs):\n","        super(mit_b5, self).__init__(\n","            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n","            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n","            drop_rate=0.0, drop_path_rate=0.1)"]},{"cell_type":"markdown","id":"D8Acq63We6x9","metadata":{"id":"D8Acq63We6x9"},"source":["### DAFormer Head"]},{"cell_type":"code","execution_count":null,"id":"UhIWyygx3FX_","metadata":{"id":"UhIWyygx3FX_"},"outputs":[],"source":["class MLP(nn.Module):\n","    \"\"\"\n","    Linear Embedding\n","    \"\"\"\n","    def __init__(self, input_dim=2048, embed_dim=768):\n","        super().__init__()\n","        self.proj = nn.Linear(input_dim, embed_dim)\n","\n","    def forward(self, x):\n","        x = x.flatten(2).transpose(1, 2)\n","        x = self.proj(x)\n","        return x\n","\n","class Context_Aware_Fusion(nn.Module):\n","    def __init__(self, input_dim=1024, embed_dim=256,dilations=[1,6,12,18]):\n","        super(Context_Aware_Fusion,self).__init__()\n","\n","\n","        self.proj = nn.Linear(input_dim, embed_dim)\n","\n","        self.aspp = nn.ModuleList()\n","\n","        self.dilations = dilations\n","        for dilation in self.dilations:\n","            if dilation == 1:\n","                self.aspp.append(\n","                    nn.Sequential(\n","                            nn.Conv2d(input_dim,embed_dim,1,1),\n","                            nn.BatchNorm2d(embed_dim),\n","                            nn.ReLU()\n","                        )\n","                    )\n","            else:\n","                self.aspp.append(\n","                    nn.Sequential(\n","                            nn.Conv2d(input_dim,input_dim,3,1,dilation,dilation,input_dim),\n","                            nn.Conv2d(input_dim,embed_dim,1,1),\n","                            nn.BatchNorm2d(embed_dim),\n","                            nn.ReLU()\n","                        )\n","                    )\n","\n","        self.bottleneck = nn.Conv2d(input_dim,embed_dim,1)\n","\n","    def forward(self, x):\n","\n","        cat_list = []\n","        for i in range(len(self.dilations)):\n","            cat_list.append(self.aspp[i](x))\n","\n","        out = torch.cat(cat_list,1)\n","\n","        out = self.bottleneck(out)\n","        return out\n","\n","\n","class daformerhead(nn.Module):\n","\n","    def __init__(self,embed_dim = 256):\n","        super(daformerhead,self).__init__()\n","        self.linear_c1 = MLP(64,embed_dim)\n","        self.linear_c2 = MLP(128,embed_dim)\n","        self.linear_c3 = MLP(320,embed_dim)\n","        self.linear_c4 = MLP(512,embed_dim)\n","\n","\n","        self.cls_head = nn.Sequential(\n","            nn.Dropout2d(0.1),\n","            nn.Conv2d(embed_dim,13,1,1)\n","        )\n","        self.fuse_layer = Context_Aware_Fusion(embed_dim*4,embed_dim,[1,6,12,18])\n","\n","\n","    def forward(self,x):\n","        c1, c2, c3, c4 = x  # len=4, 1/4,1/8,1/16,1/32\n","\n","        n, _, h, w = c4.shape\n","\n","\n","\n","        _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n","        _c4 = F.interpolate(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n","\n","        _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n","        _c3 = F.interpolate(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n","\n","        _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n","        _c2 = F.interpolate(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n","\n","\n","        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n","\n","        out = self.fuse_layer(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n","\n","        out = self.cls_head(F.interpolate(out, size=(CROP_HEIGHT,CROP_WIDTH),mode='bilinear',align_corners=False))\n","\n","\n","        return out"]},{"cell_type":"markdown","id":"G7jEIJoFe9i4","metadata":{"id":"G7jEIJoFe9i4"},"source":["### Model"]},{"cell_type":"code","execution_count":null,"id":"soPI4GLbdA6z","metadata":{"id":"soPI4GLbdA6z"},"outputs":[],"source":["class daformer(nn.Module):\n","    def __init__(self,backbone,head):\n","        super(daformer,self).__init__()\n","        self.backbone = backbone\n","        self.classifier = head\n","\n","    def forward(self,x):\n","        out = {}\n","        out['features'] = self.backbone(x)\n","        out['out'] = self.classifier(out['features'])\n","\n","        return out"]},{"cell_type":"markdown","id":"pOfAEs8yafEv","metadata":{"id":"pOfAEs8yafEv"},"source":["##UNET\n"]},{"cell_type":"code","execution_count":null,"id":"Wz_7TRogvsMc","metadata":{"id":"Wz_7TRogvsMc"},"outputs":[],"source":["def double_conv(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True)\n","    )\n","class UNet(nn.Module):\n","    def __init__(self,backbone,last=512):\n","        super(UNet, self).__init__()\n","\n","        self.backbone = nn.ModuleList()\n","\n","        tmp = []\n","\n","        for i in range(28):\n","            if (i+1)%7==0:\n","                self.backbone.append(nn.Sequential(*tmp))\n","                tmp =  [backbone.features[i]]\n","                pass\n","            else:\n","                tmp.append(backbone.features[i])\n","\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","\n","        self.classifier = nn.ModuleList()\n","\n","        self.classifier.append(double_conv(last//2 + last, last//2))\n","        self.classifier.append(double_conv(last//4 + last//2, last//4))\n","        self.classifier.append(double_conv(last//8 + last//4, last//8))\n","        self.classifier.append(nn.Conv2d(last//8, 13, 1))\n","\n","    def forward(self,x):\n","\n","\n","        layer1 = self.backbone[0](x)\n","\n","        layer2 = self.backbone[1](layer1)\n","        layer3 = self.backbone[2](layer2)\n","\n","        layer4 = self.backbone[3](layer3)\n","\n","        out = {}\n","\n","        out['feature'] = layer4\n","\n","        x = self.upsample(layer4)\n","        x = torch.cat([x, layer3], dim=1)\n","\n","        x = self.classifier[0](x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, layer2], dim=1)\n","\n","        x = self.classifier[1](x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, layer1], dim=1)\n","\n","        x = self.classifier[2](x)\n","\n","\n","        out['out'] = self.classifier[3](x)\n","\n","\n","        return out\n","\n"]},{"cell_type":"code","execution_count":null,"id":"zoCS8scj5kK9","metadata":{"id":"zoCS8scj5kK9"},"outputs":[],"source":["def double_conv(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True)\n","    )\n","class ResUNet(nn.Module):\n","    def __init__(self,backbone,last=512):\n","        super(ResUNet, self).__init__()\n","\n","        self.backbone = nn.ModuleList()\n","\n","        self.backbone.append(\n","            nn.Sequential(\n","                backbone.conv1,\n","                backbone.bn1,\n","                backbone.relu\n","            ))\n","        self.backbone.append(backbone.layer1)\n","        self.backbone.append(backbone.layer2)\n","        self.backbone.append(backbone.layer3)\n","        self.backbone.append(backbone.layer4)\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","\n","        self.classifier = nn.ModuleList()\n","\n","        self.classifier.append(double_conv(last//2 + last, last//2))\n","        self.classifier.append(double_conv(last//4 + last//2, last//4))\n","        self.classifier.append(double_conv(last//8 + last//4, last//8))\n","        self.classifier.append(nn.Conv2d(last//8, 13, 1))\n","\n","    def forward(self,x):\n","\n","        stem = self.backbone[0](x)\n","\n","        layer1 = self.backbone[1](stem)\n","\n","        layer2 = self.backbone[2](layer1)\n","        layer3 = self.backbone[3](layer2)\n","\n","        layer4 = self.backbone[4](layer3)\n","\n","        out = {}\n","\n","        out['feature'] = layer4\n","\n","        x = self.upsample(layer4)\n","        x = torch.cat([x, layer3], dim=1)\n","\n","        x = self.classifier[0](x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, layer2], dim=1)\n","\n","        x = self.classifier[1](x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, layer1], dim=1)\n","\n","        x = self.classifier[2](x)\n","\n","\n","        out['out'] = self.upsample(self.classifier[3](x))\n","\n","\n","        return out\n","\n"]},{"cell_type":"markdown","id":"z4bbpivqaheE","metadata":{"id":"z4bbpivqaheE"},"source":["##DeepLab"]},{"cell_type":"code","execution_count":null,"id":"xXHQVtT-57UM","metadata":{"id":"xXHQVtT-57UM"},"outputs":[],"source":["class ASPP(nn.Module):\n","    def __init__(self, in_channels, n_classes=21,paddings=[6,12,18,24], dilations=[6,12,18,24]):\n","        super(ASPP, self).__init__()\n","\n","        self.branch = nn.ModuleList()\n","\n","        for p,d in zip(paddings, dilations):\n","            self.branch.append(nn.Conv2d(in_channels, n_classes, kernel_size=3, stride=1, padding=p, dilation=d))\n","\n","    def forward(self, x):\n","        branchs = []\n","\n","        for b in self.branch:\n","            branchs.append(b(x))\n","\n","\n","        out = sum(branchs)\n","\n","        return out\n","\n","\n","\n","class deeplabv2(nn.Module):\n","    def __init__(self,backbone):\n","        super(deeplabv2,self).__init__()\n","        self.backbone = backbone\n","        self.classifier = ASPP(2048, 13)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n","        self.distortion = nn.Linear(2048,1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self,x):\n","        out = {}\n","\n","        b,c,h,w = x.shape\n","\n","        out['feature'] = self.backbone(x)['out']\n","        out['out'] = F.interpolate(self.classifier(out['feature']), size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n","\n","        out['distortion'] = self.sigmoid(self.distortion(self.avg_pool(out['feature']).view(b,-1)))\n","\n","        return out\n","\n","\n","\n"]},{"cell_type":"markdown","id":"xTx4SP0CDFym","metadata":{"id":"xTx4SP0CDFym"},"source":["# 함수 설정"]},{"cell_type":"markdown","id":"B0uAomSuDTCO","metadata":{"id":"B0uAomSuDTCO"},"source":["## 평가지표"]},{"cell_type":"code","execution_count":null,"id":"8f2d4101","metadata":{"id":"8f2d4101"},"outputs":[],"source":["def accuracy(pred, label):\n","    with torch.no_grad():\n","\n","        b,c,h,w = pred.shape\n","\n","        _,pred = torch.max(pred, dim=1)\n","        accuracy = torch.sum(pred==label)/(b*h*w)\n","    return accuracy\n","\n","def miou(pred,label):\n","    with torch.no_grad():\n","        _,pred = torch.max(pred, dim=1)\n","        intersection = torch.sum(((pred<12) & (pred==label)))\n","        union = torch.sum((pred<12)|(label<12))\n","\n","\n","        iou = intersection / (union+1e-9)\n","\n","    return iou\n"]},{"cell_type":"markdown","id":"Ri4cx9QjDacy","metadata":{"id":"Ri4cx9QjDacy"},"source":["## Target Loss\n"]},{"cell_type":"code","execution_count":null,"id":"rYuAq2a328_I","metadata":{"id":"rYuAq2a328_I"},"outputs":[],"source":["class target_loss(nn.Module):\n","    def __init__(self, t=0.968):\n","        # 클래스 초기화: 임계값 t를 설정하고 Cross Entropy Loss 함수를 초기화\n","        super(target_loss, self).__init__()\n","        self.t = t\n","        self.celoss = nn.CrossEntropyLoss(reduction='none')\n","\n","    def forward(self, pred, pseudo_label):\n","        b, c, h, w = pseudo_label.shape\n","\n","        # pseudo_label에서 최대값을 찾아 신뢰도와 해당 위치(pt)를 추출\n","        confidence, pt = torch.max(pseudo_label, dim=1)  # b, h, w\n","\n","        # 신뢰도가 t보다 큰 픽셀의 비율(qt)을 계산\n","        qt = (torch.sum(confidence.view(b, -1) > self.t, 1) / (h * w)).view(b, 1, 1)  # b, 1, 1\n","\n","        # 손실 계산: Cross Entropy Loss에 가중치(qt)를 곱하여 계산\n","        loss = torch.mean(self.celoss(pred, pt) * qt)  # b, h, w * b, 1, 1 -> b, h, w -> 1\n","\n","        # 계산된 손실 반환\n","        return loss\n"]},{"cell_type":"markdown","id":"EfTRRWPBDf8J","metadata":{"id":"EfTRRWPBDf8J"},"source":["## DACS\n","DACS 기법 : 이미지별 일부 클래스 섞기 ( 학습 에폭이 큰 경우에만 잘 작동하는 것으로 보이기에 삭제 )"]},{"cell_type":"code","execution_count":null,"id":"yua_5w8BDW8f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1696198495549,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"yua_5w8BDW8f","outputId":"4fec4c27-4193-498b-a0bb-86e665f3dbb8"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n클래스, 비율 별 선택된 클래스 수\\n    0.5\\t0.3\\t0.25\\t0.2\\t0.1\\n13\\t6\\t3\\t3       2\\t1\\n12\\t6\\t3\\t3\\t    2\\t1\\n11\\t5\\t3\\t2\\t    2\\t1\\n10\\t5\\t3\\t2\\t    2\\t1\\n9\\t4\\t2\\t2\\t    1\\t0\\n8\\t4\\t2\\t2\\t    1\\t0\\n7\\t3\\t2\\t1\\t    1\\t0\\n6\\t3\\t1\\t1\\t    1\\t0\\n5\\t2\\t1\\t1\\t    1\\t0\\n4\\t2\\t1\\t1\\t    0\\t0\\n3\\t1\\t0\\t0\\t    0\\t0\\n2\\t1\\t0\\t0\\t    0\\t0\\n1\\t0\\t0\\t0\\t    0\\t0\\n0\\t0\\t0\\t0\\t    0\\t0\\n\\n'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["def dacs(xs, ys, xt, yt, ratio=0.25):\n","    # 입력 데이터 크기를 가져와서 변수 b(배치 크기)와 c(클래스 수)를 초기화\n","    b, c = yt.shape[:2]\n","\n","    # source(target) 데이터를 복제하여 새로운 텐서 xm(ym)를 만듭니다.\n","    xm = xt.detach().clone()  # b, c, h, w\n","    ym = yt.detach().clone()  # b, c, h, w\n","\n","    # 배치 크기(b)만큼 반복하면서 데이터 조작을 수행\n","    for i in range(b):\n","        # 현재 배치에서 고유한 클래스(cls)를 가져옵니다.\n","        cls = ys[i].unique()\n","\n","        # ys_one_hot: 클래스를 원-핫 인코딩 형태로 변환 (b, c, h, w)\n","        ys_one_hot = F.one_hot(ys, c).permute(0, 3, 1, 2)\n","\n","        # 클래스 중 일부를 무작위로 선택하여 random_indices에 저장\n","        random_indices = torch.randperm(len(cls))[:int(len(cls) * ratio)]\n","\n","        # 선택된 클래스들을 가져와서 region을 생성\n","        selected_cls = cls[random_indices]\n","        region = torch.zeros_like(ys)[0]\n","\n","        # 선택된 클래스들에 해당하는 데이터가 있는지 확인하고 region 업데이트\n","        for j in selected_cls:\n","            region = (region | (ys[i] == j))\n","\n","        # xm 및 ym을 조작하여 데이터를 혼합\n","        xm[i] = xm[i] * (1 - region) + xs[i] * region\n","        ym[i] = ym[i] * (1 - region) + ys_one_hot[i] * region\n","\n","    # 조작된 데이터 xm과 ym 반환\n","    return xm, ym\n","\n","'''\n","클래스, 비율 별 선택된 클래스 수\n","    0.5\t0.3\t0.25\t0.2\t0.1\n","13\t6\t3\t3       2\t1\n","12\t6\t3\t3\t    2\t1\n","11\t5\t3\t2\t    2\t1\n","10\t5\t3\t2\t    2\t1\n","9\t4\t2\t2\t    1\t0\n","8\t4\t2\t2\t    1\t0\n","7\t3\t2\t1\t    1\t0\n","6\t3\t1\t1\t    1\t0\n","5\t2\t1\t1\t    1\t0\n","4\t2\t1\t1\t    0\t0\n","3\t1\t0\t0\t    0\t0\n","2\t1\t0\t0\t    0\t0\n","1\t0\t0\t0\t    0\t0\n","0\t0\t0\t0\t    0\t0\n","\n","'''"]},{"cell_type":"markdown","id":"w2CkKt2XDhKX","metadata":{"id":"w2CkKt2XDhKX"},"source":["## MIC\n"]},{"cell_type":"code","execution_count":null,"id":"N8F6bWh8Dirl","metadata":{"id":"N8F6bWh8Dirl"},"outputs":[],"source":["# 입력 데이터에 마스킹을 적용하여 일부 영역을 가릴 수 있는 masking 함수\n","def masking(input, mask_ratio=0.5, mask_size=32):\n","    # 입력 데이터의 크기와 형태를 가져옵니다.\n","    b, c, h, w = input.shape\n","\n","    # 입력 이미지를 분할하기 위한 패치의 수를 계산합니다.\n","    h_patch = h // mask_size\n","    w_patch = w // mask_size\n","\n","    # 각 패치에 대한 마스크를 무작위로 생성하고, 주어진 비율(mask_ratio)에 따라 값을 설정합니다.\n","    mask = (np.random.uniform(0, 1, (h_patch, w_patch, b)) > mask_ratio).astype(np.uint8())\n","\n","    # 마스크 크기를 입력 이미지 크기로 다시 조정합니다.\n","    mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n","\n","    # Numpy 배열을 PyTorch 텐서로 변환하고, 차원을 조정하여 입력 이미지와 호환되도록 합니다.\n","    mask = torch.from_numpy(mask).permute(2, 0, 1).reshape(b, 1, h, w).to(device)\n","\n","    # 입력 이미지를 복제하고, 마스크를 곱하여 일부 영역을 가립니다.\n","    output = input.detach().clone() * mask\n","\n","    return output"]},{"cell_type":"markdown","id":"7M54zM17bUc0","metadata":{"id":"7M54zM17bUc0"},"source":["## Slice Pred"]},{"cell_type":"code","execution_count":null,"id":"QIq9hxHubYGg","metadata":{"id":"QIq9hxHubYGg"},"outputs":[],"source":["def slide_pred(model,img,window_size=(CROP_HEIGHT,CROP_WIDTH),stride=(CROP_HEIGHT,CROP_WIDTH),softmax=False,padding=False):\n","    #추론 속도를 높이려면 입력 이미지를 나누어 Batch로 만들도록 코드를 재구성\n","    #학습, 추론 과정 모두 Batch 단위로 입력이 들어오기 때문에 메모리 문제로 인해 Batch로 만들지 못함\n","    #실제 적용에서는 무조건 Batch가 1이기 때문에 이미지 하나를 Batch로 만들 수 있음\n","    b, c, h, w = img.shape\n","\n","    if padding:\n","        # 입력 이미지 주변에 패딩을 추가합니다.\n","        padded_img = torch.zeros((b, c, h + (window_size[0] - stride[0]) * 2, w + (window_size[1] - stride[1]) * 2)).to(device)\n","        padded_img[:, :, window_size[0] - stride[0]:-(window_size[0] - stride[0]), window_size[1] - stride[1]:-(window_size[1] - stride[1])] = img\n","        output = torch.zeros((b, 14, h + (window_size[0] - stride[0]) * 2, w + (window_size[1] - stride[1]) * 2)).to(device)\n","    else:\n","        padded_img = img\n","        output = torch.zeros((b, 14, h, w)).to(device)\n","\n","    ph, pw = padded_img.shape[2:]\n","\n","    for i in range(0, ph - window_size[0] + 1, stride[0]):\n","        for j in range(0, pw - window_size[1] + 1, stride[1]):\n","            # 이미지를 슬라이딩하면서 작은 윈도우를 추출\n","            input = padded_img[:, :, i:i+window_size[0], j:j+window_size[1]]\n","\n","            if softmax:\n","                # 모델을 사용하여 입력 이미지를 추론하고 소프트맥스 적용\n","                pred = torch.softmax(model(input)['out'], 1)\n","            else:\n","                # 모델을 사용하여 입력 이미지를 추론\n","                pred = model(input)['out']\n","\n","            # 결과를 누적하여 output에 추가\n","            output[:, :13, i:i+window_size[0], j:j+window_size[1]] += pred\n","            output[:, 13, i:i+window_size[0], j:j+window_size[1]] += 1\n","\n","    # 결과를 누적한 횟수로 나누어 평균을 구함\n","    output = output[:, :13] / output[:, 13:]\n","\n","    if padding:\n","        # 패딩을 제거하고 원래 이미지 크기로 결과를 자름\n","        output = output[:, :, window_size[0] - stride[0]:-(window_size[0] - stride[0]), window_size[1] - stride[1]:-(window_size[1] - stride[1])]\n","\n","    return output\n"]},{"cell_type":"markdown","id":"a0895765-fba0-4fd9-b955-a6c0e43012e9","metadata":{"id":"a0895765-fba0-4fd9-b955-a6c0e43012e9"},"source":["# 학습"]},{"cell_type":"code","execution_count":null,"id":"D6EvJ8TtKyIE","metadata":{"id":"D6EvJ8TtKyIE"},"outputs":[],"source":["def seed_everything(seed: int = 32):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)  # type: ignore\n","    torch.backends.cudnn.deterministic = True  # type: ignore\n","    torch.backends.cudnn.benchmark = True  # type: ignore\n","seed_everything(32)"]},{"cell_type":"code","execution_count":null,"id":"9e9544c0","metadata":{"id":"9e9544c0"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","\n","#Deeplab v2\n","#model = deeplabv2(torchvision.models.segmentation.deeplabv3_resnet101(num_classes=13,weights_backbone= 'ResNet101_Weights.IMAGENET1K_V1').backbone )\n","#teacher_model = deeplabv2(torchvision.models.segmentation.deeplabv3_resnet101(num_classes=13,weights_backbone= 'ResNet101_Weights.IMAGENET1K_V1').backbone)\n","\n","#Deeplab v3+ ImageNet Pretrain\n","#model = torchvision.models.segmentation.deeplabv3_resnet101(num_classes=13, weights_backbone= 'ResNet101_Weights.IMAGENET1K_V1')\n","#teacher_model = torchvision.models.segmentation.deeplabv3_resnet101(num_classes=13, weights_backbone= 'ResNet101_Weights.IMAGENET1K_V1')\n","\n","#Deeplab v3+ CityScape Pretrain (https://github.com/VainF/DeepLabV3Plus-Pytorch)\n","#model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=13, output_stride=8)\n","#teacher_model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=13, output_stride=8)\n","#state_dict= torch.load(root_path + '/pretrain/best_deeplabv3plus_resnet101_cityscapes_os16.pth')['model_state']\n","#filtered_dict = {key: value for key, value in state_dict.items() if 'backbone' in key} #encoder만 가져오기\n","#model.load_state_dict(filtered_dict,strict=False )\n","\n","#VGG UNET\n","#model = UNet(torchvision.models.vgg13_bn(weights= 'IMAGENET1K_V1'),512)\n","#teacher_model = UNet(torchvision.models.vgg13_bn(weights= 'IMAGENET1K_V1'),512)\n","\n","## Res UNET\n","#model = ResUNet(torchvision.models.resnet101(weights= 'ResNet101_Weights.IMAGENET1K_V1'),2048)\n","#teacher_model = ResUNet(torchvision.models.resnet101(weights= 'ResNet101_Weights.IMAGENET1K_V1'),2048)\n","\n","## DaFormer (Weight from Segformer : https://github.com/NVlabs/SegFormer)\n","# b3,b4,b5의 선택지가 있음\n","model = daformer(mit_b4(),daformerhead())\n","model.load_state_dict(torch.load(f'/content/drive/MyDrive/samsung_seg(resize)/pretrain/segformer.b4.1024x1024.city.160k.pth')['state_dict'],strict=False)\n","teacher_model = daformer(mit_b4(),daformerhead())\n","\n","learning_status = {\n","    'train_accs' : [],\n","    'valid_accs' : [],\n","    'train_ious' : [],\n","    'valid_ious' : [],\n","    'train_losses' : [],\n","    'valid_losses' : [],\n","    'lrs' : []\n","}\n","min_epoch = 0\n","\n","optimizer = torch.optim.AdamW([\n","    {'params': model.backbone.parameters(), 'lr': 1e-4},\n","     {'params': model.classifier.parameters(), 'lr': 1e-3}], weight_decay=0.01)\n","\n","save_last = True\n","\n","# Warmup 파라미터\n","warmup_epochs = 50\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,last_epoch=len(learning_status['lrs'])-1, lr_lambda=lambda epoch: epoch / warmup_epochs)"]},{"cell_type":"code","execution_count":null,"id":"9c22dcb0","metadata":{"id":"9c22dcb0"},"outputs":[],"source":["BATCH_SIZE = 8\n","\n","train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=12)\n","valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=12)"]},{"cell_type":"code","execution_count":null,"id":"5c1b1900","metadata":{"id":"5c1b1900"},"outputs":[],"source":["model_save_path = root_path + '/model/DAformer/'\n","\n","model = model.to(device)\n","teacher_model = teacher_model.to(device)\n","LS = nn.CrossEntropyLoss()\n","LT = target_loss(t=0.968)\n","\n","epochs = 100\n","\n","#EMA 파라미터\n","alpha = 0.9\n","\n","#Mask 파라미터\n","masking_ratio = 0.75\n","\n","mask_size = 32\n","\n","lambda_mask = 1"]},{"cell_type":"code","execution_count":null,"id":"4060e94c","metadata":{"id":"4060e94c"},"outputs":[],"source":["\n","def train_begin(training, loader, running_loss, running_acc, running_iou):\n","    # 학습 또는 검증 모드에 따라 진행 상태 문자열(desc) 설정\n","    if training:\n","        desc = \"Train\"\n","    else:\n","        desc = \"Valid\"\n","\n","    # tqdm을 사용하여 진행 상황을 시각화하는 진행 표시 준비\n","    progress = tqdm.tqdm(loader, desc=f'Epoch:{epoch+1}/{epochs}')\n","\n","    # 데이터 로더에서 미니배치를 가져와서 반복 처리\n","    for i, data in enumerate(progress):\n","        source_img, source_mask, target_img, distortion_img, distortion_mask = data\n","\n","        # 데이터를 GPU로 이동\n","        source_img = source_img.to(device)\n","        source_mask = source_mask.to(device).long()\n","\n","        # 손실 변수 초기화\n","        loss_mask = torch.tensor(0.0).to(device)\n","        loss_distortion = torch.tensor(0.0).to(device)\n","\n","        # 훈련 모드(training)인 경우\n","        if (training):\n","            # 옵티마이저의 그래디언트 초기화\n","            optimizer.zero_grad()\n","\n","            # 대상 이미지와 왜곡 이미지를 GPU로 이동\n","            target_img = target_img.to(device)\n","            distortion_img = distortion_img.to(device)\n","            distortion_mask = distortion_mask.to(device).long()\n","\n","            # 가짜 레이블 생성 (티처 모델 활용)\n","            with torch.no_grad():\n","                pseudo_label = teacher_model(target_img)['out']\n","                pseudo_label = torch.softmax(pseudo_label, dim=1)  # b,c,h,w\n","\n","\n","\n","            #DACS 기법 : 이미지별 일부 클래스 섞기 ( 학습 에폭이 큰 경우에만 잘 작동하는 것으로 보이기에 삭제 )\n","            '''mixed_target_img,mixed_pseudo_label = dacs(source_img,source_mask,target_img,pseudo_label,mix_ratio)\n","\n","            pred_mixed_target = model(mixed_target_img)['out']\n","\n","            loss_mix = LT(pred_mixed_target,mixed_pseudo_label) * lambda_mix'''\n","\n","\n","            #Distortion loss\n","            ## 왜곡 손실 계산\n","            pred_distortion = model(distortion_img)['out']\n","\n","            loss_distortion = LS(pred_distortion,distortion_mask)\n","\n","\n","            # MIC 기법 : 패치 일부 masking\n","            target_img = masking(target_img,masking_ratio,mask_size)\n","            \n","            ## 대상 이미지의 예측과 가짜 레이블 간 손실 계산\n","            pred_target = model(target_img)['out']\n","\n","            loss_mask = LT(pred_target,pseudo_label) * lambda_mask\n","\n","\n","        # 원본 이미지에 대한 예측 계산\n","        pred_source = model(source_img)['out']\n","\n","        # 원본 이미지와 레이블 간 손실 계산\n","        loss_source = LS(pred_source,source_mask)\n","\n","        # 총 손실 계산\n","        loss_total = loss_source + loss_distortion + loss_mask\n","        # 정확도 및 IoU(Intersection over Union) 계산\n","        acc = accuracy(pred_source,source_mask)\n","\n","        iou = miou(pred_source,source_mask)\n","\n","        # 훈련(training) 모드인 경우 그래디언트 역전파 및 파라미터 업데이트\n","        if (training):\n","\n","            loss_total.backward()\n","\n","            optimizer.step()\n","        # 손실, 정확도 및 IoU 결과 저장\n","        running_loss += [loss_source.detach().cpu().numpy(),loss_distortion.detach().cpu().numpy(),loss_mask.detach().cpu().numpy()]\n","        running_acc += [acc.cpu()]\n","        running_iou += [iou.cpu()]\n","        # 진행 표시 업데이트\n","        progress.set_description(f'Epoch:{epoch+1}/{epochs} | {desc}_Acc:{np.round(running_acc/(i+1),4)} | {desc}_IoU:{np.round(running_iou/(i+1),4)} | {desc}_Loss:{np.round(running_loss/(i+1),4)} | Self-Training:{running_loss[-1]>0}')"]},{"cell_type":"code","execution_count":null,"id":"6340a621","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":23384592,"status":"error","timestamp":1696193292377,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"6340a621","outputId":"df15c7b9-5e52-494a-f29e-41ce46c28990"},"outputs":[],"source":["\n","\n","print(\"모델 저장 경로 : \"+ model_save_path)\n","fit_time = time.time()\n","start_epoch = len(learning_status['lrs'])\n","teacher_model.eval()\n","\n","\n","for i in range(len(learning_status['valid_accs'])):\n","    print(f\"LR : {learning_status['lrs'][i]}\")\n","    print(f\"Epoch:{i+1}/{epochs} | Train_Acc : {np.round(learning_status['train_accs'][i],4)} | Train_IoU : {np.round(learning_status['train_ious'][i],4)} | Train_Loss : {np.round(learning_status['train_losses'][i],4)}\")\n","    print(f\"Epoch:{i+1}/{epochs} | Valid_Acc : {np.round(learning_status['valid_accs'][i],4)} | Valid_IoU : {np.round(learning_status['valid_ious'][i],4)} | Valid_Loss : {np.round(learning_status['valid_losses'][i],4)}\")\n","    print()\n","\n","for epoch in range(start_epoch,epochs):\n","    print(\"모델명 :\", model_save_path.split('/')[-2])\n","\n","    #Warmup Schedule\n","    if epoch < warmup_epochs :\n","        scheduler.step()\n","        print(\"lr이 변경되었습니다.\",optimizer.param_groups[0]['lr'])\n","\n","    #EMA Update\n","    alpha_teacher = min(1 - 1 / (epoch + 1), alpha)\n","    for ema_param, param in zip(teacher_model.parameters(), model.parameters()):\n","        ema_param.data = alpha_teacher * ema_param.data + (1 - alpha_teacher) * param.data\n","\n","    print(\"EMA Weight Update 적용, Alpha =\",alpha_teacher)\n","\n","    running_train_loss = np.array([0.0,0.0,0.0])\n","    running_valid_loss = np.array([0.0,0.0,0.0])\n","\n","    running_train_acc = np.array([0.0])\n","    running_valid_acc = np.array([0.0])\n","\n","    running_train_iou = np.array([0.0])\n","    running_valid_iou = np.array([0.0])\n","\n","    model.train()\n","    train_begin(True,train_loader,running_train_loss,running_train_acc,running_train_iou)\n","    model.eval()\n","    '''\n","    with torch.no_grad():\n","        train_begin(False,valid_loader,running_valid_loss,running_valid_acc,running_valid_iou)\n","        '''\n","\n","    if (os.path.exists(model_save_path+'self_training/')==False):\n","        os.makedirs(model_save_path+'self_training/',exist_ok=True)\n","\n","    learning_status['train_losses'].append((running_train_loss/len(train_loader)))\n","    learning_status['valid_losses'].append((running_valid_loss/len(valid_loader)))\n","    learning_status['train_accs'].append((running_train_acc/len(train_loader)))\n","    learning_status['valid_accs'].append((running_valid_acc/len(valid_loader)))\n","    learning_status['train_ious'].append((running_train_iou/len(train_loader)))\n","    learning_status['valid_ious'].append((running_valid_iou/len(valid_loader)))\n","    learning_status['lrs'].append(optimizer.param_groups[0]['lr'])\n","\n","    df = pd.DataFrame(learning_status)\n","\n","    checkpoint = {\n","        'epoch': epoch+1 , #에폭\n","        'model': model.state_dict(),  # 모델\n","        'teacher_model': teacher_model.state_dict(), # Teacher 모델\n","        'optimizer': optimizer.state_dict(),  # 옵티마이저\n","        'scheduler': scheduler.state_dict(),  # 스케줄러\n","    }\n","\n","    if learning_status['train_losses'][-1][-1] >0:\n","        torch.save(checkpoint, model_save_path+f'self_training/Last.pth')\n","        if (epoch+1)%5 == 0:\n","            torch.save(checkpoint, model_save_path+f'self_training/Epoch({epoch+1:03d}).pth')\n","\n","        df.to_csv(model_save_path+'self_training/status.csv', index=True)\n","        save_last = False\n","\n","    elif save_last:\n","\n","        torch.save(checkpoint, model_save_path+f'Last.pth')\n","        df.to_csv(model_save_path+'status.csv', index=True)\n","\n","\n","    if sum(learning_status['valid_losses'][min_epoch]) >= sum(learning_status['valid_losses'][-1]) and sum(learning_status['valid_losses'][-1] > 0):\n","        print(f\"Valid Loss가 최소가 됐습니다. ({sum(learning_status['valid_losses'][min_epoch]):.4f}({min_epoch+1}) -> {sum(learning_status['valid_losses'][-1]):.4f}({len(learning_status['valid_losses'])}))\")\n","        print(f'해당 모델이 {model_save_path}Best.pth 경로에 저장됩니다.')\n","        min_epoch = len(learning_status['valid_losses'])-1\n","        torch.save(checkpoint, model_save_path+f'Best.pth')\n","    else:\n","        print(f\"Valid_Loss가 최소가 되지 못했습니다.(최소 Epoch:{min_epoch+1} : {sum(learning_status['valid_losses'][min_epoch]):.4f})\")\n","\n","    print('')\n","\n","\n","    #한 에폭 마무리마다 Target 데이터 중의 일부에 대해서 추론한 결과 저장\n","    with torch.no_grad():\n","        for path in [f'{data_path}/train_target_image/TRAIN_TARGET_0000.png',\n","                     f'{data_path}/train_target_image/TRAIN_TARGET_0002.png',\n","                     f'{data_path}/train_target_image/TRAIN_TARGET_0032.png',\n","                     f'{data_path}/train_target_image/TRAIN_TARGET_0650.png']:\n","\n","\n","\n","            img = cv2.imread(path)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            h,w,c = img.shape\n","            learning_type = path.split('/')[-1].split('_')[0]\n","\n","            if learning_type == \"VALID\":\n","                img = A.Normalize(mean = source_mean , std = source_std)(image=img)['image']\n","            else:\n","                img = A.Normalize(mean = target_mean , std = target_std)(image=img)['image']\n","\n","            img = ToTensorV2()(image=img)['image']\n","\n","            pred = slide_pred(model,img.unsqueeze(0).to(device),stride=(CROP_HEIGHT//2,CROP_WIDTH//2), softmax=True, padding=False)\n","\n","            pred = torch.argmax(pred,1)[0].detach().cpu().numpy()\n","\n","            img = (denormalization(np.transpose(img,(1,2,0)).numpy(),learning_type == \"VALID\")*255).astype(np.uint8)\n","\n","            color_pred = np.zeros_like(img,dtype=np.uint8)\n","\n","            for i,color in enumerate(palette):\n","                color_pred[pred==i] = color\n","\n","            os.makedirs(f\"{model_save_path}pred/{path.split('/')[-1].split('.')[0]}/\",exist_ok=True)\n","            os.makedirs(f\"{model_save_path}mix/{path.split('/')[-1].split('.')[0]}/\",exist_ok=True)\n","\n","            cv2.imwrite(f\"{model_save_path}pred/{path.split('/')[-1].split('.')[0]}/{epoch+1:03d}.png\",cv2.cvtColor(color_pred,cv2.COLOR_RGB2BGR))\n","            cv2.imwrite(f\"{model_save_path}mix/{path.split('/')[-1].split('.')[0]}/{epoch+1:03d}.png\", cv2.cvtColor(cv2.addWeighted(img, 0.5, color_pred  , 0.5, 0),cv2.COLOR_RGB2BGR))\n","\n","\n","\n","print('학습 최종 시간: {:.2f} 분\\n' .format((time.time()- fit_time)/60))\n"]},{"cell_type":"markdown","id":"73Eu7AazztA4","metadata":{"id":"73Eu7AazztA4"},"source":["# 이전 학습 상황 불러오기"]},{"cell_type":"code","execution_count":null,"id":"lG5BXk_JKyk0","metadata":{"id":"lG5BXk_JKyk0"},"outputs":[],"source":["f\"\\'/model/{model_save_path.split('/')[-2]}/\\'\""]},{"cell_type":"code","execution_count":null,"id":"d01bb7ef","metadata":{"id":"d01bb7ef"},"outputs":[],"source":["torch.cuda.empty_cache()\n","model_load_path = root_path + '/model/DAformer(addfisheye)/'\n","\n","target_epoch = 56\n","\n","\n","if os.path.exists(model_load_path+f'self_training/Epoch({target_epoch:03d}).pth'): #Self-Training을 시작한 이후 특정 에폭에 대한 모델 가중치가 존재하는 경우\n","    learning_status = pd.read_csv(model_load_path+f'self_training/status.csv',index_col=0).to_dict(orient='list')\n","    checkpoint = torch.load(model_load_path+f'self_training/Epoch({target_epoch:03d}).pth','cuda')\n","    print(1)\n","elif target_epoch > 0 and os.path.exists(model_load_path+f'self_training/Last.pth'): #Self-Training을 시작한 이후 특정 에폭에 대한 모델 가중치가 존재하지 않는 경우\n","    learning_status = pd.read_csv(model_load_path+f'self_training/status.csv',index_col=0).to_dict(orient='list')\n","    checkpoint = torch.load(model_load_path+f'self_training/Last.pth','cuda')\n","    print(2)\n","else: #Self-Training을 시작하기 전\n","    learning_status = pd.read_csv(model_load_path+f'self_training/status.csv',index_col=0).to_dict(orient='list')\n","    checkpoint = torch.load(model_load_path+f'Last.pth','cuda')\n","    print(3)\n","\n","model.to(device)\n","\n","model.load_state_dict(checkpoint['model'])\n","teacher_model.load_state_dict(checkpoint['teacher_model'])\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","scheduler.load_state_dict(checkpoint['scheduler'])\n","\n","save_last=not(os.path.exists(model_load_path+f'self_training/Epoch({target_epoch:03d}).pth'))\n","\n","for k in learning_status.keys():\n","    for i in range(len(learning_status[k])):\n","        if k!='lrs':\n","            learning_status[k][i] = np.array([item for item in learning_status[k][i][1:-1].split(' ') if item != ''],dtype=np.float32)\n","\n","\n","\n","learning_status = {\n","    'train_accs': learning_status['train_accs'][:checkpoint['epoch']],\n","    'valid_accs': learning_status['valid_accs'][:checkpoint['epoch']],\n","    'train_ious': learning_status['train_ious'][:checkpoint['epoch']],\n","    'valid_ious': learning_status['valid_ious'][:checkpoint['epoch']],\n","    'train_losses': learning_status['train_losses'][:checkpoint['epoch']],\n","    'valid_losses': learning_status['valid_losses'][:checkpoint['epoch']],\n","    'lrs': learning_status['lrs'][:checkpoint['epoch']]\n","}\n","\n","min_epoch = np.argmin(np.sum(learning_status['valid_losses'],-1))\n","\n","for i in range(len(learning_status['valid_accs'])):\n","    print(f\"Epoch:{i+1} | Train_Acc : {np.round(learning_status['train_accs'][i],4)} | Train_IoU : {np.round(learning_status['train_ious'][i],4)} | Train_Loss : {np.round(learning_status['train_losses'][i],4)}\")\n","    print(f\"Epoch:{i+1} | Valid_Acc : {np.round(learning_status['valid_accs'][i],4)} | Valid_IoU : {np.round(learning_status['valid_ious'][i],4)} | Valid_Loss : {np.round(learning_status['valid_losses'][i],4)}\")\n","    print()\n","\n","print(len(learning_status['lrs']),min_epoch+1,learning_status['train_accs'][-1],learning_status['train_losses'][-1],learning_status['valid_accs'][-1],learning_status['valid_losses'][-1],learning_status['lrs'][-1])\n","print(len(learning_status['lrs']),min_epoch+1,learning_status['train_accs'][min_epoch],learning_status['train_losses'][min_epoch],learning_status['valid_accs'][min_epoch],learning_status['valid_losses'][min_epoch],learning_status['lrs'][min_epoch])"]},{"cell_type":"markdown","id":"5y8mlgXVbFMw","metadata":{"id":"5y8mlgXVbFMw"},"source":["#모델비교\n","\n"]},{"cell_type":"code","execution_count":null,"id":"sdd7273gbFMx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1696133244337,"user":{"displayName":"김태훈","userId":"11149993911483967797"},"user_tz":-540},"id":"sdd7273gbFMx","outputId":"df254781-cc39-45a0-9988-3bb9581cce0b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["target_dataset = CustomDataset(csv_file=f'{root_path}/train_target.csv', infer=True)\n","target_dataloader = DataLoader(target_dataset, batch_size=16, shuffle=False, num_workers=16)"]},{"cell_type":"code","execution_count":null,"id":"lGHGXzlDbFMx","metadata":{"id":"lGHGXzlDbFMx"},"outputs":[],"source":["model_list = [daformer(mit_b5(),daformerhead()),\n","              daformer(mit_b5(),daformerhead())]\n","state_dict = [torch.load('/content/drive/MyDrive/samsung_seg(512,1024)/model/DAformer(B5,crop2x,32seed,valid2x,1e-4)/self_training/Epoch(040).pth','cuda'),\n","              torch.load('/content/drive/MyDrive/samsung_seg(512,1024)/model/DAformer(B5,crop2x,32seed,valid2x,1e-4)/self_training/Epoch(060).pth','cuda')]\n","\n","for i in range(len(model_list)):\n","    model_list[i].load_state_dict(state_dict[i]['model'])\n","    model_list[i].to(device)\n","    model_list[i].eval()"]},{"cell_type":"code","execution_count":null,"id":"PqHwendnbFMx","metadata":{"id":"PqHwendnbFMx"},"outputs":[],"source":["\n","with torch.no_grad():\n","    result = []\n","    for i,imgs in enumerate(tqdm.tqdm(target_dataloader)):\n","\n","        input = imgs.float().to(device)\n","        preds_list = []\n","        for model in model_list:\n","            preds_list.append(torch.argmax(slide_pred(model,input,stride=(CROP_HEIGHT//2,CROP_WIDTH//2),softmax = True ,padding=False) ,1).detach().cpu().numpy())\n","\n","        if i < 20 :\n","            for j in range(16):\n","                if j>=5:\n","                    continue\n","\n","                img = (denormalization(np.transpose(imgs[j].cpu().numpy(),(1,2,0)),False)*255).astype(np.uint8)\n","\n","                plt.figure(figsize=(30,15))\n","\n","                plt.subplot(1,1+len(preds_list),1)\n","                plt.imshow(img)\n","                for k in range(len(preds_list)):\n","                    color_pred = np.zeros_like(img,dtype=np.uint8)\n","\n","\n","                    for l,color in enumerate(palette):\n","                        color_pred[preds_list[k][j]==l] = color\n","\n","\n","                    plt.subplot(1,1+len(preds_list),k+2)\n","                    plt.imshow(color_pred)\n","\n","                plt.show()\n","\n","                plt.figure(figsize=(30,15))\n","\n","                plt.subplot(1,1+len(preds_list),1)\n","                plt.imshow(img)\n","                for k in range(len(preds_list)):\n","                    color_pred = np.zeros_like(img,dtype=np.uint8)\n","\n","\n","                    for l,color in enumerate(palette):\n","                        color_pred[preds_list[k][j]==l] = color\n","\n","\n","                    plt.subplot(1,1+len(preds_list),k+2)\n","                    plt.imshow(cv2.addWeighted(img, 0.5, color_pred  , 0.5, 0))\n","                plt.show()\n","\n","\n"]},{"cell_type":"markdown","id":"c32eb51c-a3fe-4e11-a616-3a717ba16f7e","metadata":{"id":"c32eb51c-a3fe-4e11-a616-3a717ba16f7e"},"source":["# 추론"]},{"cell_type":"code","execution_count":null,"id":"NXfP7o62Buz9","metadata":{"id":"NXfP7o62Buz9"},"outputs":[],"source":["def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"cell_type":"code","execution_count":null,"id":"12371c8b-0c78-47df-89ec-2d8b55c8ea94","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1695799071823,"user":{"displayName":"김태훈","userId":"08889369219282378844"},"user_tz":-540},"id":"12371c8b-0c78-47df-89ec-2d8b55c8ea94","outputId":"11188a23-fd83-4b10-91a9-d9c00465b5d9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["test_set = CustomDataset(csv_file=f'{root_path}/test.csv', infer=True)\n","test_loader = DataLoader(test_set, batch_size=16, shuffle=False, num_workers=16)"]},{"cell_type":"code","execution_count":null,"id":"i5V8FjGoBhYW","metadata":{"id":"i5V8FjGoBhYW"},"outputs":[],"source":["model = daformer(mit_b5(),daformerhead())"]},{"cell_type":"code","execution_count":null,"id":"Twm8CAMlNLRh","metadata":{"colab":{"background_save":true},"id":"Twm8CAMlNLRh"},"outputs":[],"source":["\n","state_dict = torch.load('/content/drive/MyDrive/samsung_seg(512,1024)/model/DAformer(B5,crop2x,32seed,valid2x,1e-4)/self_training/Epoch(060).pth','cuda')\n","model.load_state_dict(state_dict['model'])\n","print(state_dict['epoch'])"]},{"cell_type":"code","execution_count":null,"id":"uugvJh7PLInQ","metadata":{"id":"uugvJh7PLInQ"},"outputs":[],"source":["print(state_dict['epoch'])\n","model.to(device)\n","model.eval()\n","with torch.no_grad():\n","    result = []\n","    for i,imgs in enumerate(tqdm.tqdm(test_loader)):\n","        b,c,h,w = imgs.shape\n","\n","        input = imgs.float().to(device)\n","\n","        #preds = model(input)['out'] #전체 이미지를 활용한 모델은 이 코드 사용\n","        preds = slide_pred(model,input,stride=(CROP_HEIGHT//2,CROP_WIDTH//2),softmax = True ,padding=False) # b,c,h,w\n","\n","\n","        original_mask = torch.from_numpy(original_mask).reshape(1,h,w).repeat(b,1,1)\n","        pred_original = torch.argmax(preds,1).cpu().numpy() # b,c,h,w -> b,h,w\n","\n","\n","\n","        pred_resize = F.interpolate(preds, size=(540,960),mode='bilinear',align_corners=False)\n","        pred_resize = torch.argmax(pred_resize,1).cpu().numpy() # b,c,h,w -> b,h,w\n","\n","\n","        #타원을 제외한 위치에 있는 값들 후처리\n","        center = (960//2, int(540*0.375))  # x,y\n","        axis_length = (960//2, int(540*0.64))  # 장축 반지름과 단축 반지름\n","        resize_mask = np.ones((540,960),dtype=np.uint8)\n","        cv2.ellipse(resize_mask, center, axis_length, 0, 0, 360, 0, -1)\n","        resize_mask = torch.from_numpy(resize_mask).unsqueeze(0).repeat(b,1,1)\n","\n","        pred_resize[resize_mask==1] = 12\n","\n","\n","        if i < 5 :\n","            for j,pred in enumerate(pred_original):\n","                if j<5:\n","                    print(i*16+j)\n","                    img = (denormalization(np.transpose(imgs[j].cpu().numpy(),(1,2,0)),False)*255).astype(np.uint8)\n","                    color_pred = np.zeros_like(img,dtype=np.uint8)\n","                    color_resize_pred = np.zeros((540,960,3),dtype=np.uint8)\n","\n","\n","                    for k,color in enumerate(palette):\n","                        color_pred[pred==k] = color\n","                        color_resize_pred[pred_resize[j]==k] = color\n","\n","\n","                    plt.figure(figsize=(30,15))\n","                    plt.subplot(1,4,1)\n","                    plt.imshow(img)\n","\n","\n","                    plt.subplot(1,4,2)\n","                    plt.imshow(cv2.addWeighted(img, 0.5, color_pred  , 0.5, 0))\n","\n","\n","                    plt.subplot(1,4,3)\n","                    plt.imshow(color_pred)\n","\n","                    plt.subplot(1,4,4)\n","                    plt.imshow(color_resize_pred)\n","\n","\n","                    plt.show()\n","\n","        for j,pred in enumerate(pred_resize):\n","            for class_id in range(12):\n","                class_mask = (pred == class_id).astype(np.uint8)\n","                if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n","                    mask_rle = rle_encode(class_mask)\n","                    result.append(mask_rle)\n","                else: # 마스크가 존재하지 않는 경우 -1\n","                    result.append(-1)"]},{"cell_type":"code","execution_count":null,"id":"zHnokgkjDRAc","metadata":{"id":"zHnokgkjDRAc"},"outputs":[],"source":["print(state_dict['epoch'])\n","submit = pd.read_csv(f'{root_path}/sample_submission.csv')\n","submit['mask_rle'] = result\n","submit.to_csv(f'{root_path}/baseline_submit.csv', index=False)\n","submit.head()"]},{"cell_type":"markdown","id":"cMVHmV4jPM1h","metadata":{"id":"cMVHmV4jPM1h"},"source":["#앙상블\n"]},{"cell_type":"code","execution_count":null,"id":"dNWNDcDTRZRG","metadata":{"id":"dNWNDcDTRZRG"},"outputs":[],"source":["def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"cell_type":"code","execution_count":null,"id":"FBmHjmtxPoh6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696199828759,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"FBmHjmtxPoh6","outputId":"753e4c79-7eeb-4e6b-81a5-1a884f32ab17"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["test_dataset = CustomDataset(csv_file=f'{root_path}/test.csv', infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=16)"]},{"cell_type":"code","execution_count":null,"id":"C1h-8p8_PW6s","metadata":{"id":"C1h-8p8_PW6s"},"outputs":[],"source":["torch.cuda.empty_cache()\n","model_list = [daformer(mit_b3(),daformerhead()),\n","              daformer(mit_b4(),daformerhead()),\n","              daformer(mit_b5(),daformerhead())]\n","\n","state_dict = [torch.load('/content/drive/MyDrive/samsung_seg(512,1024)/model/DAformer(B3,crop2x,32seed,valid2x,1e-4)/self_training/Epoch(065).pth','cuda'),\n","              torch.load('/content/drive/MyDrive/samsung_seg(512,1024)/model/DAformer(B4,crop2x,32seed,valid2x,1e-4)/self_training/Epoch(065).pth','cuda'),\n","              torch.load('/content/drive/MyDrive/samsung_seg(512,1024)/model/DAformer(B5,crop2x,32seed,valid2x,1e-4)/self_training/Epoch(060).pth','cuda')]\n","\n","weight_list = torch.tensor([0.9 ,1.0 ,0.8]).reshape(len(model_list),1,1,1,1).to(device)\n","\n","for i in range(len(model_list)):\n","    model_list[i].load_state_dict(state_dict[i]['model'])\n","    model_list[i].to(device)\n","    model_list[i].eval()"]},{"cell_type":"code","execution_count":null,"id":"6tKf6BcAPeX-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":535714,"status":"ok","timestamp":1696200683900,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"6tKf6BcAPeX-","outputId":"6088ccd4-1dde-4fef-d334-84516eec6f53"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 119/119 [08:55<00:00,  4.50s/it]\n"]}],"source":["\n","with torch.no_grad():\n","    result = []\n","    for i,imgs in enumerate(tqdm.tqdm(test_dataloader)):\n","        b,c,h,w = imgs.shape\n","\n","        input = imgs.float().to(device)\n","        preds_list = []\n","        for model in model_list:\n","            preds_list.append(slide_pred(model,input,stride=(CROP_HEIGHT//2,CROP_WIDTH//2),softmax = True ,padding=False))\n","\n","        preds_list = torch.stack(preds_list) # len(model), b,c,h,w\n","        ensemble = torch.sum(preds_list*weight_list,0).unsqueeze(0) # 1,b,c,h,w\n","\n","        preds = torch.cat([preds_list,ensemble],0)\n","\n","\n","        pred_original = torch.argmax(preds,2).cpu().numpy() # len(model), b,c,h,w -> len(model),b,h,w\n","\n","        pred_resize = F.interpolate(ensemble.squeeze(0), size=(540,960),mode='bilinear',align_corners=False)\n","        pred_resize = torch.argmax(pred_resize,1).cpu().numpy() # b,c,h,w -> b,h,w\n","\n","\n","\n","        #타원을 제외한 위치에 있는 값들 후처리\n","        center = (960//2, int(540*0.375))  # x,y\n","        axis_length = (960//2, int(540*0.64))  # 장축 반지름과 단축 반지름\n","        resize_mask = np.ones((540,960),dtype=np.uint8)\n","        cv2.ellipse(resize_mask, center, axis_length, 0, 0, 360, 0, -1)\n","        resize_mask = torch.from_numpy(resize_mask).reshape(1,540,960).repeat(b,1,1)\n","        pred_resize[resize_mask==1] = 12\n","\n","        if i<0:\n","\n","            for j in range(5):\n","                img = (denormalization(np.transpose(imgs[j].cpu().numpy(),(1,2,0)),False)*255).astype(np.uint8)\n","\n","                plt.figure(figsize=(30,15))\n","\n","                plt.subplot(1,2+len(model_list),1)\n","                plt.imshow(img)\n","                for k in range(len(model_list)+1):\n","                    color_pred = np.zeros_like(img,dtype=np.uint8)\n","\n","\n","                    for l,color in enumerate(palette):\n","                        color_pred[pred_original[k][j]==l] = color\n","\n","\n","                    plt.subplot(1,2+len(preds_list),k+2)\n","                    plt.imshow(color_pred)\n","\n","                plt.show()\n","\n","                plt.figure(figsize=(30,15))\n","\n","                plt.subplot(1,2+len(model_list),1)\n","                plt.imshow(img)\n","                for k in range(len(model_list)+1):\n","                    color_pred = np.zeros_like(img,dtype=np.uint8)\n","\n","\n","                    for l,color in enumerate(palette):\n","                        color_pred[pred_original[k][j]==l] = color\n","\n","\n","                    plt.subplot(1,2+len(model_list),k+2)\n","                    plt.imshow(cv2.addWeighted(img, 0.5, color_pred  , 0.5, 0))\n","                plt.show()\n","\n","        for j,pred in enumerate(pred_resize):\n","            for class_id in range(12):\n","                class_mask = (pred == class_id).astype(np.uint8)\n","                if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n","                    mask_rle = rle_encode(class_mask)\n","                    result.append(mask_rle)\n","                else: # 마스크가 존재하지 않는 경우 -1\n","                    result.append(-1)"]},{"cell_type":"code","execution_count":null,"id":"oCGzHfNdUMT4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":727,"status":"ok","timestamp":1696200684608,"user":{"displayName":"김태훈 (PreF)","userId":"12262848571047260537"},"user_tz":-540},"id":"oCGzHfNdUMT4","outputId":"c33b7485-87d9-4329-d5a6-b7a46c8f6e6a"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-e6a68842-587a-4ef3-8947-39db99001f1a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>mask_rle</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TEST_0000_class_0</td>\n","      <td>208754 22 208785 9 208798 2 209712 27 209742 2...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TEST_0000_class_1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TEST_0000_class_2</td>\n","      <td>597 274 1557 275 2517 276 3477 277 4436 279 53...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TEST_0000_class_3</td>\n","      <td>205719 14 206634 7 206657 40 207575 86 207754 ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TEST_0000_class_4</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6a68842-587a-4ef3-8947-39db99001f1a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e6a68842-587a-4ef3-8947-39db99001f1a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e6a68842-587a-4ef3-8947-39db99001f1a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-abed690b-9ed1-41f3-9c1e-c26e03c56330\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-abed690b-9ed1-41f3-9c1e-c26e03c56330')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-abed690b-9ed1-41f3-9c1e-c26e03c56330 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                  id                                           mask_rle\n","0  TEST_0000_class_0  208754 22 208785 9 208798 2 209712 27 209742 2...\n","1  TEST_0000_class_1                                                 -1\n","2  TEST_0000_class_2  597 274 1557 275 2517 276 3477 277 4436 279 53...\n","3  TEST_0000_class_3  205719 14 206634 7 206657 40 207575 86 207754 ...\n","4  TEST_0000_class_4                                                 -1"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["\n","submit = pd.read_csv(f'{root_path}/sample_submission.csv')\n","submit['mask_rle'] = result\n","submit.to_csv(f'{root_path}/baseline_submit.csv', index=False)\n","submit.head()"]}],"metadata":{"colab":{"collapsed_sections":["zSHSxxYAYxpy","OymU-K_jboYv","E-wMbc1qSPRf","h5PCUOxl2DBg","be76a29e-e9c2-411a-a569-04166f074184","dc955893-22fd-4320-88be-7aa0d790cbd9","V6tw1MEk55ye","Qc21XLZTalhH","6fE34U9ee1If","D8Acq63We6x9","G7jEIJoFe9i4","pOfAEs8yafEv","z4bbpivqaheE","xTx4SP0CDFym","B0uAomSuDTCO","Ri4cx9QjDacy","EfTRRWPBDf8J","w2CkKt2XDhKX","7M54zM17bUc0","a0895765-fba0-4fd9-b955-a6c0e43012e9","73Eu7AazztA4","5y8mlgXVbFMw","c32eb51c-a3fe-4e11-a616-3a717ba16f7e","cMVHmV4jPM1h"],"provenance":[{"file_id":"1brkM1pnvHa7BizpI_AdFhLRkZelR71nc","timestamp":1696248296876},{"file_id":"1ltNtWByCmjEmOlFOkUkyMsEE7BsGjZeC","timestamp":1696061367146},{"file_id":"1l_kn-xIPVVNuUEfmYarEgnckoZ8IDLvQ","timestamp":1695973273108},{"file_id":"1x2fYCH6izWG5qraIfZWuhaq8hpHDlRZi","timestamp":1693485264162}]},"kernelspec":{"display_name":"torch","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":5}
