{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8tM0qp9KZ1",
      "metadata": {
        "id": "bc8tM0qp9KZ1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iSbFNVImbqP9",
      "metadata": {
        "id": "iSbFNVImbqP9"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9b681e-370a-4cfa-a452-dd2d7f0cd77f",
      "metadata": {
        "id": "ad9b681e-370a-4cfa-a452-dd2d7f0cd77f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2, glob,  time, json ,os,tqdm,random,math, shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.vision_transformer import _cfg\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import os\n",
        "from content.Deeplabv3 import network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cS-UYRKc95ux",
      "metadata": {
        "id": "cS-UYRKc95ux"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/VainF/DeepLabV3Plus-Pytorch.git /content/Deeplabv3\n",
        "from Deeplabv3 import network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E-wMbc1qSPRf",
      "metadata": {
        "id": "E-wMbc1qSPRf"
      },
      "source": [
        "# 데이터 폴더 복사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cr-K2-zkZxKZ",
      "metadata": {
        "id": "cr-K2-zkZxKZ"
      },
      "outputs": [],
      "source": [
        "# Colab에서 장기간 학습 시 Google Drive 연결이 잠깐 끊겨 학습이 중단되는 현상이 발생해 Colab 로컬 드라이브에 복사해두어 학습 진행\n",
        "# Colab 환경이 아닌 경우 root_path 와 data_path를 동일한 값으로 설정하고 이 코드만 실행해도 됨\n",
        "\n",
        "#기본 경로설정\n",
        "root_path = '/content/drive/MyDrive/samsung_seg'\n",
        "data_path = '/content/samsung_seg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cxnCuLdyJenZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxnCuLdyJenZ",
        "outputId": "a8e998c9-6f7e-4dcb-8dcc-9a8716b6b227"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [01:27<00:00, 87.76s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if os.path.exists(data_path):\n",
        "    shutil.rmtree(data_path)\n",
        "\n",
        "listdir = ['train_source_image',\n",
        "           'train_source_gt',\n",
        "           'train_target_image',\n",
        "           'val_source_gt',\n",
        "           'val_source_image',\n",
        "           'test_image']\n",
        "\n",
        "#listdir = ['test_image'] #추론만 할 경우\n",
        "\n",
        "for dir in tqdm.tqdm(listdir):\n",
        "    shutil.copytree(f'{root_path}/{dir}',f'{data_path}/{dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h5PCUOxl2DBg",
      "metadata": {
        "id": "h5PCUOxl2DBg"
      },
      "source": [
        "# 데이터 시각화 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0noiVoCOLnrE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "0noiVoCOLnrE",
        "outputId": "fea77e4e-ca0b-4a0e-89c1-9bbab4f573b3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHHCAYAAABtF1i4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABfRklEQVR4nO3deZyN5f/H8deZYRbDGIMZ+67slC1kqSYTfpaiFFlLKcpWX1TWspfIXrJVQoqEbBNJkbImu2zJzJBlGMswc//+uM0xx8xgxLnO8H72OI/Ouc59zv2ZM8c577nva3FYlmUhIiIiYoiX6QJERETk3qYwIiIiIkYpjIiIiIhRCiMiIiJilMKIiIiIGKUwIiIiIkYpjIiIiIhRCiMiIiJilMKIiIiIGKUwIpJOtG3blkKFCpkuw+369++Pw+EwXcYdV6dOHerUqXNbn/PAgQM4HA6mTZt2W59X5HZTGJF0Zdq0aTgcDuclQ4YM5M2bl7Zt23LkyJFUH/fdd9/RsGFDQkND8fHxITg4mFq1avHBBx8QExPjsm2hQoVc9uHn50fx4sV58803OXHixE3XGhUVxRtvvEGJEiXIlCkTAQEBVKxYkffee49Tp07d6ksgqWjbtq3L7y1z5swUKVKEZs2a8fXXX5OQkGC6xDtm5syZjBo1ynQZIrcsg+kCRG7FwIEDKVy4MBcuXGDdunVMmzaNNWvWsG3bNvz8/JzbJSQk8MILLzBt2jTKli3Lq6++Sv78+Tlz5gxr167lnXfeYfHixURERLg8f4UKFejRowcAFy5cYMOGDYwaNYoff/yR9evX37C+3377jfr163P27Fmef/55KlasCMDvv//O0KFDWb16NcuWLbuNr4gA+Pr6MnnyZADOnz/PwYMH+e6772jWrBl16tTh22+/JTAw0HCVKfsv74eZM2eybds2unbt6tJesGBBzp8/T8aMGf9jdSJ3mCWSjkydOtUCrN9++82lvWfPnhZgzZ4926V9yJAhFmB169bNSkhISPZ8//zzjzV06FCXtoIFC1oNGjRItu0bb7xhAdbu3buvW+PJkyetvHnzWqGhodaOHTuS3R8ZGWm9++67132OlLRp08YqWLBgmh+Xkvj4eOv8+fO35bnutH79+lk381HVpk0bKyAgIMX7Et8HzzzzzO0u7z+LjY39z8/RoEGD2/beEDFBp2nkrlCzZk0A9u3b52w7d+4cw4YNo3Tp0owYMSLFfge5c+emZ8+eN7WPXLlyAZAhw/UPKE6aNIkjR44wcuRISpQokez+0NBQ3nnnHZe28ePHU7p0aXx9fcmTJw+dOnW6qVM5sbGx9OjRg/z58+Pr68v999/P+++/j3XNYtwOh4POnTvzxRdfOPezZMkSAGbNmkXFihXJkiULgYGBlC1bltGjR99w3++//z7Vq1cne/bs+Pv7U7FiRebOnZtsu8R9z58/nzJlyuDr60vp0qWd+09qzZo1VK5cGT8/P4oWLcqkSZNuWMfN6NWrF3Xr1uWrr75i9+7dLvd9//331KxZk4CAALJkyUKDBg34888/XbaJjIykXbt25MuXD19fX3Lnzk3jxo05cOBAsueqXbu287WsXLkyM2fOdN5fp04dypQpw4YNG6hVqxaZMmXirbfect6XtM/IqlWrcDgczJ49m7feeotcuXIREBBAo0aNOHz4sMtzLlq0iIMHDzpPUSX2LUqtz8gPP/zg/JmDgoJo3LgxO3bscNkmsa/O3r17adu2LUFBQWTNmpV27dpx7tw5l22XL1/Oww8/TFBQEJkzZ+b+++93/lwiN0OnaeSukPilkC1bNmfbmjVrOHXqFG+88Qbe3t5per5Lly5x/PhxwD5Ns2nTJkaOHEmtWrUoXLjwdR+7YMEC/P39adas2U3tq3///gwYMICwsDBeeeUVdu3axYQJE/jtt9/4+eefUz3EblkWjRo1YuXKlbzwwgtUqFCBpUuX8uabb3LkyBE+/PBDl+1/+OEH5syZQ+fOncmRIweFChVi+fLlPPfcczz22GMMGzYMgB07dvDzzz/TpUuX69Y9evRoGjVqRMuWLYmLi2PWrFk8/fTTLFy4kAYNGrhsu2bNGr755hteffVVsmTJwkcffUTTpk05dOgQ2bNnB+CPP/6gbt265MyZk/79+3P58mX69etHaGjoTb2ON9KqVSuWLVvG8uXLue+++wD47LPPaNOmDeHh4QwbNoxz584xYcIEHn74YTZt2uT8Um/atCl//vknr732GoUKFSI6Oprly5dz6NAh5zbTpk2jffv2lC5dmt69exMUFMSmTZtYsmQJLVq0cNbx77//Uq9ePZ599lmef/75G/58gwYNwuFw0LNnT6Kjoxk1ahRhYWFs3rwZf39/3n77bU6fPs3ff//t/J1nzpw51edbsWIF9erVo0iRIvTv35/z588zZswYatSowcaNG5N1kn7mmWcoXLgwQ4YMYePGjUyePJmQkBDn++XPP//k//7v/yhXrhwDBw7E19eXvXv38vPPP6fl1yP3OtOHZkTSIvE0zYoVK6xjx45Zhw8ftubOnWvlzJnT8vX1tQ4fPuzcdvTo0RZgzZ8/3+U5Ll++bB07dszlkvQUTsGCBS0g2aVGjRrW8ePHb1hjtmzZrPLly9/UzxMdHW35+PhYdevWteLj453tY8eOtQBrypQpzrZrT9PMnz/fAqz33nvP5TmbNWtmORwOa+/evc42wPLy8rL+/PNPl227dOliBQYGWpcvX76pepM6d+6cy+24uDirTJky1qOPPurSDlg+Pj4u9WzZssUCrDFjxjjbmjRpYvn5+VkHDx50tm3fvt3y9vb+z6dpLMuyNm3a5DxlZ1mWdebMGSsoKMjq0KGDy3aRkZFW1qxZne0nT560AGvEiBGpPvepU6esLFmyWFWrVk12+ivpe6t27doWYE2cODHZc9SuXduqXbu28/bKlSstwMqbN68VExPjbJ8zZ44FWKNHj3a2pXaaZv/+/RZgTZ061dlWoUIFKyQkxPr333+dbVu2bLG8vLys1q1bO9sST4+1b9/e5TmffPJJK3v27M7bH374oQVYx44dS+GVEbk5Ok0j6VJYWBg5c+Ykf/78NGvWjICAABYsWEC+fPmc2ySOkrn2r8Q//viDnDlzulz+/fdfl22qVq3K8uXLWb58OQsXLmTQoEH8+eefNGrUiPPnz1+3tpiYGLJkyXJTP8eKFSuIi4uja9eueHld/efYoUMHAgMDWbRoUaqPXbx4Md7e3rz++usu7T169MCyLL7//nuX9tq1a1OqVCmXtqCgIGJjY1m+fPlN1ZuUv7+/8/rJkyc5ffo0NWvWZOPGjcm2DQsLo2jRos7b5cqVIzAwkL/++guA+Ph4li5dSpMmTShQoIBzu5IlSxIeHp7m2lKS+D44c+YMYJ9aOHXqFM899xzHjx93Xry9valatSorV650/pw+Pj6sWrWKkydPpvjcy5cv58yZM/Tq1culAzWQ7PSgr68v7dq1u+m6W7du7fJ+atasGblz52bx4sU3/RyJjh49yubNm2nbti3BwcHO9nLlyvH444+n+JwdO3Z0uV2zZk3+/fdf57+voKAgAL799tu7esSS3FkKI5IujRs3juXLlzN37lzq16/P8ePH8fX1ddkm8QP87NmzLu3FihVzBo1WrVql+Pw5cuQgLCyMsLAwGjRowFtvvcXkyZP55ZdfnKM1UhMYGOj8wruRgwcPAnD//fe7tPv4+FCkSBHn/ak9Nk+ePMmCT8mSJV2eO1FKp5deffVV7rvvPurVq0e+fPlo3759in05UrJw4UIeeugh/Pz8CA4OJmfOnEyYMIHTp08n2zZpwEiULVs255f7sWPHOH/+PMWLF0+23bWvza1KfB8kvl579uwB4NFHH00WTpctW0Z0dDRgh4dhw4bx/fffExoaSq1atRg+fDiRkZHO507sq1SmTJkb1pE3b158fHxuuu5rXxOHw0GxYsWS9Ve5Gam938B+3xw/fpzY2FiX9mt/d4mnQhN/d82bN6dGjRq8+OKLhIaG8uyzzzJnzhwFE0kThRFJl6pUqUJYWBhNmzZlwYIFlClThhYtWrgEj8TOo9u2bXN5bObMmZ1Bo0iRIje9z8ceewyA1atXX3e7EiVKsHv3buLi4m76ud0h6ZGMRCEhIWzevJkFCxY4+5/Uq1ePNm3aXPe5fvrpJxo1aoSfnx/jx49n8eLFLF++nBYtWiTrPAuk2mcnpW3vlMT3QbFixQCcX5afffaZM5wmvXz77bfOx3bt2pXdu3czZMgQ/Pz86NOnDyVLlmTTpk1priOl34Mnu9Hvzt/fn9WrV7NixQpatWrF1q1bad68OY8//jjx8fHuLFXSMYURSfe8vb0ZMmQI//zzD2PHjnW216xZk6xZszJr1qzb8lfa5cuXgeRHWq7VsGFDzp8/z9dff33D5yxYsCAAu3btcmmPi4tj//79zvtTe+w///yT7CjMzp07XZ77Rnx8fGjYsCHjx49n3759vPzyy8yYMYO9e/em+pivv/4aPz8/li5dSvv27alXrx5hYWE3tb+U5MyZE39/f+fRiqSufW1u1WeffYbD4eDxxx8HcJ42CgkJcYbTpJdrZ0MtWrQoPXr0YNmyZWzbto24uDg++OADl+e6NvjeDte+JpZlsXfvXpeOpjc7Q21q7zew3zc5cuQgICAgzTV6eXnx2GOPMXLkSLZv386gQYP44YcfnKe6RG5EYUTuCnXq1KFKlSqMGjWKCxcuAJApUyb+97//sW3bNnr16pXiX+Fp+cv8u+++A6B8+fLX3a5jx47kzp2bHj16JBtGChAdHc17770H2H0pfHx8+Oijj1xq+fTTTzl9+nSyUSlJ1a9fn/j4eJcABvDhhx/icDioV6/eDX+ma/vKeHl5Ua5cOQAuXryY6uO8vb1xOBwuf/keOHCA+fPn33CfqT1feHg48+fP59ChQ872HTt2sHTp0lt6zqSGDh3KsmXLaN68ufO0R3h4OIGBgQwePJhLly4le8yxY8cAe4h44nsqUdGiRcmSJYvzNapbty5ZsmRhyJAhybb9r0d/ZsyY4RI4586dy9GjR11+vwEBASmeHrtW7ty5qVChAtOnT3cZOr5t2zaWLVtG/fr101xfSrMSV6hQAbj+e0gkKQ3tlbvGm2++ydNPP820adOcne569erFjh07GDFiBMuWLaNp06bky5ePkydPsnHjRr766itCQkKSdTo8cuQIn3/+OWAfpdiyZQuTJk0iR44cvPbaa9etI1u2bMybN4/69etToUIFlxlYN27cyJdffkm1atUA+4hA7969GTBgAE888QSNGjVi165djB8/nsqVK/P888+nup+GDRvyyCOP8Pbbb3PgwAHKly/PsmXL+Pbbb+natatLh9HUvPjii5w4cYJHH32UfPnycfDgQcaMGUOFChWcfU9S0qBBA0aOHMkTTzxBixYtiI6OZty4cRQrVoytW7fecL8pGTBgAEuWLKFmzZq8+uqrXL58mTFjxlC6dOmbfs7Lly87f28XLlzg4MGDLFiwgK1bt/LII4/w8ccfO7cNDAxkwoQJtGrVigcffJBnn32WnDlzcujQIRYtWkSNGjUYO3Ysu3fv5rHHHuOZZ56hVKlSZMiQgXnz5hEVFcWzzz7rfK4PP/yQF198kcqVK9OiRQuyZcvGli1bOHfuHNOnT7+l1wQgODiYhx9+mHbt2hEVFcWoUaMoVqwYHTp0cG5TsWJFZs+eTffu3alcuTKZM2emYcOGKT7fiBEjqFevHtWqVeOFF15wDu3NmjUr/fv3T3N9AwcOZPXq1TRo0ICCBQsSHR3N+PHjyZcvHw8//PCt/thyrzE2jkfkFqQ2A6tl2bOKFi1a1CpatGiyoarz5s2z6tevb+XMmdPKkCGDFRQUZD388MPWiBEjrFOnTrlse+3QXi8vLyskJMR67rnnXIan3sg///xjdevWzbrvvvssPz8/K1OmTFbFihWtQYMGWadPn3bZduzYsVaJEiWsjBkzWqGhodYrr7xinTx50mWblGZgPXPmjNWtWzcrT548VsaMGa3ixYtbI0aMSDbbLGB16tQpWY1z58616tata4WEhFg+Pj5WgQIFrJdfftk6evToDX++Tz/91CpevLjl6+trlShRwpo6dWqKs6Wmtu+CBQtabdq0cWn78ccfrYoVK1o+Pj5WkSJFrIkTJ6ZpBtakv7dMmTJZhQoVspo2bWrNnTvXZeh0UitXrrTCw8OtrFmzWn5+flbRokWttm3bWr///rtlWZZ1/Phxq1OnTlaJEiWsgIAAK2vWrFbVqlWtOXPmJHuuBQsWWNWrV7f8/f2twMBAq0qVKtaXX37pvL927dpW6dKlU6wjtaG9X375pdW7d28rJCTE8vf3txo0aOAy/NmyLOvs2bNWixYtrKCgIAtwvk9SGtprWZa1YsUKq0aNGs46GzZsaG3fvt1lm8TX/dohu4n/Bvfv329ZlmVFRERYjRs3tvLkyWP5+PhYefLksZ577rkbzlQskpTDstzYg0xERG7KqlWreOSRR/jqq69uegI9kfRKfUZERETEKIURERERMUphRERERIxKcxhZvXo1DRs2JE+ePDgcjpsayrdq1SoefPBBfH19KVasWLIVJEVExFWdOnWwLEv9ReSekOYwEhsbS/ny5Rk3btxNbb9//34aNGjAI488wubNm+natSsvvvjibZk7QERERNK//zSaxuFwMG/ePJo0aZLqNj179mTRokUuMxM+++yznDp16qbXwBAREZG71x2f9Gzt2rXJpokODw+na9euqT7m4sWLLjP3JSQkcOLECbJnz37T0x6LiIiIWZZlcebMGfLkyeOyMvm17ngYiYyMJDQ01KUtNDSUmJgYzp8/n+KiUUOGDGHAgAF3ujQRERFxg8OHD5MvX75U7/fI6eB79+5N9+7dnbdPnz6d4hLkxpUDfjJdRHIhR0NoP6296TKSCY0M9ci6/s2ymQXVa5suI5m/s+bl/epvmi4jReX+/puf3n/fdBnJnCtXjF0/TTZdRjK78eclSpguI5lynOMnbs9ChLfV5tNQe63pKpILiYT200xXkbLQctDe876QYmJiyJ8/P1myZLnudnc8jOTKlYuoqCiXtqioKAIDA1NdStvX1xdfX987Xdp/5w0Emi4iOa+zXh75+vn7+BPogS9YnCMz/hlNV5Gcb0Yv8NDl5r19fT3wNwkZvL3JHJjZdBnJZMIfT/yw8CYDgXje60Xmy4DnfYbh5eORZQHg7w2BnvceS3SjLhZ3fJ6RatWqERER4dK2fPly50JhIiIicm9Lcxg5e/YsmzdvZvPmzYA9dHfz5s3OZb979+5N69atndt37NiRv/76i//973/s3LmT8ePHM2fOHLp163Z7fgIRERFJ19IcRn7//XceeOABHnjgAQC6d+/OAw88QN++fQE4evSoM5gAFC5cmEWLFrF8+XLKly/PBx98wOTJkwkPD79NP4KIiIikZ2nuM5I4K2BqUppdtU6dOmzatCmtuxIREZF7gNamEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohRERERExSmFEREREjFIYEREREaMURkRERMQohREREREx6pbCyLhx4yhUqBB+fn5UrVqV9evXX3f7UaNGcf/99+Pv70/+/Pnp1q0bFy5cuKWCRURE5O6S5jAye/ZsunfvTr9+/di4cSPly5cnPDyc6OjoFLefOXMmvXr1ol+/fuzYsYNPP/2U2bNn89Zbb/3n4kVERCT9S3MYGTlyJB06dKBdu3aUKlWKiRMnkilTJqZMmZLi9r/88gs1atSgRYsWFCpUiLp16/Lcc8/d8GiKiIiI3BvSFEbi4uLYsGEDYWFhV5/Ay4uwsDDWrl2b4mOqV6/Ohg0bnOHjr7/+YvHixdSvXz/V/Vy8eJGYmBiXi4iIiNydMqRl4+PHjxMfH09oaKhLe2hoKDt37kzxMS1atOD48eM8/PDDWJbF5cuX6dix43VP0wwZMoQBAwakpTQRERFJp+74aJpVq1YxePBgxo8fz8aNG/nmm29YtGgR7777bqqP6d27N6dPn3ZeDh8+fKfLFBEREUPSdGQkR44ceHt7ExUV5dIeFRVFrly5UnxMnz59aNWqFS+++CIAZcuWJTY2lpdeeom3334bL6/kecjX1xdfX9+0lCYiIiLpVJqOjPj4+FCxYkUiIiKcbQkJCURERFCtWrUUH3Pu3LlkgcPb2xsAy7LSWq+IiIjcZdJ0ZASge/futGnThkqVKlGlShVGjRpFbGws7dq1A6B169bkzZuXIUOGANCwYUNGjhzJAw88QNWqVdm7dy99+vShYcOGzlAiIiIi9640h5HmzZtz7Ngx+vbtS2RkJBUqVGDJkiXOTq2HDh1yORLyzjvv4HA4eOeddzhy5Ag5c+akYcOGDBo06Pb9FCIiIpJupTmMAHTu3JnOnTuneN+qVatcd5AhA/369aNfv363sisRERG5y2ltGhERETFKYURERESMUhgRERERoxRGRERExCiFERERETFKYURERESMUhgRERERoxRGRERExCiFERERETFKYURERESMUhgRERERoxRGRERExCiFERERETFKYURERESMUhgRERERoxRGRERExCiFERERETFKYURERESMUhgRERERoxRGRERExCiFERERETEqg+kCxDNZlsWJEyeIjY3FsiyX+woWLGioKhERuRspjEgyf//9N19//TWnT59OFkQcDgd9+/Y1VJmIiNyNFEYkmYULF5InTx5atGhBlixZTJcjIiJ3OYURSebEiRM888wzBAcHmy5FRETuAerAKsnkzZuXEydOmC5DRETuEToyIslUqVKFZcuWcfbsWUJCQvD29na5PzQ01FBlkiYnTkC2bOBwuLZbFpw8CTryJSIeQmFEkpkzZw4A3377rbPN4XBgWZY6sKYnb70Fw4dDYKBre2ysfd/EiWbqEhG5hsKIJNOlSxfTJcjtcu1REYCLFyFjRvfXIiKSCoURSSYoKMh0CfJfXDmyBcCCBa7Bw7Jg/37In9/9dYmIpEJhRFJ04sQJ1q1bx/HjxwHImTMnVatW1Qib9ODw4avXjxyBpH1+MmSAfPng8cfdX5eISCoURiSZvXv3MmvWLHLlykX+K39BHz58mPHjx/Pcc89RtGhRwxXKdfXoYf9/2jRo3hz8/Y2WIyJyIwojkkxERAQPPfQQYWFhLu0rVqxgxYoVCiPpRdu2pitIUSwwFIgAooGEa+7/y+0ViYhpCiOSzLFjx2jWrFmy9gceeIB169YZqEhuycWLsGQJ7NwJZ87Y/UWSGjTISFkvAj8CrYDcQApdbEXkHqMwIskEBAQQGRlJ9uzZXdojIyMJCAgwVJWk2Wefwe7dULUqZM2a8sgaA74HFgE1TBciIh5DYUSSefDBB1m4cCEnT5506TPy888/89BDDxmuTm7atm3QuTMUK2a6EhfZAHWDFpGkFEYkmVq1auHj48PatWuJiIgAIEuWLNSuXZuqVasark5uWqZM4IFHst4F+gLTgUyGaxERz6AwIsk4HA6qVatGtWrVuHjxIgC+vr6Gq5I0a9TInmekXTvw8TFdjdMHwD4gFCgEXDv92kZ3FyQiximMyHUphKQz773nevvYMXjjDcie3XW+EYB33nFfXUk0MbJXEfFkCiMCwKRJk2jdujX+/v5MmjTputu+/PLLbqpK0qx8edMV3FA/0wWIiMdRGBEA7r//fjJkyOC8LulUw4amKxARSTOFEQGgTp06KV4Xud2ykfLcIg7ADygGtAXaubEmETFLYUSSGT16NB06dCBTJtexDhcuXGDSpEla1Te96NYt5XaHw148L2dOqFYNarh3xo++wCCgHlDlStt6YAnQCdgPvAJcBjq4tTIRMUVhRJI5deoU1rWzdQKXL18mJibGQEVySxo0gMWLoUwZKFTIbjtwAP78E2rXhn//hZkzISEBatZ0W1lrgPeAjte0TwKWAV8D5YCPUBgRuVcojIjTrl27nNf37t2Ln5+f83ZCQgL79+8nW7ZsJkqTW7F3LzRubAePpFavhu3boWNHyJsXfvjBrWFkKTAshfbHgCtL/FEf6OW2ikTENIURcZo1axZgzzMyf/58l/u8vb0JCgqibt26BiqTW7J9Ozz1VPL2EiVg7lz7etmyMG+eW8sKBr4Drj2J9B1XZ2aNBbK4sygRMUphRJz69bMHXabWZ0TSmUyZYOtWuGb1ZbZute8DezG9JEfA3KEPdp+QlVztM/IbsBiYeOX2cqB28oeKyF1KYcSklcAjqdw3Drs3nwHqoHqXaNDA7hOya9fVPiMHD8Iff0DLlvbtHTugeHG3ltUBKAWMBb650nY/9kq+1a/c7pHC40Tk7qUwYtJTwAqg4jXto7H/fDQURr7//nuCg4OTrUOzfv16Tpw4wRNPPGGmMEmbmjUhd25YtQo2bbLbcuWyZ2QtWtS+/fjjRkqrgVbtFZGrFEZMGoE9vnE1UOJK2wfAQOw11g3ZsWMHzz77bLL2/Pnzs2bNGoWR9KRYMY9YtTcGCExy/XoCb3C/iNx9FEZMehE4AYRhj3ecDQzGPnlu8M/Gc+fOuYykSeTr68u5c+cMVCQ37fx58Pe/ev16Erdzg2zAUSAECCLlSc+sK+3xbqtKRDyFwohp/wP+BSphfwovBR4yWhHBwcHs3buXKlWquLTv2bNHQ3s9XbduMHw4BAamPulZookTr3//bfQDV0fKrHTbXkUkvVAYcbePUmjLC2QCamFPRbn+Svvr7irKVbVq1Vi8eDGxsbEULlwYgP3797N27VrCw8PNFCU3p3t3CAi4et1D1E7luogIKIy434eptHsDP1+5gH282lAYeeCBB7h8+TI//fQTq1evBiAoKIgGDRpQPh2sCntPu+++lK8nde4cbNvmnnqSOI49f0jBJG1/Au9faW8CtHB7VSLiCRRG3G2/6QJuTuXKlalcuTKxsbFkzJgRHx8f0yXJ7fLvvzBlClxzGu5Oew3Ig91HGyAaqHmlrSj24njxQCu3ViUinsDLdAHi2QICAhRE5LZYBzRKcnsGdj+SzcC32H23x7m/LBHxADoy4m5pOY0/8o5VcV2jR4++7v2aFE1uRSRQKMntH7Cn2kn8EGoEDHFzTSLiGRRG3G3TTW6X0thHN7l2srOEhAQiIyPZu3cv1atXT+VRItcXCJziap+R9cALSe53ABfdXJOIeAaFEXdLB+MaH3oo5bHF69ev5+jRo26uRtLshx+uf//Jk+6p4xoPYQ8m+wR7GvgzwKNJ7t8N5DdQl4iYpzAiN6148eJERETQuHFj06XI9axYceNtgoNvvM1t9i7wGPA5cBl4C3sytESz0LBfkXuVwohpvwNzgENA3DX3fZN8c5O2b9+Ovxtn7ZRbNHiw6QpSVA7YgT16PRdQ9Zr7n8VeQE9E7j0KIybNAloD4cAyoC72seoo4ElzZU2aNClZ29mzZ4mNjaVBgwYGKpK7RQ4gteNqemeJ3LsURkwajD0JWicgC/ZqvYWBl4Hc5sq6//77XW47HA4CAgIoVKgQOXLkMFSViIjcrW5pnpFx48ZRqFAh/Pz8qFq1KuvXr7/u9qdOnaJTp07kzp0bX19f7rvvPhYvXnxLBd9V9nH1z0Ef7GkoHUA34GMzJSUkJJAtWzYqVapEnTp1qFOnDrVr16ZSpUoKInJPOHPqDPMnz2ds77GcPnEagJ0bdxJ9JNpwZSJ3rzQfGZk9ezbdu3dn4sSJVK1alVGjRhEeHs6uXbsICQlJtn1cXByPP/44ISEhzJ07l7x583Lw4EGCgoJuR/3pWzbsIQVgr0+zDSiLPf7R0OK4Xl5eLFy4kE6dOpkpQMSgPVv38GrYq2TOmpl/DvxDkw5NyBqclR+++YHIQ5EMnDHQdIke6dSpM6xf/yfR0SdISEhwua916/8zVJWkJ2kOIyNHjqRDhw60a9cOgIkTJ7Jo0SKmTJlCr169km0/ZcoUTpw4wS+//ELGjBkBKFSo0H+r+m5RC1iOHUCeBrpgzwS1HHvYgSF58+YlMjJSgVHuOR92/5D/a/t/dBnehVpZajnba9SvwTst3jFYmef67rvVtGzZh7NnzxEYGIDDcXWSJIfDoTAiNyVNYSQuLo4NGzbQu3dvZ5uXlxdhYWGsXbs2xccsWLCAatWq0alTJ7799lty5sxJixYt6NmzJ97e3ik+5uLFi1y8eHX6o5iYmLSUmX6MBS5cuf42kBH4BWgKGPzcq1y5MsuWLSMmJobcuXMnmw4+NDTUUGWSJn/8AV5eULq0a/uff4JlQZkyRspajL0u5LXrPy8FEoB6bq/oqj9/+5O3Jr2VrD0kbwj/Rv5roCLP16PHKNq3b8TgwZ3IlMnPdDmSTqUpjBw/fpz4+PhkX0ahoaHs3Lkzxcf89ddf/PDDD7Rs2ZLFixezd+9eXn31VS5dukS/fv1SfMyQIUMYMGBAWkpLn5JO9eAFJD+wZMTcuXMB+P77751tDocDy7JwOBz07dvXVGmSFvPmwZMpDMuyLPs+Q2GkFzA0hXbryn0mw4iPrw9nY84maz+4+yDZcmZL4RFy5Eg0r7/eXEFE/pM7PpomISGBkJAQPv74Y7y9valYsSJHjhxhxIgRqYaR3r1707371UVcYmJiyJ//LpybsTXwCPbpmqKGa0lCa8/cJaKjIXcKw7Jy5bLvM2QPKc8nUgLY6+ZarlWrUS0mD5zM0Dl2XHI4HEQeimRMzzE82vTRGzz63hQeXo3ff99BkSL5TJci6ViawkiOHDnw9vYmKirKpT0qKopcuXKl+JjcuXOTMWNGl1MyJUuWJDIykri4uBRXhPX19cXX1zctpaVPPtgrg72A3YG1NlDnyv+Lmyvr9OnT5M+fHy8v18FWCQkJHD58WH1J0gt/fzh+HK4dBXXsGBj895UV+AvXRfPADiIBbq/GVbcPuvG/Zv/j8ZDHuXj+Ii/Vfol/I/+lXLVyvDroVcPVeaYGDWrw5puj2b79L8qWLUbGjK5fK40aaV5dubE0hREfHx8qVqxIREQETZo0AewvqIiICDp37pziY2rUqMHMmTNJSEhwfrnt3r07xb4I95zJV/5/BFgN/Ah8wNV5Rv42U9b06dPp0aMHAQGuXw0XLlxg+vTpxk/TfMZnTGQi+9nPWtZSkIKMYhSFKUzjVKfUugeVLw9z5sArr0DOnHZbdDTMnQvlyhkrqzHQFZjH1QOCe4Ee2Cv3mpQ5a2bGLx/P5p83s2fLHs6dPUeJB0tQNeza+WIlUYcOgwAYOHBysvscDgfx8def+kEEbuE0Tffu3WnTpg2VKlWiSpUqjBo1itjYWOfomtatW5M3b16GDLEXA3/llVcYO3YsXbp04bXXXmPPnj0MHjyY119//fb+JOlZNiD7lf8HYf9Wcporx7KsFNvPnz/vHBFlygQm0Je+dKUrgxhEPPEABBHEKEYpjCTVtCmMHg19+0K2K/0dTp6E4sWhWTNjZQ0HnsA+LZN4YP9voCbwvqmigMuXLlPDvwZfbP6CCjUqUKFGBYPVpB8JCb+ZLkHuAmkOI82bN+fYsWP07duXyMhIKlSowJIlS5ydWg8dOuRyeD9//vwsXbqUbt26Ua5cOfLmzUuXLl3o2bPn7fsp0qu3gFXAJqAk9umZXth9SAz0lZs9ezZg/zXz7bffupxasyyLqKgo4313xjCGT/iEJjRhaJJukJWoxBu8YbAyD+TvDz17wo4dcPgw+PhA3rxw331Gy8qKPWhsObAF8Mdet6bW9R7kBhkyZiBXgVwkxCfceGMRua1uqQNr586dUz0ts2rVqmRt1apVY926dbeyq7vbUOwjIP2ApwCz3xH4+dm94S3LwsfHx+UoiJeXFw8++CAVK1Y0VR4A+9nPAzyQrN0XX2KJNVCRh3M4oFQp++JBHNhLMdU1Xcg12r3djnFvjWPgZwPJGpzVdDnpxo8/buD99z9nx479AJQqVZg332xNzZrJ/626wyXieZmF9KEWhU38ZSdpprVpTNqE3U9kFXZfER+udmKtg9vDSePG9imOrFmzUr16dY/s01OYwmxmMwUp6NK+hCWUpKShqjzIDz9AzZqQMaN9/Xoedd/okI+AlwC/K9evx+QJ3Dlj5/D33r+pl6ceuQrmwj/AdZXqLzZ+Yagyz/X554tp124ATz31KK+//iwAP/+8hccee4Vp0/rTosUTbq8pI958zQ76GD/elr5cvnyZVatWsW/fPlq0aEGWLFn4559/CAwMJHPmzHd03wojJpW/ckn89N3C1YXzEuBKdwi3q1GjhsvtU6dOsXPnTnLmzEnRombHIHenO53oxAUuYGGxnvV8yZcMYQiTSd6B7p6zYgVUqWKHkRUrUt/O4XBrGPkQaIkdRj68znYOzIaROk3qGNx7+jRo0BSGD3+dbt1aOttef/1ZRo78nHffnWwkjAA0oQTz2Uk3qhnZf3pz8OBBnnjiCQ4dOsTFixd5/PHHyZIlC8OGDePixYtMnDjxju7/rg0jX3/99U1v27Rp0ztYyXVY2EdHVl25rAFisE+gGxwNN2vWLEqWLEmlSpW4cOECkydPxtvbm3PnzlG3bl0qV65srLYXeRF//HmHdzjHOVrQgjzkYTSjeZZnjdXlMfr0sfuKAAwebLaWJDZj9xUB2G+wjht5qd9LpktId/766wgNGyY/AtGoUW3eemu8gYpsxQlmIKv5mcNUJDcBuB7pfR2NkEqqS5cuVKpUiS1btpA9e3Zn+5NPPkmHDh3u+P7v2jBy+vRp53WHw8GTTz7J6dOn+f333wGoWLEiQUFBfPPNN6ZKtGdgPYt9dKQ20AF7SEGQuZIAjh49Sni4PVn39u3byZw5My+//DLbt29n1apVRsMIQMsr/53jHGc5SwjJF2i8Z3XrBsOHQ2AgjBwJHTtCpkymqyIYOAqEAI8C32D8bS63Sf78oURErKdYMdfO7StW/Er+/OaWjviUTQThxwaOsoGjLvfZR+AURpL66aef+OWXX5Kdni9UqBBHjhy54/u/a8NI+/btndeHDh3KnDlz6Nixo3NFSS8vL8aPH2923ZvPscNHoLkSUnLp0iXnpHP79u2jRIkSOBwO8uXLx6lTp4zWtp/9XOYyxSlOpiv/AexhDxnJSKFkU2ndY3x9ITbWDiO7d0O8oXN918gM/IsdRlYBl4xWk7rKXpVdFnq71nrNmZFMjx7P8/rr77N5826qV7fnr/n55y1Mm7aQ0aN7GKtrP5pJOi0SEhKIT+Hz4u+//yZLlix3fP93bRhJqn379jz88MMuS1snJCQwcuRIfvnlF/73v/+ZKazBlf/vBfZhj230xz59k/rn4R0XHBzMzp07KVGiBPv27eOhhx4CIDY21vjMuG1pS3vaU/yaKWp/5VcmM5lVrDJTmKcoWdI+IpI4I/KECZAhlX/mSZZcuNPCsFc+SOxi/CSQWvfoG3S7vaNGzBvhcvvypcvs2rSLRdMX8dIAncJJySuvNCNXrux88MHnzJlj91MqWbIQs2cPpnHjOmaLA+KIZz8nKUowGfC68QPuUXXr1mXUqFF8/PHHgH1G4ezZs/Tr14/69evf8f3fE2EkQ4YMlChRgt27d7u0lyhRItmU5271L/AMsBI7fOwBimBPD58Ne4SNAbVr1+brr79m6dKlFC5c2Dm3yL59+8id0lonbrSJTdSgRrL2h3iIzqQ83Pye0r49rF1rT/m+Zw/kyWPPL2LY58B07Mz9I1AaMH/yKLk6KXx5hjULo2jpoiybvYwmLzRxe03pwZNPPsKTTz5iugwX57jEayxmOlsA2M1rFCEbr7GYvATSi4cNV+hZPvjgA8LDwylVqhQXLlygRYsW7Nmzhxw5cvDll1/e8f3fE2Fk6tSpfPrppwwePJj16+3DrFWrVqVXr15MnTrVXGHdgIzAIXAZldoc6I6xMFKqVCkKFCjAmTNnXNYcKlKkCCVLmh0+68DBGc4kaz/NaedsrKZdioeM3jfe7o6Ij4faV3o/HzwITz3lEX1GLgEdr1z/HRhG+uozUuahMgx6aZDpMjzS4cORV07j2v1D1q/fxsyZSylVqjAvvfSUsbp6s4ItRLGKtjzB5872MIrQnx8VRq6RL18+tmzZwqxZs9i6dStnz57lhRdeoGXLlvj7+9/4Cf6jeyKMvPHGG0RGRtKjRw/nX/ZHjx5lxIgRfPCBoW98gGXAUq7OiZ2oOHDQ/eUklTlz5mTjyvPmzWuomqtqUYshDOFLvsQb+xs/nniGMISHDX64JFjw/R5YfRBi4mBgHcgZAN/uguz+8HABNxWStAPrdfo+uFs2rnZg9Zyqbs6F8xeY/dFscuY1uEaDB2vR4h1eeulJWrVqQGTkccLCOlGmTFG++OJ7IiP/pW/fOz8SIyXz2cVsmvEQ+XAkedeVJoR9nDBSk6fLkCEDzz//vJl9G9mrm1mWxYgRIxgxYoSzI86ZM8n/una7WFI+Vn0CMNg1Iy4ujjVr1rB//35iY2OTrVXTpYu5jmHDGEYtanE/91OTmgD8xE/EEMMPBnsbLN4Da/+Gp0rCZ1uvtufJAhF/uTGMpIMOrD/iuR1YH8n2iEsHVsuyOHfmHH6Z/Bj4+UCDlXmubdv2UaVKaQDmzFlB2bJF+fnnKSxbto6OHYcYCyPHiCUkhXWgY4lzCSdy1Z49e1i5ciXR0dEufSyBO75A6j0RRpLyiBCSqCYwA3j3ym0H9mRnw7F7+xny3XffceDAAcqVK+eWXtRpUYpSbGUrYxnLFrbgjz+taU1nOhNMsLG61v0Nz5eDkjngiz+utucPhCh3zlKfDjqwWnhuB9buH3Z3CSMOLwfZcmajTNUyBGYzOOwtNhYCkn+xeoJLly7j62v/Nles+JVGjew5R0qUKMTRo8eN1VWJPCxiN69dGcKb+FudzEaqJTscfedZFhyOgZAA8PPAb95PPvmEV155hRw5cpArVy7XfwcOh8LI7dK0aVOeeeYZChQokGwctbH1VoYDj2GfRI8D/gf8iX1k5GczJYGdjlu0aEGBAu76cz5t8pCHwXjOhF4Apy5ASApHuRIscOu6a+rA+p9UfrQyoflDUxzeG3koklwFcqXwKDcIDYVnnrF/vw97Vl+H0qWLMHHi1zRo8DDLl6/n3XdfAeCff46RPbu59X0G8xj1+ILtHOMyCYzmV7ZzjF84zI+0dXs9FlDsI/jzVSie/Yabu917773HoEGDjC1ie0+Ekddee41BgwYxbdo0GjduzNSpUylatCiVK1dm3Lhx5gorA+wGxgJZsCdAewp7OniDg1b8/f3d0mHpVp3iFOtZTzTRJOD6Td+a1kZqyp0F9pyA7Nd8w248Cvnd+Xns4+ORHVj9SR8dWBsVbsSSo0sIDnE9ynbq31M0KtzI3Dwjn38O06bZU/gXKmSHktat7bBp2LBhr/Hkk28yYsRntGnTgPLl7UW1FixY7Tx9Y8LDFGAzLzOUNZQlhGXs40Fys5YXKIv7J2Pzctgh5N/zXDMxgWc4efIkTz/9tLH93xNh5NVXX+Wll15i1qxZtG3bluHDh7N//34GDBhAcLC5Q/uAPUf222ZLuNYjjzzCqlWraNKkicvKvZ7gO76jJS05y1kCCXQ59+vAYSyMNCgO0zbbR0gsYFMkRJ2FdUegk6kJa3uYm3DqelaaLuA6ru0flej82fP4+Bk8wtSkiX05dgw++8wOJn36QHi4HUwaNUr9dNwdZFkWRYrk49ChhVy+HE+2JKeyXnrpSTJl8nN7TUkVJZhPaGS0hqSGPgZvLocJDaCMh00c/fTTT7Ns2TI6dux4443vgHsijBQoUIBffvkFgPPnzzv7QXz22WesW7eO1157zX3FbL3xJk7l7lgV17V27VpOnDjB+++/T1BQULK5WF5++WUzhQE96EF72jOYwc7ZVz1BhVx26Fi0B3y9YcEuKJDVbitlchDGyZOwZQucOAGXL7ve98wzZmoC/gYWYI9qj7vmvpHuL4eR3e29OhwOJvadiF+SL9GE+AS2/bqN+yq4eRntlOTMaff16d4dxoyBN9+ExYshRw576v9evdx6JMyyLIoVa8Kff86heHHX07qFCrn/qE0MF29620ADowRaz4dzl6D8RPDxBv9rvoFPmDlDAkCxYsXo06cP69ato2zZssn+EH399Tu7hOU9EUYiIyMJDg7m0KFDHDp0iIceeoitW7dSuHDh6079fEdUwO5Jde0sq4l/kCVtMzQQ4v777zez45twhCO8zuseFUQSFc8OXT3pXPCOHTB+vP1FFRkJefPCv//aPekM9geKABphz++3E/ts5QHsfwIPGqpp16ZdgP3luvePvWT0ufpBnNEnI8XLF6fVG60MVZdEVBRMn24fGTl4EJo1gxdegL//hmHDYN06WLbMbeV4eXlRvHgB/v33NMU94NxDEENveqRMPHe2Q2ZKRoW7fZc37eOPPyZz5sz8+OOP/Pjjjy73ORwOhZHb4YcffqBRo0Zs3ryZqVOn8uGHH9KsWTMqVark/oXyki5Zugl4A3gTnKtcr8We7Gy4e8tKqk6dOuZ2fgPhhPM7v1OEIqZL8Xzz58Pjj9uH8F9/HV5+GbJkgSlToLS5c/m9sd/2A7C7Sn2NPeS3JWBmsXmYtHISAAPaDaDH6B5kDsx8g0e42TffwNSpsHQplCoFr74Kzz8PQUFXt6le3R5N5WZDh3bmzTdHM2FCL8qUKeb2/Se1kjbO6wc4RS8iaEt5qmHPIr2Ww0xnC0N4zEh9bSoY2e1N2b/f7Hra90QYeemll5ynGsaPH8+///5L9erVWbBgAZMmTXJvMQWTXH8a+AhIOu1/OSA/0Ado4r6yUvLPP/9w/Lg9NC9nzpzGp4IHaEAD3uRNtrOdspQlI66HEhu58fxwt6U3v+2HJv4iOnoUXnzRvu7tDZcugZ8fNGxoD/lN7OjqZjuAxMmlMwDnsechGQg0Bl4xUpWt39R+Bvd+He3awbPPws8/Q2qrZufJA2+7vwNa69b9OHfuAuXLt8DHJyP+/q6nP06ccN9g7dpJFsocyAxGUpfnKOtsa8T9lCWUj9lAGyq4ra6UXLgMcdcc/Q40u/SXU2LfKXeeObgnwohlWS6rEc6ePZvZs2cbrOiKP4DCKbQXBra7uZYkYmNjmTt3LgcOHMDPzz53fuHCBQoXLkzTpk0JMDjfQQfsCZQGknwCKgcOt04J/0wpt+3q1vj6Xu0nkjWr3fkxcfTF2bPGygrgaj+R3NjDfROP05ibleKq7b9vZ/mc5UQeiuRynGs/mxHfjEjlUXfYTz9BuVQ6kc2fb3du9feHfu4PU6NGeWZH6bUcZqJzNdKrKpGHF1lgoCKIjYOeK2DOn/aommvFu//MkYsZM2YwYsQI9uzZA8B9993Hm2++SatWd/4U5T0RRgCyZs3KCy+84Fxb5c8//2TKlCnExMSYK6okMASYzNUZoOKutBlcAub7778nLi6OV199lZw57d6Xx44dY/78+SxZsoSmTZsaq+3aobwmVctvuoIbKFwY9u6F3LmhTBmYOxeOHIFNm+z7DHkIWIP9Fq8P9MDO5d9cuc+kpbOW0q91P6qFV2PdsnU8VPchDu4+yImoEzxiciG4evVgzZrkv7evv7aH+Ma6c2Y9V23a/J+xfV9PfrLyCRsZzuMu7ZPZSH7MzH/yv+Ww8oA9mqbVPBhXH46cgUkb7JE2Jo0cOZI+ffrQuXNnatSwFyNds2YNHTt25Pjx43Tr1u2O7v+eCCMVK1Zk6dKlnD9/3rlQXvfu3Xn77bepW7cumzZtMlPYRKAh9to0iX/0bMXuxPqdmZIA9u7dS6tWrZxBBOzTNPXr1+ezzz4zV9g1LnABP8wOHUwqwYLNkXD0ykGHPJmhfC57fgEjnn4aLl4ZXdCwoX39998hJMS+z5CR2FPqgN1v5CwwG3vuBRMjaZKaOngq3T/szjOdnqFWllr0GN2DvIXzMvjlweTIncNcYS++CGFh9mmaxNl1Z8+2h/VOm2auriv27fubqVMXsG/f34we/QYhIcF8//3PFCiQi9Klixqp6UPCacocvmcvVbHX1VrPEfZwgq8xM5Lsu90w40moUwjafQs1C0KxYCiY1Z65uaWhEZQAY8aMYcKECbRufXV6hEaNGlG6dGn69+9/x8OI1403Sf8+/PBDFixYQKFChWjatClNmzalcOHCLFy4kFGjRpkrrArwF/AedhgpBwy60lbFXFmWZeHtnXzZWS8vr1TnYXCXeOJ5l3fJS14yk5m/+AuAPvThUz41Vld0LPRbBVM3w6aj9mXKZui/Co6Z+KM1IcEe1ps4j46vL7RsCX372kNAs5sZ9hOPPaw3cSxPAHYm34rdkbVgKo9zl7/3/c3DDewZTjP6ZORC7AUcDgcturXgm4/d3Nk9qQEDoH59O5CcOAEzZ9r9SGbMMBosAX78cQNlyzbn11+38c03Kzl79hwAW7bsoV8/N/fJS6I+xdlNZxpyHyc4zwnO05D72E1n6huaduzEeSiSzb4e6GvfBnvtqtWGF0c9evQo1atXT9ZevXp1jh49esf3f0+EkUqVKjFs2DCXfiPx8fEMHz6cSpUqGawM+9P4Jew/CUcCHa60GVS4cGGWLFniso5PTEwMS5cupUgRs6NYBjGIaUxjOMPxSbK6SRnKMJnJxuqavQ1yZrIPtb5Ty74MeQxyZIJZfxooyMsLRo+Gc+cM7Dx13kBd4KTpQlIRmC2Qc2fs1yxn3pzs3bYXgDOnznDh3AWTpdnzipQvDw89BB06wJdfgsFTpol69RrLe++9wvLl4/FJMiT60UcrsW7dNoOV2adqBvMY39Ccb2jOIB4zdooG7CCy/8qbv0QOu+8I2EdMggwf5C1WrBhz5sxJ1j579myKu2Hc9j1xmiYmJoYCBQqwa9cul/b8+fO7f+G8BUA9IOOV69djaOLAevXqMWvWLEaNGkXWrPY/3NOnTxMSEsJTTz1lpqgrZjCDj/mYx3iMjlydKbA85dnJTmN17T4BvWpAQJJJOjP7wJMlYYSpdYby5LE7reYweHohBWWwD/6Z67WSugdqPcC65esoVrYYYU+H8UGXD/j9h9/5dfmvVHnMzYcrF6TwAfHUU3Zn1ueeA4fj6jaNzM0y+scfe5k5871k7SEhwRw/fsqttWwlijKE4IWDrURdd9tyBqaEb1cBtkRB7ULQ62Fo+CWMXW+PqjEy4i6JAQMG0Lx5c1avXu3sM/Lzzz8TERGRYki53e6JMDJ79mw+/fRT3njjDedMrDVq1OD9999n1qxZ7i2mCRCJPbFCk+ts58DYpGdZs2blpZde4q+//nIZ2mv6qAjYk54VI/lcBgkkcMngwvQZvOyhete6eBm8TR1/bNzY7uDYqBEULJh8wTxD6w+9hz3PyLtARZIfCDS4Ni7/G/s/4i7YY33av92eDBkzsOWXLTza9FFeeOcF9xbTpEnq902ZYl/ADiXxhj4sgKCgLBw9epzChfO6tG/atIu8ed07/XAFJhLJG4QQQAUm4sCBRfJTy/bIO/cPXelW7er1sCKwsxNsOArFg6Gs+7ORi6ZNm/Lrr78ycuRI5s+fD0DJkiVZv349DzzwwB3f/z0RRt544w0sy2LGjBlkyJABh8NBXFwc48eP5213j8tPSOW6B9i/fz+LFy/mxRdfxNfXl6JFi1K0qN357MKFC4wfP54GDRpQsKC5M/ulKMVP/ETBa3oXzGUuD3Dn/8GkplwIfP4HtC4HhYLstv2n7E5p5d39IbNwoT3Z2dix9u3x41PebuJE99WEPY9ID65Oq9OI5JMQG8zgAGQNvnoI38vLi7a92porJsHDPiBS8eyzdenZcwxffTUUh8NBQoLFzz9v5o03RtG6dfKhtXfSfrqQ88rszPvpkup2sW7+w+WH/dB5Max70XUukYJB9umZ6lNgYgO7Q6tJFStW5IsvvjCy73sijFy6dImuXbvSu3dv55frvn37eOWVV9i/f7/7J/NaC/wLJB0RNwPoB8RiHzEZA+5eOmHdunU8+OCD+Pom37Gfnx8VK1Zk3bp1RsNIX/rShjYc4QgJJPAN37CLXcxgBgtZaKyu5mXshfKG/Xz1SEh8gh1Emrt7stOFC6FWLXv9Eg8yAHvlXk9cKK+yV+UbTvDkcDj49fKvbqroirVr7Sn8/y/Jh8WMGfZ8IrGx9tGTMWPsDsqGDB7cic6dh1OgwP9x+XI8pUo9TXx8Ai1ahPOOm48mFUyyFnTBFNaFvshlxvEbw/mZSN5wW12j1kGHB1Oe1CyrH7xcEUauMxNGvLy8buq9f/nata1us7s6jPj4+NC/f38ef/xxLl68yIgRI/j2229p27YtS5YsIT4+ng8//ND9hQ0E6nA1jPwBvAC0xZ58YQSQB+jv3rKioqIICwtL9f6iRYs6T3O521/8RWEK05jGfMd3DGQgAQTQl748yIN8x3c8fs18Au6QYMGyfbA1Ci4n2AvmVcsHOCB3Zggx2Rn5Pg9Y2C2JxIPlZuZ9vb4R81KfzOyPtX8w66NZWAkGRpINGACPPHI1jPzxh70WTdu29tTvI0bYfYP693d7aQkJCYwYMYMFC1YTF3eZVq3q07Tpo5w9e54HHrg/2cJ57nKRy/RnFcv5Cx+8+R81aEIJprKJt/kBb7zo5uYZbbZEwbDUP1qpWxTeN/PRyrx581K9b+3atXz00UckuOEo3V0dRgYOHMjLL7/MihUrqF69Ol999RVTp07loYceokePHnz11VdueZGT2Yx9wjzRLKAq8MmV2/mxj5L0d2tVnD17NsUhvYm8vLw4Z2h0RnGKc5SjhBBCTWoSTDB/8AehBjqhJfX9HrsnfMmckNkLtkWDf0ZoU95oWXY/Ag/kmVVBncZ1krUd2HWAsb3G8tN3P/FEyyfoONDA0upbtsB7STqHzpoFVavCJ1c+LPLnt4+SGAgjgwZNoX//jwkLq4K/vy8zZy7FsiymTDE7pX5fVjKJDYRRhF84zNN8RTsqsI6/GUk4T1MKbzcPJI06CxlT/2glgxccMzTwrXHjxsnadu3aRa9evfjuu+9o2bIlAwcmn/H6drurw8jTTz9N69at+e677yhdujRbt24lQ4YMlC9v+JviJLh8h/6IPcImUWXgsFsrAiAwMJDo6GiCE+emuEZUVBSZM5tZQOzaTmjf8z2xmJt1MtG6v6FFWah15fDqjmMw9jdoVc7gZGdgzydyIwaOCt7HjQPJCXcUch3H/jnGpH6TWDh9IdXCq/HF5i8oZmoBuJMnITTJh8WPP9qzsSaqXBkOG/iwAGbMWMT48T15+WV7ePGKFb/SoEFXJk/u41wLzISv2M4MnqQR97ONaMoxgcsksIWON72i7+2WN9D+Q6VYyh+tbI2yj6Sa9s8//9CvXz+mT59OeHg4mzdvpkyZMm7Z910dRvLly8eGDRsAe/r3ixcvmjktc61Q7NV782NP/74R+4R6ojNwzfpvblGsWDFWrlxJsWLFyJDB9a1x6dIlVq1axX0ecug/pR7yJpy4AGVCrt4ueWXwwOkLkM3MYBVbw4bGRstczwAwOMvD9Z09fZYpg6cwe8xs7q9wPxMiJvBATXOdogE7iOzfbx8BiYuDjRvtUzeJzpyBjAY+LIBDhyKpX7+G83ZYWFUcDgf//HOMfPnMHbH8mxgqYvcDLEMIvmSgGw8ZCyIA9YtBn5XwRDHwu+Zb9/wle8LE/zP40Xr69GkGDx7MmDFjqFChAhEREdSsWdOtNdzVYcTb25u4uDjn7cuXL3PW4AJhTvWBXsAwYD6QCUj6e98KGJhBuVatWnz88ceMGTOGKlWqkP3KLJ3Hjx/nt99+w7Ist79BEzmu/Hdtm2nxCZDxmj8CvR0QbzorVaoEgSYHyabsWexR7Z5m+vDpzBg2g+y5sjPoy0EpnrYxon596NULhg2zF8TLlAmS/hvcuhWKmplu/fLlePz8XHtkZsyYgUuX7mxHxxuJx8KHq+dEMuBFZnyu84g7751a8M3HcN8Y6FwF7r8yAfLO4zDuN/vz4m0zH60MHz6cYcOGkStXLr788ssUT9u4w10dRhwOB9OmTePilfU5/Pz8mDhxIrHXLCrl9oXf3gWewu7JlxmYDi7/VqZgT1PpZpkzZ6Z9+/YsWrSIiIgIl2WkixYtSv369Y2epmlLW3yvDDG6wAU60pGAa2ap+Ab3T9k9bYt9zjfRpQR7SK9PknPErxie6NcTmI+OqRvbayy+/r7kK5aPRdMXsWj6ohS3c/uqve++a090Vrs2ZM4M06e7zhczZQrUNfBhgb1sRNu2/fH1vVrPhQsX6dhxCAEBV4/KfePm18z+rPgW3yuB5AKX6cgiAq453PwNzd1WU2hm+KU9vLIIekdA4qoaDgeEF7UXzAs1dJqmV69e+Pv7U6xYMaZPn8706dNT3O6bb+7sZ+tdHUaufVE///xzQ5VcIwewGjiNHUau7dj01ZV2A4KCgmjZsiXnz5/nxAn77H1wcDD+hg/5t6GNy+3ned5QJa4eype8rWre5G2Ch5xYS1mD1g1uOLzRiBw5YPVqOH3aDiPXdjD/6iu73YCUVut9/vn6KWzpXm2o4HL7eecqpGYVDILFLeHkedh7wv73UDzY8OlcoHXr1h7x3r+rw0j79u1Nl3B9qZ08T6WTkzv5+/uTN6/nfKtOZarpElLUtoLpClLg5snMbpYnT+HVf1p/0yVcX9ZUPixS6WzuDlOnmh01k5qpmDnNcLOy+UNlz/loZZoHrPoM98hCeSIiIuK5FEZERETEKIURERERMUphRERERIxSGBERERGjFEZERETEKIURERERMUphRERERIxSGBERERGjFEZERETEKIURERERMUphRERERIxSGBERERGjFEZERETEKIURERERMUphRERERIxSGBERERGjFEZERETEKIURERERMUphRERERIxSGBERERGjFEZERETEKIURERERMUphRERERIxSGBERERGjFEZERETEKIURERERMUphRERERIy6pTAybtw4ChUqhJ+fH1WrVmX9+vU39bhZs2bhcDho0qTJrexWRERE7kJpDiOzZ8+me/fu9OvXj40bN1K+fHnCw8OJjo6+7uMOHDjAG2+8Qc2aNW+5WBEREbn7pDmMjBw5kg4dOtCuXTtKlSrFxIkTyZQpE1OmTEn1MfHx8bRs2ZIBAwZQpEiR/1SwiIiI3F3SFEbi4uLYsGEDYWFhV5/Ay4uwsDDWrl2b6uMGDhxISEgIL7zwwk3t5+LFi8TExLhcRERE5O6UpjBy/Phx4uPjCQ0NdWkPDQ0lMjIyxcesWbOGTz/9lE8++eSm9zNkyBCyZs3qvOTPnz8tZYqIiEg6ckdH05w5c4ZWrVrxySefkCNHjpt+XO/evTl9+rTzcvjw4TtYpYiIiJiUIS0b58iRA29vb6Kiolzao6KiyJUrV7Lt9+3bx4EDB2jYsKGzLSEhwd5xhgzs2rWLokWLJnucr68vvr6+aSlNRERE0qk0HRnx8fGhYsWKREREONsSEhKIiIigWrVqybYvUaIEf/zxB5s3b3ZeGjVqxCOPPMLmzZt1+kVERETSdmQEoHv37rRp04ZKlSpRpUoVRo0aRWxsLO3atQOgdevW5M2blyFDhuDn50eZMmVcHh8UFASQrF1ERETuTWkOI82bN+fYsWP07duXyMhIKlSowJIlS5ydWg8dOoSXlyZ2FRERkZuT5jAC0LlzZzp37pzifatWrbruY6dNm3YruxQREZG7lA5hiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYdUthZNy4cRQqVAg/Pz+qVq3K+vXrU932k08+oWbNmmTLlo1s2bIRFhZ23e1FRETk3pLmMDJ79my6d+9Ov3792LhxI+XLlyc8PJzo6OgUt1+1ahXPPfccK1euZO3ateTPn5+6dety5MiR/1y8iIiIpH9pDiMjR46kQ4cOtGvXjlKlSjFx4kQyZcrElClTUtz+iy++4NVXX6VChQqUKFGCyZMnk5CQQERExH8uXkRERNK/NIWRuLg4NmzYQFhY2NUn8PIiLCyMtWvX3tRznDt3jkuXLhEcHJzqNhcvXiQmJsblIiIiInenNIWR48ePEx8fT2hoqEt7aGgokZGRN/UcPXv2JE+ePC6B5lpDhgwha9aszkv+/PnTUqaIiIikI24dTTN06FBmzZrFvHnz8PPzS3W73r17c/r0aefl8OHDbqxSRERE3ClDWjbOkSMH3t7eREVFubRHRUWRK1eu6z72/fffZ+jQoaxYsYJy5cpdd1tfX198fX3TUpqIiIikU2k6MuLj40PFihVdOp8mdkatVq1aqo8bPnw47777LkuWLKFSpUq3Xq2IiIjcddJ0ZASge/futGnThkqVKlGlShVGjRpFbGws7dq1A6B169bkzZuXIUOGADBs2DD69u3LzJkzKVSokLNvSebMmcmcOfNt/FFEREQkPUpzGGnevDnHjh2jb9++REZGUqFCBZYsWeLs1Hro0CG8vK4ecJkwYQJxcXE0a9bM5Xn69etH//79/1v1IiIiku6lOYwAdO7cmc6dO6d436pVq1xuHzhw4FZ2ISIiIvcIrU0jIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFG3FEbGjRtHoUKF8PPzo2rVqqxfv/6623/11VeUKFECPz8/ypYty+LFi2+pWBEREbn7pDmMzJ49m+7du9OvXz82btxI+fLlCQ8PJzo6OsXtf/nlF5577jleeOEFNm3aRJMmTWjSpAnbtm37z8WLiIhI+pfmMDJy5Eg6dOhAu3btKFWqFBMnTiRTpkxMmTIlxe1Hjx7NE088wZtvvknJkiV59913efDBBxk7dux/Ll5ERETSvzSFkbi4ODZs2EBYWNjVJ/DyIiwsjLVr16b4mLVr17psDxAeHp7q9iIiInJvyZCWjY8fP058fDyhoaEu7aGhoezcuTPFx0RGRqa4fWRkZKr7uXjxIhcvXnTePn36dFrKdJ94IMZ0EcklnElwef08xfm488R44At2xjrL+Uumq0ju4qUEOH/edBkpir940QN/k3AuPp6zMWdNl5HMOQ/9sIjnHDF43uvF2VjA8z7DSIjzyLIAOB8PMZ73Hou5UpNlWdfdLk1hxF2GDBnCgAEDTJdxY1uBrKaLSC6aaIYy1HQZKepGN9MlJHcGWGq6iJQcgTldTReRIg9968PWvZC1jukq0g2P/T16qmjw0I9WYCt089zf5pkzZ8iaNfX60hRGcuTIgbe3N1FRUS7tUVFR5MqVK8XH5MqVK03bA/Tu3Zvu3bs7b586dYqCBQty6NCh6/4wYouJiSF//vwcPnyYwMBA0+V4PL1eaafXLG30eqWNXq+089TXzLIszpw5Q548ea67XZrCiI+PDxUrViQiIoImTZoAkJCQQEREBJ07d07xMdWqVSMiIoKuXbs625YvX061atVS3Y+vry++vr7J2rNmzepRL7KnCwwM1OuVBnq90k6vWdro9UobvV5p54mv2c0cREjzaZru3bvTpk0bKlWqRJUqVRg1ahSxsbG0a9cOgNatW5M3b16GDBkCQJcuXahduzYffPABDRo0YNasWfz+++98/PHHad21iIiI3IXSHEaaN2/OsWPH6Nu3L5GRkVSoUIElS5Y4O6keOnQIL6+rg3SqV6/OzJkzeeedd3jrrbcoXrw48+fPp0yZMrfvpxAREZF065Y6sHbu3DnV0zKrVq1K1vb000/z9NNP38quAPu0Tb9+/VI8dSPJ6fVKG71eaafXLG30eqWNXq+0S++vmcO60XgbERERkTtIC+WJiIiIUQojIiIiYpTCiIiIiBilMCIiIiJGeXwYGTduHIUKFcLPz4+qVauyfv160yV5rCFDhlC5cmWyZMlCSEgITZo0YdeuXabLSjeGDh2Kw+FwmaBPXB05coTnn3+e7Nmz4+/vT9myZfn9999Nl+Wx4uPj6dOnD4ULF8bf35+iRYvy7rvv3nCdjnvF6tWradiwIXny5MHhcDB//nyX+y3Lom/fvuTOnRt/f3/CwsLYs2ePmWI9wPVer0uXLtGzZ0/Kli1LQEAAefLkoXXr1vzzzz/mCk4Djw4js2fPpnv37vTr14+NGzdSvnx5wsPDiY6ONl2aR/rxxx/p1KkT69atY/ny5Vy6dIm6desSGxtrujSP99tvvzFp0iTKlStnuhSPdfLkSWrUqEHGjBn5/vvv2b59Ox988AHZsmUzXZrHGjZsGBMmTGDs2LHs2LGDYcOGMXz4cMaMGWO6NI8QGxtL+fLlGTduXIr3Dx8+nI8++oiJEyfy66+/EhAQQHh4OBcuXHBzpZ7heq/XuXPn2LhxI3369GHjxo1888037Nq1i0aNGhmo9BZYHqxKlSpWp06dnLfj4+OtPHnyWEOGDDFYVfoRHR1tAdaPP/5ouhSPdubMGat48eLW8uXLrdq1a1tdunQxXZJH6tmzp/Xwww+bLiNdadCggdW+fXuXtqeeespq2bKloYo8F2DNmzfPeTshIcHKlSuXNWLECGfbqVOnLF9fX+vLL780UKFnufb1Ssn69estwDp48KB7ivoPPPbISFxcHBs2bCAsLMzZ5uXlRVhYGGvXrjVYWfpx+vRpAIKDgw1X4tk6depEgwYNXN5rktyCBQuoVKkSTz/9NCEhITzwwAN88sknpsvyaNWrVyciIoLdu3cDsGXLFtasWUO9evUMV+b59u/fT2RkpMu/y6xZs1K1alV9B9yk06dP43A4CAoKMl3KDd3SDKzucPz4ceLj453TzCcKDQ1l586dhqpKPxISEujatSs1atTQ1PvXMWvWLDZu3Mhvv/1muhSP99dffzFhwgS6d+/OW2+9xW+//cbrr7+Oj48Pbdq0MV2eR+rVqxcxMTGUKFECb29v4uPjGTRoEC1btjRdmseLjIwESPE7IPE+Sd2FCxfo2bMnzz33nMctnJcSjw0j8t906tSJbdu2sWbNGtOleKzDhw/TpUsXli9fjp+fn+lyPF5CQgKVKlVi8ODBADzwwANs27aNiRMnKoykYs6cOXzxxRfMnDmT0qVLs3nzZrp27UqePHn0mskdc+nSJZ555hksy2LChAmmy7kpHnuaJkeOHHh7exMVFeXSHhUVRa5cuQxVlT507tyZhQsXsnLlSvLly2e6HI+1YcMGoqOjefDBB8mQIQMZMmTgxx9/5KOPPiJDhgzEx8ebLtGj5M6dm1KlSrm0lSxZkkOHDhmqyPO9+eab9OrVi2effZayZcvSqlUrunXr5lzVXFKX+Dmv74C0SQwiBw8eZPny5eniqAh4cBjx8fGhYsWKREREONsSEhKIiIigWrVqBivzXJZl0blzZ+bNm8cPP/xA4cKFTZfk0R577DH++OMPNm/e7LxUqlSJli1bsnnzZry9vU2X6FFq1KiRbKj47t27KViwoKGKPN+5c+dcVjEH8Pb2JiEhwVBF6UfhwoXJlSuXy3dATEwMv/76q74DUpEYRPbs2cOKFSvInj276ZJumkefpunevTtt2rShUqVKVKlShVGjRhEbG0u7du1Ml+aROnXqxMyZM/n222/JkiWL87xq1qxZ8ff3N1yd58mSJUuy/jQBAQFkz55d/WxS0K1bN6pXr87gwYN55plnWL9+PR9//DEff/yx6dI8VsOGDRk0aBAFChSgdOnSbNq0iZEjR9K+fXvTpXmEs2fPsnfvXuft/fv3s3nzZoKDgylQoABdu3blvffeo3jx4hQuXJg+ffqQJ08emjRpYq5og673euXOnZtmzZqxceNGFi5cSHx8vPM7IDg4GB8fH1Nl3xzTw3luZMyYMVaBAgUsHx8fq0qVKta6detMl+SxgBQvU6dONV1auqGhvdf33XffWWXKlLF8fX2tEiVKWB9//LHpkjxaTEyM1aVLF6tAgQKWn5+fVaRIEevtt9+2Ll68aLo0j7By5coUP7PatGljWZY9vLdPnz5WaGio5evraz322GPWrl27zBZt0PVer/3796f6HbBy5UrTpd+Qw7I0FaCIiIiY47F9RkREROTeoDAiIiIiRimMiIiIiFEKIyIiImKUwoiIiIgYpTAiIiIiRimMiIiIiFEKIyJilMPhYP78+abLEBGDFEZE5Ja1bdv2np2aW0RuH4URERERMUphRERuizp16vD666/zv//9j+DgYHLlykX//v1dttmzZw+1atXCz8+PUqVKsXz58mTPc/jwYZ555hmCgoIIDg6mcePGHDhwAICdO3eSKVMmZs6c6dx+zpw5+Pv7s3379jv544nIHaQwIiK3zfTp0wkICODXX39l+PDhDBw40Bk4EhISeOqpp/Dx8eHXX39l4sSJ9OzZ0+Xxly5dIjw8nCxZsvDTTz/x888/kzlzZp544gni4uIoUaIE77//Pq+++iqHDh3i77//pmPHjgwbNoxSpUqZ+JFF5DbQQnkicsvatm3LqVOnmD9/PnXq1CE+Pp6ffvrJeX+VKlV49NFHGTp0KMuWLaNBgwYcPHiQPHnyALBkyRLq1avHvHnzaNKkCZ9//jnvvfceO3bswOFwABAXF0dQUBDz58+nbt26APzf//0fMTEx+Pj44O3tzZIlS5zbi0j6k8F0ASJy9yhXrpzL7dy5cxMdHQ3Ajh07yJ8/vzOIAFSrVs1l+y1btrB3716yZMni0n7hwgX27dvnvD1lyhTuu+8+vLy8+PPPPxVERNI5hRERuW0yZszoctvhcJCQkHDTjz979iwVK1bkiy++SHZfzpw5nde3bNlCbGwsXl5eHD16lNy5c9960SJinMKIiLhFyZIlOXz4sEt4WLduncs2Dz74ILNnzyYkJITAwMAUn+fEiRO0bduWt99+m6NHj9KyZUs2btyIv7//Hf8ZROTOUAdWEXGLsLAw7rvvPtq0acOWLVv46aefePvtt122admyJTly5KBx48b89NNP7N+/n1WrVvH666/z999/A9CxY0fy58/PO++8w8iRI4mPj+eNN94w8SOJyG2iMCIibuHl5cW8efM4f/48VapU4cUXX2TQoEEu22TKlInVq1dToEABnnrqKUqWLMkLL7zAhQsXCAwMZMaMGSxevJjPPvuMDBkyEBAQwOeff84nn3zC999/b+gnE5H/SqNpRERExCgdGRERERGjFEZERETEKIURERERMUphRERERIxSGBERERGjFEZERETEKIURERERMUphRERERIxSGBERERGjFEZERETEKIURERERMUphRERERIz6fzKSsbcfS7TpAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 이미지 정규화를 적용하는 함수\n",
        "#-train/valid/test 데이터셋에 같은 normalize 적용\n",
        "#-train target 따로 적용\n",
        "def denormalization(img, source=True):\n",
        "    if source:\n",
        "        # source 매개변수가 True이면, source_mean과 source_std 값을 사용합니다.\n",
        "        mean = source_mean\n",
        "        std = source_std\n",
        "    else:\n",
        "        # source 매개변수가 False이면, target_mean과 target_std 값을 사용합니다.\n",
        "        mean = target_mean\n",
        "        std = target_std\n",
        "\n",
        "    # 이미지를 역정규화합니다.\n",
        "    out_img = img * std + mean\n",
        "    return out_img\n",
        "\n",
        "# 색상 팔레트 및 클래스 이름을 정의합니다.\n",
        "palette = [[0, 0, 0], [0, 255, 0], [127, 127, 127], [255, 0, 255], [153, 76, 0], [0, 153, 153], [255, 0, 0], [204, 255, 204], [0, 255, 255], [255, 255, 204], [255, 0, 127], [255, 127, 0], [255, 255, 255]]\n",
        "cls_name = ['Road', 'Sidewalk', 'Construction', 'Fence', 'Pole', 'Traffic Light', 'Traffic Sign', 'Nature', 'Sky', 'Person', 'Rider', 'Car', 'None']\n",
        "\n",
        "# 팔레트의 각 색상 및 클래스 이름을 순회하면서 시각적으로 표시합니다.\n",
        "indices = list(range(len(palette)))\n",
        "\n",
        "# matplotlib을 사용하여 RGB 값을 가지고 색상을 생성하고 영역을 채웁니다.\n",
        "for i, (rgb, label) in enumerate(zip(palette, cls_name)):\n",
        "    r, g, b = [x / 255 for x in rgb]  # RGB 값을 0에서 1 범위로 정규화합니다.\n",
        "    color = (r, g, b)  # 정규화된 RGB 값을 color 변수에 저장합니다.\n",
        "    plt.fill_between([i, i + 1], 0, 1, color=color)  # 현재 색상으로 영역을 채웁니다.\n",
        "\n",
        "    # 첫 번째 항목 (인덱스 0)인 경우 텍스트 색상을 흰색으로, 그 외에는 검은색으로 설정합니다.\n",
        "    if i == 0:\n",
        "        c = \"white\"\n",
        "    else:\n",
        "        c = \"black\"\n",
        "\n",
        "    # 클래스 이름을 중앙 정렬하여 현재 색상 아래에 표시합니다.\n",
        "    plt.text(i + 0.5, 0.5, label, ha='center', rotation=90, color=c)\n",
        "\n",
        "# x와 y 축의 범위, 레이블 및 제목을 설정하여 그래프를 보여줍니다.\n",
        "plt.xlim(0, len(palette))\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel('Index')  # x 축 레이블 설정\n",
        "plt.title('RGB Colors and Descriptions')  # 그래프 제목 설정\n",
        "plt.show()  # 그래프를 화면에 표시합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be76a29e-e9c2-411a-a569-04166f074184",
      "metadata": {
        "id": "be76a29e-e9c2-411a-a569-04166f074184"
      },
      "source": [
        "# 데이터로더 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94129e2a",
      "metadata": {
        "id": "94129e2a"
      },
      "outputs": [],
      "source": [
        "#사전에 미리 Resize해둔 이미지 크기\n",
        "IMAGE_WIDTH = 1024\n",
        "IMAGE_HEIGHT = 512\n",
        "\n",
        "#Crop한 이미지의 크기\n",
        "CROP_WIDTH = IMAGE_WIDTH//2\n",
        "CROP_HEIGHT = IMAGE_HEIGHT//2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yoRPuBksIxDv",
      "metadata": {
        "id": "yoRPuBksIxDv"
      },
      "outputs": [],
      "source": [
        "#Train_Source + Valid_Source 의 Mean, Std\n",
        "source_mean = [0.5897106 , 0.5952661 , 0.57897425]\n",
        "source_std = [0.16688786, 0.15721062, 0.1589595]\n",
        "\n",
        "#Train_Target 의 Mean, Std\n",
        "target_mean = [0.4714665 , 0.47141412, 0.49733913]\n",
        "target_std = [0.23908237, 0.24033973, 0.25281718]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMNi4B3FJY2U"
      },
      "outputs": [],
      "source": [
        "# PyTorch의 Dataset 클래스를 상속하여 사용자 정의 데이터셋을 생성합니다.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv_file, target_csv_file=None, mix_bg_prob=0, infer=False):\n",
        "\n",
        "        # 여러 CSV 파일을 병합합니다.\n",
        "        if type(csv_file) == list:\n",
        "            self.data = pd.read_csv(csv_file[0])\n",
        "            for i in range(1, len(csv_file)):\n",
        "                self.data = pd.concat([self.data, pd.read_csv(csv_file[i])], ignore_index=True)\n",
        "        else:\n",
        "            self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        self.infer = infer\n",
        "\n",
        "        self.mix_bg_prob = mix_bg_prob\n",
        "\n",
        "        # 공통으로 적용되는 데이터 변환을 설정합니다.\n",
        "        self.augmentation = A.Compose([\n",
        "            A.ColorJitter(p=0.25),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomCrop(width=CROP_WIDTH, height=CROP_HEIGHT)\n",
        "        ])\n",
        "\n",
        "        self.source_norm = A.Normalize(mean=source_mean, std=source_std)\n",
        "        self.target_norm = A.Normalize(mean=target_mean, std=target_std)\n",
        "\n",
        "        self.len_source = len(self.data)\n",
        "\n",
        "        if target_csv_file:\n",
        "            self.target_data = pd.read_csv(target_csv_file)\n",
        "            self.len_target = len(self.target_data)\n",
        "        else:\n",
        "            self.target_data = None\n",
        "            self.len_target = 1\n",
        "\n",
        "    # 배경 이미지와 원본 이미지를 혼합하는 메소드\n",
        "    def mix_bg(self, source_image, source_mask, target_image):\n",
        "        # 소스 이미지의 높이, 너비, 채널 수를 가져옵니다.\n",
        "        h, w, c = source_image.shape\n",
        "\n",
        "        # 타원 모양의 마스크를 생성합니다.\n",
        "        # 타원은 중심 좌표와 장축, 단축 반지름 정보를 사용하여 정의됩니다.\n",
        "        center = (IMAGE_WIDTH // 2, int(IMAGE_HEIGHT * 0.375))  # 타원 중심 좌표 (x, y)\n",
        "        axis_length = (IMAGE_WIDTH // 2, int(IMAGE_HEIGHT * 0.64))  # 장축 반지름과 단축 반지름\n",
        "\n",
        "        # 마스크를 초기화하고 타원을 그려 마스크를 생성합니다.\n",
        "        mask = np.zeros((h, w, 1), dtype=np.uint8)\n",
        "        cv2.ellipse(mask, center, axis_length, 0, 0, 360, 1, -1)\n",
        "\n",
        "        # 타원 모양으로 이미지를 혼합합니다.\n",
        "        # 타원 내부는 source_image로, 타원 외부는 target_image로 혼합합니다.\n",
        "        mixed_image = source_image * mask + target_image * (1 - mask)\n",
        "\n",
        "        # 마스크도 혼합하여 새로운 마스크를 생성합니다.\n",
        "        # 타원 내부는 source_mask로, 타원 외부는 클래스 12로 (또는 다른 값으로) 설정합니다.\n",
        "        mixed_mask = source_mask * mask[:, :, 0] + np.ones_like(source_mask) * 12 * (1 - mask[:, :, 0])\n",
        "\n",
        "        # 혼합된 이미지와 마스크를 반환합니다.\n",
        "        return mixed_image, mixed_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len_source\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source_idx = idx\n",
        "\n",
        "        # 이미지와 마스크 파일 경로 설정\n",
        "        img_path = self.data.iloc[source_idx, 1].replace('./', data_path + '/')\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.infer == True:\n",
        "            # 추론 모드에서 이미지를 정규화하고 PyTorch Tensor로 변환\n",
        "            image = A.Compose([self.target_norm, ToTensorV2()])(image=image)['image']\n",
        "            return image\n",
        "\n",
        "        mask_path = self.data.iloc[source_idx, 2].replace('./', data_path + '/')\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask[mask == 255] = 12\n",
        "\n",
        "        # 이미지의 하늘 부분을 배경 클래스로 변환\n",
        "        mask[IMAGE_HEIGHT // 3 * 2:][mask[IMAGE_HEIGHT // 3 * 2:] == 8] = 12\n",
        "\n",
        "        #image와 동일한 크기와 형태를 가지지만 값은 모두 0으로 초기화\n",
        "        target_image = np.zeros_like(image)\n",
        "        distortion_image = image.copy()\n",
        "        distortion_mask = mask.copy()\n",
        "\n",
        "        # self.target_data가 존재하는 경우에만 실행\n",
        "        if self.target_data is not None:\n",
        "            # 대상 이미지 파일 경로 설정\n",
        "            # idx % self.len_target를 사용하여 대상 데이터에서 이미지 선택\n",
        "            target_img_path = self.target_data.iloc[idx % self.len_target, 1].replace('./', data_path + '/')\n",
        "\n",
        "            # 대상 이미지를 읽어옵니다.\n",
        "            target_image = cv2.imread(target_img_path)\n",
        "\n",
        "            # 대상 이미지의 색상 체계를 BGR에서 RGB로 변환합니다.\n",
        "            target_image = cv2.cvtColor(target_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # mix_bg_prob 확률로 이미지 혼합 작업 수행\n",
        "            if self.mix_bg_prob > np.random.uniform(0, 1):\n",
        "                # 이미지 혼합을 수행할 때 distortion_image와 distortion_mask는 이미 초기화되어 있어야 합니다.\n",
        "                distortion_image, distortion_mask = self.mix_bg(distortion_image.astype(np.uint8),\n",
        "                                                                distortion_mask.astype(np.uint8), target_image)\n",
        "\n",
        "\n",
        "        # 데이터 변환 수행\n",
        "        source_tensor = A.Compose([self.augmentation, self.source_norm, ToTensorV2()])(image=image, mask=mask)\n",
        "        target_image = A.Compose([self.augmentation, self.target_norm, ToTensorV2()])(image=target_image)['image']\n",
        "        distortion_tensor = A.Compose([self.augmentation,\n",
        "                                       A.ElasticTransform(alpha=100, sigma=10, alpha_affine=25, border_mode=1, p=1),\n",
        "                                       self.source_norm, ToTensorV2()])(image=distortion_image, mask=distortion_mask)\n",
        "\n",
        "        image = source_tensor['image']\n",
        "        mask = source_tensor['mask']\n",
        "        distortion_image = distortion_tensor['image']\n",
        "        distortion_mask = distortion_tensor['mask']\n",
        "\n",
        "        return image, mask, target_image, distortion_image, distortion_mask\n"
      ],
      "id": "PMNi4B3FJY2U"
    },
    {
      "cell_type": "markdown",
      "id": "dc955893-22fd-4320-88be-7aa0d790cbd9",
      "metadata": {
        "id": "dc955893-22fd-4320-88be-7aa0d790cbd9"
      },
      "source": [
        "# 데이터 증강, 로더 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b708503-2ff9-4584-9d73-40990b3572f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b708503-2ff9-4584-9d73-40990b3572f8",
        "outputId": "bb61a534-6bb1-464d-b3ed-4a51b89fea1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3126\n",
            "3126\n",
            "2923\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_set = CustomDataset(csv_file=[f'{root_path}/train_source.csv', f'{root_path}/val_source.csv', f'{root_path}/val_source.csv'],\n",
        "                          target_csv_file = f'{root_path}/train_target.csv',\n",
        "                          mix_bg_prob = 0.25)\n",
        "valid_set = CustomDataset(csv_file=f'{root_path}/val_source.csv' )\n",
        "#실제로 사용하지는 않음, 1/10의 데이터를 사용하여도 100에폭까지 오버피팅이 발생하지 않았음\n",
        "\n",
        "print(len(train_set))\n",
        "print(train_set.len_source)\n",
        "print(train_set.len_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9QOxNMvJY2V"
      },
      "source": [
        "# 이미지 시각화"
      ],
      "id": "z9QOxNMvJY2V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MIT-2G1bcvRt",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MIT-2G1bcvRt"
      },
      "outputs": [],
      "source": [
        "# 이미지 인덱스 2660에서 2669까지 반복\n",
        "for i in range(2660, 2670):\n",
        "    # 현재 시간 측정\n",
        "    start_time = time.time()\n",
        "\n",
        "    # train_set.__getitem__(i)을 통해 이미지, 마스크 및 기타 데이터를 가져옵니다.\n",
        "    img, mask, target_img, distortion_img, distortion_mask = train_set.__getitem__(i)\n",
        "\n",
        "    # 처리 시간 출력\n",
        "    print(time.time() - start_time)\n",
        "\n",
        "    # 이미지 및 대상 이미지를 밀어내기 및 밀어내기 반전 작업 수행\n",
        "    img = (denormalization(np.transpose(img, (1, 2, 0)).numpy()) * 255).astype(np.uint8)\n",
        "    target_img = (denormalization(np.transpose(target_img, (1, 2, 0)).numpy(), False) * 255).astype(np.uint8)\n",
        "    distortion_img = (denormalization(np.transpose(distortion_img, (1, 2, 0)).numpy()) * 255).astype(np.uint8)\n",
        "    print(target_img.min(), target_img.max(), target_img.shape)\n",
        "\n",
        "    # color_mask 및 color_distortion_mask 초기화\n",
        "    color_mask = np.zeros_like(img, dtype=np.uint8)\n",
        "    color_distortion_mask = np.zeros_like(img, dtype=np.uint8)\n",
        "\n",
        "    # 각 클래스에 대해 color_mask 및 color_distortion_mask 업데이트\n",
        "    for j, color in enumerate(palette):\n",
        "        color_mask[mask == j] = color\n",
        "        color_distortion_mask[distortion_mask == j] = color\n",
        "\n",
        "    # 이미지 및 결과를 시각화하기 위한 플롯 생성\n",
        "    plt.figure(figsize=(30, 15))\n",
        "\n",
        "    # 원본 이미지\n",
        "    plt.subplot(1, 6, 1)\n",
        "    plt.imshow(img)\n",
        "\n",
        "    # 클래스에 따라 색칠된 마스크 이미지\n",
        "    plt.subplot(1, 6, 2)\n",
        "    plt.imshow(color_mask)\n",
        "\n",
        "    # 원본 이미지와 색칠된 마스크 이미지를 중간 비중으로 혼합\n",
        "    plt.subplot(1, 6, 3)\n",
        "    plt.imshow(cv2.addWeighted(img, 0.5, color_mask, 0.5, 0))\n",
        "\n",
        "    # 타겟 이미지\n",
        "    plt.subplot(1, 6, 4)\n",
        "    plt.imshow(target_img)\n",
        "\n",
        "    # 왜곡된 이미지\n",
        "    plt.subplot(1, 6, 5)\n",
        "    plt.imshow(distortion_img)\n",
        "\n",
        "    # 왜곡된 이미지와 색칠된 왜곡 마스크 이미지를 중간 비중으로 혼합\n",
        "    plt.subplot(1, 6, 6)\n",
        "    plt.imshow(cv2.addWeighted(distortion_img, 0.5, color_distortion_mask, 0.5, 0))\n",
        "\n",
        "    # 플롯 표시\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V6tw1MEk55ye",
      "metadata": {
        "id": "V6tw1MEk55ye"
      },
      "source": [
        "#모델 선언\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qc21XLZTalhH",
      "metadata": {
        "id": "Qc21XLZTalhH"
      },
      "source": [
        "##DAFormer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fE34U9ee1If",
      "metadata": {
        "id": "6fE34U9ee1If"
      },
      "source": [
        "### Mix-Transformer(MiT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbppPjuvJY2Y"
      },
      "outputs": [],
      "source": [
        "# 참고 논문 및 깃허브 참고 논문 및 깃허브url: https://arxiv.org/abs/2111.14887\n",
        "# https://github.com/lhoyer/DAFormer/blob/master/mmseg/models/backbones/mix_transformer.py"
      ],
      "id": "hbppPjuvJY2Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LqlZ7HC8wMWl",
      "metadata": {
        "id": "LqlZ7HC8wMWl"
      },
      "outputs": [],
      "source": [
        "# class Mlp(nn.Module):\n",
        "#     def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "#         super().__init__()\n",
        "#         out_features = out_features or in_features\n",
        "#         hidden_features = hidden_features or in_features\n",
        "#         self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "#         self.dwconv = DWConv(hidden_features)\n",
        "#         self.act = act_layer()\n",
        "#         self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "#         self.drop = nn.Dropout(drop)\n",
        "\n",
        "#         self.apply(self._init_weights)\n",
        "\n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "#             fan_out //= m.groups\n",
        "#             m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "#             if m.bias is not None:\n",
        "#                 m.bias.data.zero_()\n",
        "\n",
        "#     def forward(self, x, H, W):\n",
        "#         x = self.fc1(x)\n",
        "#         x = self.dwconv(x, H, W)\n",
        "#         x = self.act(x)\n",
        "#         x = self.drop(x)\n",
        "#         x = self.fc2(x)\n",
        "#         x = self.drop(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
        "#         super().__init__()\n",
        "#         assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.num_heads = num_heads\n",
        "#         head_dim = dim // num_heads\n",
        "#         self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "#         self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
        "#         self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "#         self.attn_drop = nn.Dropout(attn_drop)\n",
        "#         self.proj = nn.Linear(dim, dim)\n",
        "#         self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "#         self.sr_ratio = sr_ratio\n",
        "#         if sr_ratio > 1:\n",
        "#             self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
        "#             self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "#         self.apply(self._init_weights)\n",
        "\n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "#             fan_out //= m.groups\n",
        "#             m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "#             if m.bias is not None:\n",
        "#                 m.bias.data.zero_()\n",
        "\n",
        "#     def forward(self, x, H, W):\n",
        "#         B, N, C = x.shape\n",
        "#         q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "#         if self.sr_ratio > 1:\n",
        "#             x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "#             x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n",
        "#             x_ = self.norm(x_)\n",
        "#             kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "#         else:\n",
        "#             kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "#         k, v = kv[0], kv[1]\n",
        "\n",
        "#         attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "#         attn = attn.softmax(dim=-1)\n",
        "#         attn = self.attn_drop(attn)\n",
        "\n",
        "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "#         x = self.proj(x)\n",
        "#         x = self.proj_drop(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class Block(nn.Module):\n",
        "\n",
        "#     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "#                  drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n",
        "#         super().__init__()\n",
        "#         self.norm1 = norm_layer(dim)\n",
        "#         self.attn = Attention(\n",
        "#             dim,\n",
        "#             num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#             attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n",
        "#         # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "#         self.norm2 = norm_layer(dim)\n",
        "#         mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "#         self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "#         self.apply(self._init_weights)\n",
        "\n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "#             fan_out //= m.groups\n",
        "#             m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "#             if m.bias is not None:\n",
        "#                 m.bias.data.zero_()\n",
        "\n",
        "#     def forward(self, x, H, W):\n",
        "#         x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
        "#         x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class OverlapPatchEmbed(nn.Module):\n",
        "#     \"\"\" Image to Patch Embedding\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
        "#         super().__init__()\n",
        "#         img_size = to_2tuple(img_size)\n",
        "#         patch_size = to_2tuple(patch_size)\n",
        "\n",
        "#         self.img_size = img_size\n",
        "#         self.patch_size = patch_size\n",
        "#         self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n",
        "#         self.num_patches = self.H * self.W\n",
        "#         self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
        "#                               padding=(patch_size[0] // 2, patch_size[1] // 2))\n",
        "#         self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "#         self.apply(self._init_weights)\n",
        "\n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "#             fan_out //= m.groups\n",
        "#             m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "#             if m.bias is not None:\n",
        "#                 m.bias.data.zero_()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.proj(x)\n",
        "#         _, _, H, W = x.shape\n",
        "#         x = x.flatten(2).transpose(1, 2)\n",
        "#         x = self.norm(x)\n",
        "\n",
        "#         return x, H, W\n",
        "\n",
        "\n",
        "# class MixVisionTransformer(nn.Module):\n",
        "#     def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n",
        "#                  num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
        "#                  attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n",
        "#                  depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n",
        "#         super().__init__()\n",
        "#         self.num_classes = num_classes\n",
        "#         self.depths = depths\n",
        "\n",
        "#         # patch_embed\n",
        "#         self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n",
        "#                                               embed_dim=embed_dims[0])\n",
        "#         self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n",
        "#                                               embed_dim=embed_dims[1])\n",
        "#         self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n",
        "#                                               embed_dim=embed_dims[2])\n",
        "#         self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n",
        "#                                               embed_dim=embed_dims[3])\n",
        "\n",
        "#         # transformer encoder\n",
        "#         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
        "#         cur = 0\n",
        "#         self.block1 = nn.ModuleList([Block(\n",
        "#             dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#             drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
        "#             sr_ratio=sr_ratios[0])\n",
        "#             for i in range(depths[0])])\n",
        "#         self.norm1 = norm_layer(embed_dims[0])\n",
        "\n",
        "#         cur += depths[0]\n",
        "#         self.block2 = nn.ModuleList([Block(\n",
        "#             dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#             drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
        "#             sr_ratio=sr_ratios[1])\n",
        "#             for i in range(depths[1])])\n",
        "#         self.norm2 = norm_layer(embed_dims[1])\n",
        "\n",
        "#         cur += depths[1]\n",
        "#         self.block3 = nn.ModuleList([Block(\n",
        "#             dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#             drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
        "#             sr_ratio=sr_ratios[2])\n",
        "#             for i in range(depths[2])])\n",
        "#         self.norm3 = norm_layer(embed_dims[2])\n",
        "\n",
        "#         cur += depths[2]\n",
        "#         self.block4 = nn.ModuleList([Block(\n",
        "#             dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#             drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n",
        "#             sr_ratio=sr_ratios[3])\n",
        "#             for i in range(depths[3])])\n",
        "#         self.norm4 = norm_layer(embed_dims[3])\n",
        "\n",
        "#         # classification head\n",
        "#         # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "#         self.apply(self._init_weights)\n",
        "\n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "#         elif isinstance(m, nn.Conv2d):\n",
        "#             fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "#             fan_out //= m.groups\n",
        "#             m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
        "#             if m.bias is not None:\n",
        "#                 m.bias.data.zero_()\n",
        "\n",
        "#     def reset_drop_path(self, drop_path_rate):\n",
        "#         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n",
        "#         cur = 0\n",
        "#         for i in range(self.depths[0]):\n",
        "#             self.block1[i].drop_path.drop_prob = dpr[cur + i]\n",
        "\n",
        "#         cur += self.depths[0]\n",
        "#         for i in range(self.depths[1]):\n",
        "#             self.block2[i].drop_path.drop_prob = dpr[cur + i]\n",
        "\n",
        "#         cur += self.depths[1]\n",
        "#         for i in range(self.depths[2]):\n",
        "#             self.block3[i].drop_path.drop_prob = dpr[cur + i]\n",
        "\n",
        "#         cur += self.depths[2]\n",
        "#         for i in range(self.depths[3]):\n",
        "#             self.block4[i].drop_path.drop_prob = dpr[cur + i]\n",
        "\n",
        "#     def freeze_patch_emb(self):\n",
        "#         self.patch_embed1.requires_grad = False\n",
        "\n",
        "#     @torch.jit.ignore\n",
        "#     def no_weight_decay(self):\n",
        "#         return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n",
        "\n",
        "#     def get_classifier(self):\n",
        "#         return self.head\n",
        "\n",
        "#     def reset_classifier(self, num_classes, global_pool=''):\n",
        "#         self.num_classes = num_classes\n",
        "#         self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "#     def forward_features(self, x):\n",
        "#         B = x.shape[0]\n",
        "#         outs = []\n",
        "\n",
        "#         # stage 1\n",
        "#         x, H, W = self.patch_embed1(x)\n",
        "#         for i, blk in enumerate(self.block1):\n",
        "#             x = blk(x, H, W)\n",
        "#         x = self.norm1(x)\n",
        "#         x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "#         outs.append(x)\n",
        "\n",
        "#         # stage 2\n",
        "#         x, H, W = self.patch_embed2(x)\n",
        "#         for i, blk in enumerate(self.block2):\n",
        "#             x = blk(x, H, W)\n",
        "#         x = self.norm2(x)\n",
        "#         x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "#         outs.append(x)\n",
        "\n",
        "#         # stage 3\n",
        "#         x, H, W = self.patch_embed3(x)\n",
        "#         for i, blk in enumerate(self.block3):\n",
        "#             x = blk(x, H, W)\n",
        "#         x = self.norm3(x)\n",
        "#         x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "#         outs.append(x)\n",
        "\n",
        "#         # stage 4\n",
        "#         x, H, W = self.patch_embed4(x)\n",
        "#         for i, blk in enumerate(self.block4):\n",
        "#             x = blk(x, H, W)\n",
        "#         x = self.norm4(x)\n",
        "#         x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
        "#         outs.append(x)\n",
        "\n",
        "#         return outs\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.forward_features(x)\n",
        "#         # x = self.head(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class DWConv(nn.Module):\n",
        "#     def __init__(self, dim=768):\n",
        "#         super(DWConv, self).__init__()\n",
        "#         self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
        "\n",
        "#     def forward(self, x, H, W):\n",
        "#         B, N, C = x.shape\n",
        "#         x = x.transpose(1, 2).view(B, C, H, W)\n",
        "#         x = self.dwconv(x)\n",
        "#         x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "#         return x\n",
        "\n",
        "\n",
        "\n",
        "# class mit_b0(MixVisionTransformer):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super(mit_b0, self).__init__(\n",
        "#             patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
        "#             qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n",
        "#             drop_rate=0.0, drop_path_rate=0.1)\n",
        "\n",
        "\n",
        "# class mit_b1(MixVisionTransformer):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super(mit_b1, self).__init__(\n",
        "#             patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
        "#             qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n",
        "#             drop_rate=0.0, drop_path_rate=0.1)\n",
        "\n",
        "\n",
        "# class mit_b2(MixVisionTransformer):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super(mit_b2, self).__init__(\n",
        "#             patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
        "#             qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1],\n",
        "#             drop_rate=0.0, drop_path_rate=0.1)\n",
        "\n",
        "\n",
        "# class mit_b3(MixVisionTransformer):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super(mit_b3, self).__init__(\n",
        "#             patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
        "#             qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n",
        "#             drop_rate=0.0, drop_path_rate=0.1)\n",
        "\n",
        "\n",
        "# class mit_b4(MixVisionTransformer):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super(mit_b4, self).__init__(\n",
        "#             patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
        "#             qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n",
        "#             drop_rate=0.0, drop_path_rate=0.1)\n",
        "\n",
        "\n",
        "# class mit_b5(MixVisionTransformer):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super(mit_b5, self).__init__(\n",
        "#             patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
        "#             qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n",
        "#             drop_rate=0.0, drop_path_rate=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D8Acq63We6x9",
      "metadata": {
        "id": "D8Acq63We6x9"
      },
      "source": [
        "### DAFormer Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UhIWyygx3FX_",
      "metadata": {
        "id": "UhIWyygx3FX_"
      },
      "outputs": [],
      "source": [
        "# #입력 차원(input_dim)을 임베딩 차원(embed_dim)으로 선형으로 매핑하는 간단한 MLP 클래스를 정의,다층 퍼셉트론\n",
        "# #참고 논문 :https://arxiv.org/abs/2111.14887\n",
        "# class MLP(nn.Module):\n",
        "#     \"\"\"\n",
        "#     Linear Embedding\n",
        "#     \"\"\"\n",
        "#     def __init__(self, input_dim=2048, embed_dim=768):\n",
        "#         super().__init__()\n",
        "#         self.proj = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.flatten(2).transpose(1, 2)\n",
        "#         x = self.proj(x)\n",
        "#         return x\n",
        "\n",
        "# class Context_Aware_Fusion(nn.Module):\n",
        "#     def __init__(self, input_dim=1024, embed_dim=256,dilations=[1,6,12,18]):\n",
        "#         super(Context_Aware_Fusion,self).__init__()\n",
        "\n",
        "\n",
        "#         self.proj = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "#         self.aspp = nn.ModuleList()\n",
        "\n",
        "#         self.dilations = dilations\n",
        "#         for dilation in self.dilations:\n",
        "#             if dilation == 1:\n",
        "#                 self.aspp.append(\n",
        "#                     nn.Sequential(\n",
        "#                             nn.Conv2d(input_dim,embed_dim,1,1),\n",
        "#                             nn.BatchNorm2d(embed_dim),\n",
        "#                             nn.ReLU()\n",
        "#                         )\n",
        "#                     )\n",
        "#             else:\n",
        "#                 self.aspp.append(\n",
        "#                     nn.Sequential(\n",
        "#                             nn.Conv2d(input_dim,input_dim,3,1,dilation,dilation,input_dim),\n",
        "#                             nn.Conv2d(input_dim,embed_dim,1,1),\n",
        "#                             nn.BatchNorm2d(embed_dim),\n",
        "#                             nn.ReLU()\n",
        "#                         )\n",
        "#                     )\n",
        "\n",
        "#         self.bottleneck = nn.Conv2d(input_dim,embed_dim,1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         cat_list = []\n",
        "#         for i in range(len(self.dilations)):\n",
        "#             cat_list.append(self.aspp[i](x))\n",
        "\n",
        "#         out = torch.cat(cat_list,1)\n",
        "\n",
        "#         out = self.bottleneck(out)\n",
        "#         return out\n",
        "\n",
        "\n",
        "# class daformerhead(nn.Module):\n",
        "\n",
        "#     def __init__(self,embed_dim = 256):\n",
        "#         super(daformerhead,self).__init__()\n",
        "#         self.linear_c1 = MLP(64,embed_dim)\n",
        "#         self.linear_c2 = MLP(128,embed_dim)\n",
        "#         self.linear_c3 = MLP(320,embed_dim)\n",
        "#         self.linear_c4 = MLP(512,embed_dim)\n",
        "\n",
        "\n",
        "#         self.cls_head = nn.Sequential(\n",
        "#             nn.Dropout2d(0.1),\n",
        "#             nn.Conv2d(embed_dim,13,1,1)\n",
        "#         )\n",
        "#         self.fuse_layer = Context_Aware_Fusion(embed_dim*4,embed_dim,[1,6,12,18])\n",
        "\n",
        "\n",
        "#     def forward(self,x):\n",
        "#         c1, c2, c3, c4 = x  # len=4, 1/4,1/8,1/16,1/32\n",
        "\n",
        "#         n, _, h, w = c4.shape\n",
        "\n",
        "\n",
        "\n",
        "#         _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n",
        "#         _c4 = F.interpolate(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
        "\n",
        "#         _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n",
        "#         _c3 = F.interpolate(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
        "\n",
        "#         _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n",
        "#         _c2 = F.interpolate(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
        "\n",
        "\n",
        "#         _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n",
        "\n",
        "#         out = self.fuse_layer(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n",
        "\n",
        "#         out = self.cls_head(F.interpolate(out, size=(CROP_HEIGHT,CROP_WIDTH),mode='bilinear',align_corners=False))\n",
        "\n",
        "\n",
        "#         return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G7jEIJoFe9i4",
      "metadata": {
        "id": "G7jEIJoFe9i4"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "soPI4GLbdA6z",
      "metadata": {
        "id": "soPI4GLbdA6z"
      },
      "outputs": [],
      "source": [
        "# class daformer(nn.Module):\n",
        "#     def __init__(self,backbone,head):\n",
        "#         super(daformer,self).__init__()\n",
        "#         self.backbone = backbone\n",
        "#         self.classifier = head\n",
        "\n",
        "#     def forward(self,x):\n",
        "#         out = {}\n",
        "#         out['features'] = self.backbone(x)\n",
        "#         out['out'] = self.classifier(out['features'])\n",
        "\n",
        "#         return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xTx4SP0CDFym",
      "metadata": {
        "id": "xTx4SP0CDFym"
      },
      "source": [
        "# 함수 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B0uAomSuDTCO",
      "metadata": {
        "id": "B0uAomSuDTCO"
      },
      "source": [
        "## 평가지표"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f2d4101",
      "metadata": {
        "id": "8f2d4101"
      },
      "outputs": [],
      "source": [
        "def accuracy(pred, label):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        b,c,h,w = pred.shape\n",
        "\n",
        "        _,pred = torch.max(pred, dim=1)\n",
        "        accuracy = torch.sum(pred==label)/(b*h*w)\n",
        "    return accuracy\n",
        "\n",
        "def miou(pred,label):\n",
        "    with torch.no_grad():\n",
        "        _,pred = torch.max(pred, dim=1)\n",
        "        intersection = torch.sum(((pred<12) & (pred==label)))\n",
        "        union = torch.sum((pred<12)|(label<12))\n",
        "\n",
        "\n",
        "        iou = intersection / (union+1e-9)\n",
        "\n",
        "    return iou\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ri4cx9QjDacy",
      "metadata": {
        "id": "Ri4cx9QjDacy"
      },
      "source": [
        "## Target Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rYuAq2a328_I",
      "metadata": {
        "id": "rYuAq2a328_I"
      },
      "outputs": [],
      "source": [
        "#모델이 생성한 가상 라벨과 실제 라벨 간의 Cross Entropy Loss를 임계값에 따라 가중치를 적용하여 계산\n",
        "class target_loss(nn.Module):\n",
        "    def __init__(self, t=0.968):\n",
        "        # 클래스 초기화: 임계값 t를 설정하고 Cross Entropy Loss 함수를 초기화\n",
        "        super(target_loss, self).__init__()\n",
        "        self.t = t\n",
        "        self.celoss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def forward(self, pred, pseudo_label):\n",
        "        b, c, h, w = pseudo_label.shape\n",
        "\n",
        "        # pseudo_label에서 최대값을 찾아 신뢰도와 해당 위치(pt)를 추출\n",
        "        confidence, pt = torch.max(pseudo_label, dim=1)  # b, h, w\n",
        "\n",
        "        # 신뢰도가 t보다 큰 픽셀의 비율(qt)을 계산\n",
        "        qt = (torch.sum(confidence.view(b, -1) > self.t, 1) / (h * w)).view(b, 1, 1)  # b, 1, 1\n",
        "\n",
        "        # 손실 계산: Cross Entropy Loss에 가중치(qt)를 곱하여 계산\n",
        "        loss = torch.mean(self.celoss(pred, pt) * qt)  # b, h, w * b, 1, 1 -> b, h, w -> 1\n",
        "\n",
        "        # 계산된 손실 반환\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w2CkKt2XDhKX",
      "metadata": {
        "id": "w2CkKt2XDhKX"
      },
      "source": [
        "## MIC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N8F6bWh8Dirl",
      "metadata": {
        "id": "N8F6bWh8Dirl"
      },
      "outputs": [],
      "source": [
        "# 입력 데이터에 마스킹을 적용하여 일부 영역을 가릴 수 있는 masking 함수\n",
        "#입력 데이터에 일부 영역을 가리는 마스킹 함수로, 입력 이미지를 무작위로 패치로 나누고\n",
        "# 주어진 비율에 따라 일부 패치를 가려 마스크를 생성하여 입력 이미지에 적용\n",
        "\n",
        "def masking(input, mask_ratio=0.5, mask_size=32):\n",
        "    # 입력 데이터의 크기와 형태를 가져옵니다.\n",
        "    b, c, h, w = input.shape\n",
        "\n",
        "    # 입력 이미지를 분할하기 위한 패치의 수를 계산합니다.\n",
        "    h_patch = h // mask_size\n",
        "    w_patch = w // mask_size\n",
        "\n",
        "    # 각 패치에 대한 마스크를 무작위로 생성하고, 주어진 비율(mask_ratio)에 따라 값을 설정합니다.\n",
        "    mask = (np.random.uniform(0, 1, (h_patch, w_patch, b)) > mask_ratio).astype(np.uint8())\n",
        "\n",
        "    # 마스크 크기를 입력 이미지 크기로 다시 조정합니다.\n",
        "    mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Numpy 배열을 PyTorch 텐서로 변환하고, 차원을 조정하여 입력 이미지와 호환되도록 합니다.\n",
        "    mask = torch.from_numpy(mask).permute(2, 0, 1).reshape(b, 1, h, w).to(device)\n",
        "\n",
        "    # 입력 이미지를 복제하고, 마스크를 곱하여 일부 영역을 가립니다.\n",
        "    output = input.detach().clone() * mask\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7M54zM17bUc0",
      "metadata": {
        "id": "7M54zM17bUc0"
      },
      "source": [
        "## Slice Pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QIq9hxHubYGg",
      "metadata": {
        "id": "QIq9hxHubYGg"
      },
      "outputs": [],
      "source": [
        "#이미지를 슬라이딩 윈도우로 나누어 모델에 전달하고, 각 윈도우에서 얻은 예측을 누적하여 평균을 구하는 알고리즘.\n",
        "# 패딩 여부에 따라 입력 이미지 주변에 패딩을 추가하거나 결과에서 패딩을 제거함.\n",
        "def slide_pred(model,img,window_size=(CROP_HEIGHT,CROP_WIDTH),stride=(CROP_HEIGHT,CROP_WIDTH),softmax=False,padding=False):\n",
        "    #추론 속도를 높이려면 입력 이미지를 나누어 Batch로 만들도록 코드를 재구성\n",
        "    #학습, 추론 과정 모두 Batch 단위로 입력이 들어오기 때문에 메모리 문제로 인해 Batch로 만들지 못함\n",
        "    #실제 적용에서는 무조건 Batch가 1이기 때문에 이미지 하나를 Batch로 만들 수 있음\n",
        "    b, c, h, w = img.shape\n",
        "\n",
        "    if padding:\n",
        "        # 입력 이미지 주변에 패딩을 추가합니다.\n",
        "        padded_img = torch.zeros((b, c, h + (window_size[0] - stride[0]) * 2, w + (window_size[1] - stride[1]) * 2)).to(device)\n",
        "        padded_img[:, :, window_size[0] - stride[0]:-(window_size[0] - stride[0]), window_size[1] - stride[1]:-(window_size[1] - stride[1])] = img\n",
        "        output = torch.zeros((b, 14, h + (window_size[0] - stride[0]) * 2, w + (window_size[1] - stride[1]) * 2)).to(device)\n",
        "    else:\n",
        "        padded_img = img\n",
        "        output = torch.zeros((b, 14, h, w)).to(device)\n",
        "\n",
        "    ph, pw = padded_img.shape[2:]\n",
        "\n",
        "    for i in range(0, ph - window_size[0] + 1, stride[0]):\n",
        "        for j in range(0, pw - window_size[1] + 1, stride[1]):\n",
        "            # 이미지를 슬라이딩하면서 작은 윈도우를 추출\n",
        "            input = padded_img[:, :, i:i+window_size[0], j:j+window_size[1]]\n",
        "\n",
        "            if softmax:\n",
        "                # 모델을 사용하여 입력 이미지를 추론하고 소프트맥스 적용\n",
        "                pred = torch.softmax(model(input)['out'], 1)\n",
        "            else:\n",
        "                # 모델을 사용하여 입력 이미지를 추론\n",
        "                pred = model(input)['out']\n",
        "\n",
        "            # 결과를 누적하여 output에 추가\n",
        "            output[:, :13, i:i+window_size[0], j:j+window_size[1]] += pred\n",
        "            output[:, 13, i:i+window_size[0], j:j+window_size[1]] += 1\n",
        "\n",
        "    # 결과를 누적한 횟수로 나누어 평균을 구함\n",
        "    output = output[:, :13] / output[:, 13:]\n",
        "\n",
        "    if padding:\n",
        "        # 패딩을 제거하고 원래 이미지 크기로 결과를 자름\n",
        "        output = output[:, :, window_size[0] - stride[0]:-(window_size[0] - stride[0]), window_size[1] - stride[1]:-(window_size[1] - stride[1])]\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0895765-fba0-4fd9-b955-a6c0e43012e9",
      "metadata": {
        "id": "a0895765-fba0-4fd9-b955-a6c0e43012e9"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D6EvJ8TtKyIE",
      "metadata": {
        "id": "D6EvJ8TtKyIE"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 32):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)  # type: ignore\n",
        "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
        "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
        "seed_everything(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9544c0",
      "metadata": {
        "id": "9e9544c0"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "#Deeplab v3+ CityScape Pretrain (https://github.com/VainF/DeepLabV3Plus-Pytorch)\n",
        "model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=13, output_stride=8)\n",
        "teacher_model = network.modeling.__dict__['deeplabv3plus_resnet101'](num_classes=13, output_stride=8)\n",
        "state_dict= torch.load(root_path + '/pretrain/best_deeplabv3plus_resnet101_cityscapes_os16.pth')['model_state']\n",
        "filtered_dict = {key: value for key, value in state_dict.items() if 'backbone' in key} #encoder만 가져오기\n",
        "model.load_state_dict(filtered_dict,strict=False )\n",
        "\n",
        "## DaFormer (Weight from Segformer : https://github.com/NVlabs/SegFormer)\n",
        "# b3,b4,b5의 선택지가 있음\n",
        "#model = daformer(mit_b4(),daformerhead())\n",
        "#model.load_state_dict(torch.load(f'/content/drive/MyDrive/samsung_seg(resize)/pretrain/segformer.b4.1024x1024.city.160k.pth')['state_dict'],strict=False)\n",
        "teacher_model = daformer(mit_b4(),daformerhead())\n",
        "\n",
        "learning_status = {\n",
        "    'train_accs' : [],\n",
        "    'valid_accs' : [],\n",
        "    'train_ious' : [],\n",
        "    'valid_ious' : [],\n",
        "    'train_losses' : [],\n",
        "    'valid_losses' : [],\n",
        "    'lrs' : []\n",
        "}\n",
        "min_epoch = 0\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.backbone.parameters(), 'lr': 1e-4},\n",
        "     {'params': model.classifier.parameters(), 'lr': 1e-3}], weight_decay=0.01)\n",
        "\n",
        "save_last = True\n",
        "\n",
        "# Warmup 파라미터\n",
        "#Warmup 스케쥴링은 학습 초기에 큰 학습률이 Gradient 분포를 왜곡하는 것을 방지하면서 모델의 일반화 능력을 키워줌\n",
        "warmup_epochs = 50\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,last_epoch=len(learning_status['lrs'])-1, lr_lambda=lambda epoch: epoch / warmup_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c22dcb0",
      "metadata": {
        "id": "9c22dcb0"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=12)\n",
        "valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c1b1900",
      "metadata": {
        "id": "5c1b1900"
      },
      "outputs": [],
      "source": [
        "model_save_path = root_path + '/model/pth/'\n",
        "\n",
        "model = model.to(device)\n",
        "teacher_model = teacher_model.to(device)\n",
        "LS = nn.CrossEntropyLoss()\n",
        "LT = target_loss(t=0.968)\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "#EMA 파라미터\n",
        "alpha = 0.9\n",
        "\n",
        "#Mask 파라미터\n",
        "masking_ratio = 0.75\n",
        "\n",
        "mask_size = 32\n",
        "\n",
        "lambda_mask = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4060e94c",
      "metadata": {
        "id": "4060e94c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_begin(training, loader, running_loss, running_acc, running_iou):\n",
        "    # 학습 또는 검증 모드에 따라 진행 상태 문자열(desc) 설정\n",
        "    if training:\n",
        "        desc = \"Train\"\n",
        "    else:\n",
        "        desc = \"Valid\"\n",
        "\n",
        "    # tqdm을 사용하여 진행 상황을 시각화하는 진행 표시 준비\n",
        "    progress = tqdm.tqdm(loader, desc=f'Epoch:{epoch+1}/{epochs}')\n",
        "\n",
        "    # 데이터 로더에서 미니배치를 가져와서 반복 처리\n",
        "    for i, data in enumerate(progress):\n",
        "        source_img, source_mask, target_img, distortion_img, distortion_mask = data\n",
        "\n",
        "        # 데이터를 GPU로 이동\n",
        "        source_img = source_img.to(device)\n",
        "        source_mask = source_mask.to(device).long()\n",
        "\n",
        "        # 손실 변수 초기화\n",
        "        loss_mask = torch.tensor(0.0).to(device)\n",
        "        loss_distortion = torch.tensor(0.0).to(device)\n",
        "\n",
        "        # 훈련 모드(training)인 경우\n",
        "        if (training):\n",
        "            # 옵티마이저의 그래디언트 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 대상 이미지와 왜곡 이미지를 GPU로 이동\n",
        "            target_img = target_img.to(device)\n",
        "            distortion_img = distortion_img.to(device)\n",
        "            distortion_mask = distortion_mask.to(device).long()\n",
        "\n",
        "            # 가짜 레이블 생성 (티처 모델 활용)\n",
        "            with torch.no_grad():\n",
        "                pseudo_label = teacher_model(target_img)['out']\n",
        "                pseudo_label = torch.softmax(pseudo_label, dim=1)  # b,c,h,w\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #Distortion loss\n",
        "            ## 왜곡 손실 계산\n",
        "            pred_distortion = model(distortion_img)['out']\n",
        "\n",
        "            loss_distortion = LS(pred_distortion,distortion_mask)\n",
        "\n",
        "\n",
        "            # MIC 기법 : 패치 일부 masking\n",
        "            target_img = masking(target_img,masking_ratio,mask_size)\n",
        "\n",
        "            ## 대상 이미지의 예측과 가짜 레이블 간 손실 계산\n",
        "            pred_target = model(target_img)['out']\n",
        "\n",
        "            loss_mask = LT(pred_target,pseudo_label) * lambda_mask\n",
        "\n",
        "\n",
        "        # 원본 이미지에 대한 예측 계산\n",
        "        pred_source = model(source_img)['out']\n",
        "\n",
        "        # 원본 이미지와 레이블 간 손실 계산\n",
        "        loss_source = LS(pred_source,source_mask)\n",
        "\n",
        "        # 총 손실 계산\n",
        "        loss_total = loss_source + loss_distortion + loss_mask\n",
        "        # 정확도 및 IoU(Intersection over Union) 계산\n",
        "        acc = accuracy(pred_source,source_mask)\n",
        "\n",
        "        iou = miou(pred_source,source_mask)\n",
        "\n",
        "        # 훈련(training) 모드인 경우 그래디언트 역전파 및 파라미터 업데이트\n",
        "        if (training):\n",
        "\n",
        "            loss_total.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "        # 손실, 정확도 및 IoU 결과 저장\n",
        "        running_loss += [loss_source.detach().cpu().numpy(),loss_distortion.detach().cpu().numpy(),loss_mask.detach().cpu().numpy()]\n",
        "        running_acc += [acc.cpu()]\n",
        "        running_iou += [iou.cpu()]\n",
        "        # 진행 표시 업데이트\n",
        "        progress.set_description(f'Epoch:{epoch+1}/{epochs} | {desc}_Acc:{np.round(running_acc/(i+1),4)} | {desc}_IoU:{np.round(running_iou/(i+1),4)} | {desc}_Loss:{np.round(running_loss/(i+1),4)} | Self-Training:{running_loss[-1]>0}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6340a621",
      "metadata": {
        "id": "6340a621"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(\"모델 저장 경로 : \"+ model_save_path)\n",
        "fit_time = time.time()\n",
        "start_epoch = len(learning_status['lrs'])\n",
        "teacher_model.eval()\n",
        "\n",
        "\n",
        "for i in range(len(learning_status['valid_accs'])):\n",
        "    print(f\"LR : {learning_status['lrs'][i]}\")\n",
        "    print(f\"Epoch:{i+1}/{epochs} | Train_Acc : {np.round(learning_status['train_accs'][i],4)} | Train_IoU : {np.round(learning_status['train_ious'][i],4)} | Train_Loss : {np.round(learning_status['train_losses'][i],4)}\")\n",
        "    print(f\"Epoch:{i+1}/{epochs} | Valid_Acc : {np.round(learning_status['valid_accs'][i],4)} | Valid_IoU : {np.round(learning_status['valid_ious'][i],4)} | Valid_Loss : {np.round(learning_status['valid_losses'][i],4)}\")\n",
        "    print()\n",
        "\n",
        "for epoch in range(start_epoch,epochs):\n",
        "    print(\"모델명 :\", model_save_path.split('/')[-2])\n",
        "\n",
        "    #Warmup Schedule\n",
        "    if epoch < warmup_epochs :\n",
        "        scheduler.step()\n",
        "        print(\"lr이 변경되었습니다.\",optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    #EMA Update\n",
        "    alpha_teacher = min(1 - 1 / (epoch + 1), alpha)\n",
        "    for ema_param, param in zip(teacher_model.parameters(), model.parameters()):\n",
        "        ema_param.data = alpha_teacher * ema_param.data + (1 - alpha_teacher) * param.data\n",
        "\n",
        "    print(\"EMA Weight Update 적용, Alpha =\",alpha_teacher)\n",
        "\n",
        "    running_train_loss = np.array([0.0,0.0,0.0])\n",
        "    running_valid_loss = np.array([0.0,0.0,0.0])\n",
        "\n",
        "    running_train_acc = np.array([0.0])\n",
        "    running_valid_acc = np.array([0.0])\n",
        "\n",
        "    running_train_iou = np.array([0.0])\n",
        "    running_valid_iou = np.array([0.0])\n",
        "\n",
        "    model.train()\n",
        "    train_begin(True,train_loader,running_train_loss,running_train_acc,running_train_iou)\n",
        "    model.eval()\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        train_begin(False,valid_loader,running_valid_loss,running_valid_acc,running_valid_iou)\n",
        "        '''\n",
        "\n",
        "    if (os.path.exists(model_save_path+'self_training/')==False):\n",
        "        os.makedirs(model_save_path+'self_training/',exist_ok=True)\n",
        "\n",
        "    learning_status['train_losses'].append((running_train_loss/len(train_loader)))\n",
        "    learning_status['valid_losses'].append((running_valid_loss/len(valid_loader)))\n",
        "    learning_status['train_accs'].append((running_train_acc/len(train_loader)))\n",
        "    learning_status['valid_accs'].append((running_valid_acc/len(valid_loader)))\n",
        "    learning_status['train_ious'].append((running_train_iou/len(train_loader)))\n",
        "    learning_status['valid_ious'].append((running_valid_iou/len(valid_loader)))\n",
        "    learning_status['lrs'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    df = pd.DataFrame(learning_status)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch+1 , #에폭\n",
        "        'model': model.state_dict(),  # 모델\n",
        "        'teacher_model': teacher_model.state_dict(), # Teacher 모델\n",
        "        'optimizer': optimizer.state_dict(),  # 옵티마이저\n",
        "        'scheduler': scheduler.state_dict(),  # 스케줄러\n",
        "    }\n",
        "\n",
        "    if learning_status['train_losses'][-1][-1] >0:\n",
        "        torch.save(checkpoint, model_save_path+f'self_training/Last.pth')\n",
        "        if (epoch+1)%5 == 0:\n",
        "            torch.save(checkpoint, model_save_path+f'self_training/Epoch({epoch+1:03d}).pth')\n",
        "\n",
        "        df.to_csv(model_save_path+'self_training/status.csv', index=True)\n",
        "        save_last = False\n",
        "\n",
        "    elif save_last:\n",
        "\n",
        "        torch.save(checkpoint, model_save_path+f'Last.pth')\n",
        "        df.to_csv(model_save_path+'status.csv', index=True)\n",
        "\n",
        "\n",
        "    if sum(learning_status['valid_losses'][min_epoch]) >= sum(learning_status['valid_losses'][-1]) and sum(learning_status['valid_losses'][-1] > 0):\n",
        "        print(f\"Valid Loss가 최소가 됐습니다. ({sum(learning_status['valid_losses'][min_epoch]):.4f}({min_epoch+1}) -> {sum(learning_status['valid_losses'][-1]):.4f}({len(learning_status['valid_losses'])}))\")\n",
        "        print(f'해당 모델이 {model_save_path}Best.pth 경로에 저장됩니다.')\n",
        "        min_epoch = len(learning_status['valid_losses'])-1\n",
        "        torch.save(checkpoint, model_save_path+f'Best.pth')\n",
        "    else:\n",
        "        print(f\"Valid_Loss가 최소가 되지 못했습니다.(최소 Epoch:{min_epoch+1} : {sum(learning_status['valid_losses'][min_epoch]):.4f})\")\n",
        "\n",
        "    print('')\n",
        "\n",
        "\n",
        "    #한 에폭 마무리마다 Target 데이터 중의 일부에 대해서 추론한 결과 저장\n",
        "    with torch.no_grad():\n",
        "        for path in [f'{data_path}/train_target_image/TRAIN_TARGET_0000.png',\n",
        "                     f'{data_path}/train_target_image/TRAIN_TARGET_0002.png',\n",
        "                     f'{data_path}/train_target_image/TRAIN_TARGET_0032.png',\n",
        "                     f'{data_path}/train_target_image/TRAIN_TARGET_0650.png']:\n",
        "\n",
        "\n",
        "\n",
        "            img = cv2.imread(path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            h,w,c = img.shape\n",
        "            learning_type = path.split('/')[-1].split('_')[0]\n",
        "\n",
        "            if learning_type == \"VALID\":\n",
        "                img = A.Normalize(mean = source_mean , std = source_std)(image=img)['image']\n",
        "            else:\n",
        "                img = A.Normalize(mean = target_mean , std = target_std)(image=img)['image']\n",
        "\n",
        "            img = ToTensorV2()(image=img)['image']\n",
        "\n",
        "            pred = slide_pred(model,img.unsqueeze(0).to(device),stride=(CROP_HEIGHT//2,CROP_WIDTH//2), softmax=True, padding=False)\n",
        "\n",
        "            pred = torch.argmax(pred,1)[0].detach().cpu().numpy()\n",
        "\n",
        "            img = (denormalization(np.transpose(img,(1,2,0)).numpy(),learning_type == \"VALID\")*255).astype(np.uint8)\n",
        "\n",
        "            color_pred = np.zeros_like(img,dtype=np.uint8)\n",
        "\n",
        "            for i,color in enumerate(palette):\n",
        "                color_pred[pred==i] = color\n",
        "\n",
        "            os.makedirs(f\"{model_save_path}pred/{path.split('/')[-1].split('.')[0]}/\",exist_ok=True)\n",
        "            os.makedirs(f\"{model_save_path}mix/{path.split('/')[-1].split('.')[0]}/\",exist_ok=True)\n",
        "\n",
        "            cv2.imwrite(f\"{model_save_path}pred/{path.split('/')[-1].split('.')[0]}/{epoch+1:03d}.png\",cv2.cvtColor(color_pred,cv2.COLOR_RGB2BGR))\n",
        "            cv2.imwrite(f\"{model_save_path}mix/{path.split('/')[-1].split('.')[0]}/{epoch+1:03d}.png\", cv2.cvtColor(cv2.addWeighted(img, 0.5, color_pred  , 0.5, 0),cv2.COLOR_RGB2BGR))\n",
        "\n",
        "\n",
        "\n",
        "print('학습 최종 시간: {:.2f} 분\\n' .format((time.time()- fit_time)/60))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73Eu7AazztA4",
      "metadata": {
        "id": "73Eu7AazztA4"
      },
      "source": [
        "# 이전 학습 상황 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lG5BXk_JKyk0",
      "metadata": {
        "id": "lG5BXk_JKyk0"
      },
      "outputs": [],
      "source": [
        "f\"\\'/model/{model_save_path.split('/')[-2]}/\\'\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01bb7ef",
      "metadata": {
        "id": "d01bb7ef"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model_load_path = root_path + '/model/DAformer(addfisheye)/'\n",
        "\n",
        "target_epoch = 56\n",
        "\n",
        "\n",
        "if os.path.exists(model_load_path+f'self_training/Epoch({target_epoch:03d}).pth'): #Self-Training을 시작한 이후 특정 에폭에 대한 모델 가중치가 존재하는 경우\n",
        "    learning_status = pd.read_csv(model_load_path+f'self_training/status.csv',index_col=0).to_dict(orient='list')\n",
        "    checkpoint = torch.load(model_load_path+f'self_training/Epoch({target_epoch:03d}).pth','cuda')\n",
        "    print(1)\n",
        "elif target_epoch > 0 and os.path.exists(model_load_path+f'self_training/Last.pth'): #Self-Training을 시작한 이후 특정 에폭에 대한 모델 가중치가 존재하지 않는 경우\n",
        "    learning_status = pd.read_csv(model_load_path+f'self_training/status.csv',index_col=0).to_dict(orient='list')\n",
        "    checkpoint = torch.load(model_load_path+f'self_training/Last.pth','cuda')\n",
        "    print(2)\n",
        "else: #Self-Training을 시작하기 전\n",
        "    learning_status = pd.read_csv(model_load_path+f'self_training/status.csv',index_col=0).to_dict(orient='list')\n",
        "    checkpoint = torch.load(model_load_path+f'Last.pth','cuda')\n",
        "    print(3)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "teacher_model.load_state_dict(checkpoint['teacher_model'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "\n",
        "save_last=not(os.path.exists(model_load_path+f'self_training/Epoch({target_epoch:03d}).pth'))\n",
        "\n",
        "for k in learning_status.keys():\n",
        "    for i in range(len(learning_status[k])):\n",
        "        if k!='lrs':\n",
        "            learning_status[k][i] = np.array([item for item in learning_status[k][i][1:-1].split(' ') if item != ''],dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "learning_status = {\n",
        "    'train_accs': learning_status['train_accs'][:checkpoint['epoch']],\n",
        "    'valid_accs': learning_status['valid_accs'][:checkpoint['epoch']],\n",
        "    'train_ious': learning_status['train_ious'][:checkpoint['epoch']],\n",
        "    'valid_ious': learning_status['valid_ious'][:checkpoint['epoch']],\n",
        "    'train_losses': learning_status['train_losses'][:checkpoint['epoch']],\n",
        "    'valid_losses': learning_status['valid_losses'][:checkpoint['epoch']],\n",
        "    'lrs': learning_status['lrs'][:checkpoint['epoch']]\n",
        "}\n",
        "\n",
        "min_epoch = np.argmin(np.sum(learning_status['valid_losses'],-1))\n",
        "\n",
        "for i in range(len(learning_status['valid_accs'])):\n",
        "    print(f\"Epoch:{i+1} | Train_Acc : {np.round(learning_status['train_accs'][i],4)} | Train_IoU : {np.round(learning_status['train_ious'][i],4)} | Train_Loss : {np.round(learning_status['train_losses'][i],4)}\")\n",
        "    print(f\"Epoch:{i+1} | Valid_Acc : {np.round(learning_status['valid_accs'][i],4)} | Valid_IoU : {np.round(learning_status['valid_ious'][i],4)} | Valid_Loss : {np.round(learning_status['valid_losses'][i],4)}\")\n",
        "    print()\n",
        "\n",
        "print(len(learning_status['lrs']),min_epoch+1,learning_status['train_accs'][-1],learning_status['train_losses'][-1],learning_status['valid_accs'][-1],learning_status['valid_losses'][-1],learning_status['lrs'][-1])\n",
        "print(len(learning_status['lrs']),min_epoch+1,learning_status['train_accs'][min_epoch],learning_status['train_losses'][min_epoch],learning_status['valid_accs'][min_epoch],learning_status['valid_losses'][min_epoch],learning_status['lrs'][min_epoch])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32eb51c-a3fe-4e11-a616-3a717ba16f7e",
      "metadata": {
        "id": "c32eb51c-a3fe-4e11-a616-3a717ba16f7e"
      },
      "source": [
        "# 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NXfP7o62Buz9",
      "metadata": {
        "id": "NXfP7o62Buz9"
      },
      "outputs": [],
      "source": [
        "def rle_encode(mask):\n",
        "    pixels = mask.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12371c8b-0c78-47df-89ec-2d8b55c8ea94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12371c8b-0c78-47df-89ec-2d8b55c8ea94",
        "outputId": "11188a23-fd83-4b10-91a9-d9c00465b5d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "test_set = CustomDataset(csv_file=f'{root_path}/test.csv', infer=True)\n",
        "test_loader = DataLoader(test_set, batch_size=16, shuffle=False, num_workers=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i5V8FjGoBhYW",
      "metadata": {
        "id": "i5V8FjGoBhYW"
      },
      "outputs": [],
      "source": [
        "model = daformer(mit_b5(),daformerhead())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Twm8CAMlNLRh",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Twm8CAMlNLRh"
      },
      "outputs": [],
      "source": [
        "\n",
        "state_dict = torch.load('/content/drive/MyDrive/samsung_seg(512,1024)/model/DAformer(B5,crop2x,32seed,valid2x,1e-4)/self_training/Epoch(060).pth','cuda')\n",
        "model.load_state_dict(state_dict['model'])\n",
        "print(state_dict['epoch'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uugvJh7PLInQ",
      "metadata": {
        "id": "uugvJh7PLInQ"
      },
      "outputs": [],
      "source": [
        "print(state_dict['epoch'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    result = []\n",
        "    for i,imgs in enumerate(tqdm.tqdm(test_loader)):\n",
        "        b,c,h,w = imgs.shape\n",
        "\n",
        "        input = imgs.float().to(device)\n",
        "\n",
        "        #preds = model(input)['out'] #전체 이미지를 활용한 모델은 이 코드 사용\n",
        "        preds = slide_pred(model,input,stride=(CROP_HEIGHT//2,CROP_WIDTH//2),softmax = True ,padding=False) # b,c,h,w\n",
        "\n",
        "\n",
        "        original_mask = torch.from_numpy(original_mask).reshape(1,h,w).repeat(b,1,1)\n",
        "        pred_original = torch.argmax(preds,1).cpu().numpy() # b,c,h,w -> b,h,w\n",
        "\n",
        "\n",
        "\n",
        "        pred_resize = F.interpolate(preds, size=(540,960),mode='bilinear',align_corners=False)\n",
        "        pred_resize = torch.argmax(pred_resize,1).cpu().numpy() # b,c,h,w -> b,h,w\n",
        "\n",
        "\n",
        "        #타원을 제외한 위치에 있는 값들 후처리\n",
        "        center = (960//2, int(540*0.375))  # x,y\n",
        "        axis_length = (960//2, int(540*0.64))  # 장축 반지름과 단축 반지름\n",
        "        resize_mask = np.ones((540,960),dtype=np.uint8)\n",
        "        cv2.ellipse(resize_mask, center, axis_length, 0, 0, 360, 0, -1)\n",
        "        resize_mask = torch.from_numpy(resize_mask).unsqueeze(0).repeat(b,1,1)\n",
        "\n",
        "        pred_resize[resize_mask==1] = 12\n",
        "\n",
        "\n",
        "        if i < 5 :\n",
        "            for j,pred in enumerate(pred_original):\n",
        "                if j<5:\n",
        "                    print(i*16+j)\n",
        "                    img = (denormalization(np.transpose(imgs[j].cpu().numpy(),(1,2,0)),False)*255).astype(np.uint8)\n",
        "                    color_pred = np.zeros_like(img,dtype=np.uint8)\n",
        "                    color_resize_pred = np.zeros((540,960,3),dtype=np.uint8)\n",
        "\n",
        "\n",
        "                    for k,color in enumerate(palette):\n",
        "                        color_pred[pred==k] = color\n",
        "                        color_resize_pred[pred_resize[j]==k] = color\n",
        "\n",
        "\n",
        "                    plt.figure(figsize=(30,15))\n",
        "                    plt.subplot(1,4,1)\n",
        "                    plt.imshow(img)\n",
        "\n",
        "\n",
        "                    plt.subplot(1,4,2)\n",
        "                    plt.imshow(cv2.addWeighted(img, 0.5, color_pred  , 0.5, 0))\n",
        "\n",
        "\n",
        "                    plt.subplot(1,4,3)\n",
        "                    plt.imshow(color_pred)\n",
        "\n",
        "                    plt.subplot(1,4,4)\n",
        "                    plt.imshow(color_resize_pred)\n",
        "\n",
        "\n",
        "                    plt.show()\n",
        "\n",
        "        for j,pred in enumerate(pred_resize):\n",
        "            for class_id in range(12):\n",
        "                class_mask = (pred == class_id).astype(np.uint8)\n",
        "                if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
        "                    mask_rle = rle_encode(class_mask)\n",
        "                    result.append(mask_rle)\n",
        "                else: # 마스크가 존재하지 않는 경우 -1\n",
        "                    result.append(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zHnokgkjDRAc",
      "metadata": {
        "id": "zHnokgkjDRAc"
      },
      "outputs": [],
      "source": [
        "print(state_dict['epoch'])\n",
        "submit = pd.read_csv(f'{root_path}/sample_submission.csv')\n",
        "submit['mask_rle'] = result\n",
        "submit.to_csv(f'{root_path}/baseline_submit.csv', index=False)\n",
        "submit.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "h5PCUOxl2DBg",
        "be76a29e-e9c2-411a-a569-04166f074184",
        "dc955893-22fd-4320-88be-7aa0d790cbd9",
        "G7jEIJoFe9i4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}